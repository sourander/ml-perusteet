{"config":{"lang":["fi"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tervetuloa kurssille","text":"<p>Oppimateriaali on tarkoitettu Kajaanin Ammattikorkeakoulun opiskelijoille. Huomaa, ett\u00e4 t\u00e4m\u00e4 kurssi on perinteisen koneoppimisen kurssi. Alla (ks. Kuvio 1) t\u00e4h\u00e4n liittyv\u00e4 humoristinen sarjakuvapiirros. Generatiiviset teko\u00e4lyt ovat nyt kovassa huudossa, mutta perinteinen koneoppiminen on edelleen t\u00e4rke\u00e4 osa teko\u00e4ly\u00e4. K\u00e4yt\u00e4nn\u00f6ss\u00e4 t\u00e4m\u00e4 tarkoittaa, ett\u00e4 voit unohtaa t\u00e4m\u00e4n kurssin ajaksi teknologiat ja kirjastot kuten TensorFlow ja PyTorch.</p> <p>Ohjelmointikielen\u00e4 kurssilla on Python, joten kursilla oletetaan v\u00e4hint\u00e4\u00e4nkin Pythonin alkeiden hallintaa. My\u00f6s seuraavat teknologiat on hyv\u00e4 olla hallussa, tai ne pit\u00e4\u00e4 valmistautua ottamaan haltuun kurssin aikana: git, Jupyter Notebook, Pip ja Venv. Kurssiteht\u00e4viss\u00e4 k\u00e4ytet\u00e4\u00e4n my\u00f6s Numpy-, Pandas- ja Scikit-learn-kirjastoja. Huomaa, ett\u00e4 kurssiteht\u00e4v\u00e4t eli harjoitusty\u00f6t on hostattu muualla, esimerkiksi Moodlessa ja/tai Gitlabissa.</p> <p></p> <p>Kuvio 1. Sarjakuvapiirros koneoppimisesta. (Copyright note: The memeified comic is by unknown author, but found from r/machinelearningmemes posted by u/joelllthedestoryer. Published with permission from the creator of the original comic strip MadeByTio.)</p> <p>Tip</p> <p>Pandas- ja Scikit-learn kirjastoja tarvitset harjoitust\u00f6iss\u00e4, joissa sinun tulee k\u00e4ytt\u00e4\u00e4 algoritmien valmiita toteutuksia. N\u00e4iden k\u00e4ytt\u00f6\u00e4 ei opasteta t\u00e4ss\u00e4 materiaalissa. Voit kuitenkin tutustua niihin esimerkiksi seuraavien linkkien kautta:</p> <ul> <li>Pandas Cookbook</li> <li>Khuyen Tran's Efficient Python Tricks and Tools for Data Scientists</li> <li>Pandas Cheat Sheet</li> </ul>"},{"location":"#faktavirheet","title":"Faktavirheet","text":"<p>Mik\u00e4li oppimateriaali sis\u00e4lt\u00e4\u00e4 virheellist\u00e4 tietoa, tee jompi kumpi:</p> <ul> <li>Forkkaa GitHubin repository ja tarjoa Pull Request, joka sis\u00e4lt\u00e4\u00e4 korjausehdotukset.</li> <li>Ota yhteytt\u00e4 yll\u00e4pitoon ja esittele virheellisen tiedon korjaus.</li> </ul>"},{"location":"algoritmit/bayes/intuitio/","title":"Intuitio","text":"<p>T\u00e4m\u00e4 ei ole matematiikan kurssi eik\u00e4 t\u00e4t\u00e4 materiaalia ole kirjoittanut matematiikan opettaja. Siit\u00e4 huolimatta kurssilla t\u00e4ytyy hieman k\u00e4sitell\u00e4 todenn\u00e4k\u00f6isyysmatematiikkaa, tilastotiedett\u00e4 ja lineaarialgebraa. T\u00e4ss\u00e4 osiossa k\u00e4sitell\u00e4\u00e4n Bayesin teoreemaa intuition tasolla. T\u00e4m\u00e4 pohjustaa seuraavia vaiheita, joissa k\u00e4yt\u00e4mme Bayesin teoreemaa koneoppimisessa. Hyvin yksinkertainen Bayesilainen esimerkki olisi \"todenn\u00e4k\u00f6isyys, ett\u00e4 opiskelija on l\u00e4p\u00e4issyt pizzakurssin\". T\u00e4m\u00e4 merkitt\u00e4isiin n\u00e4in:</p> \\[ \\begin{align*}     B &amp;= \\text{ passed the pizza course} \\\\     P(B) &amp;= 0.75 \\end{align*} \\] <p>Huomaa, ett\u00e4 <code>B</code> on kaavassa tapahtuma. <code>P(B)</code> on tapahtuman todenn\u00e4k\u00f6isyys. T\u00e4ss\u00e4 lapsellisessa esimerkiss\u00e4 oletetaan, ett\u00e4 aiemman datan nojalla -kurssin on l\u00e4p\u00e4issyt 75 % opiskelijoista, joita voit tavata k\u00e4yt\u00e4v\u00e4ll\u00e4. </p>"},{"location":"algoritmit/bayes/intuitio/#riippuvaisuus","title":"Riippuvaisuus","text":""},{"location":"algoritmit/bayes/intuitio/#riippumattomat-tapahtumat","title":"Riippumattomat tapahtumat","text":"<p>Mik\u00e4li k\u00e4yt\u00e4v\u00e4ll\u00e4 tulee kaksi satunnaista opiskelijaa vastaan, voimme pohtia, ett\u00e4: \"Kuinka todenn\u00e4k\u00f6ist\u00e4 on, ett\u00e4 kummatkin ovat l\u00e4p\u00e4isseet kurssin?\". Olettaen ett\u00e4 tapahtumat ovat riippumattomia, voimme laskea t\u00e4m\u00e4n seuraavasti:</p> \\[ P(B_1, B_2) = P(B_1) \\times P(B_2) = 0.75 \\times 0.75 = 0.5625 \\] <p>T\u00e4m\u00e4n voi helposti todistaa k\u00e4ym\u00e4ll\u00e4 l\u00e4pi kaikki kombinaatiot. Termi t\u00e4lle on tulojoukko tai karteesinen tulo (engl. Cartesian product).</p> IPython<pre><code>from itertools import product\n\nB1 = [1, 1, 1, 0]  # 3/4 = 0.75\nB2 = [1, 1, 1, 0]\n\nn = len(list(product(B1, B2)))\ncount = 0\n\nfor b1, b2 in product(B1, B2):\n    print(f\"{b1=}, {b2=} -&gt; {b1 and b2}\", end=\"\")\n    if b1 and b2:\n        count += 1\n        print(f\"  &lt;== {count}/{n}\")\n    else:\n        print()\n</code></pre> Klikkaa tuloste esiin <p>Koodissa tulostetaa yksi rivi per oppilas (B1,B2) kombinaatio. Tuloste on:</p> stdout<pre><code>b1=1, b2=1 -&gt; 1  &lt;== 1/16\nb1=1, b2=1 -&gt; 1  &lt;== 2/16\nb1=1, b2=1 -&gt; 1  &lt;== 3/16\nb1=1, b2=0 -&gt; 0\nb1=1, b2=1 -&gt; 1  &lt;== 4/16\nb1=1, b2=1 -&gt; 1  &lt;== 5/16\nb1=1, b2=1 -&gt; 1  &lt;== 6/16\nb1=1, b2=0 -&gt; 0\nb1=1, b2=1 -&gt; 1  &lt;== 7/16\nb1=1, b2=1 -&gt; 1  &lt;== 8/16\nb1=1, b2=1 -&gt; 1  &lt;== 9/16\nb1=1, b2=0 -&gt; 0\nb1=0, b2=1 -&gt; 0\nb1=0, b2=1 -&gt; 0\nb1=0, b2=1 -&gt; 0\nb1=0, b2=0 -&gt; 0\n</code></pre> <p>Kaikista vaihtoehdoista 9/16 on sellaisia, joissa molemmat opiskelijat ovat l\u00e4p\u00e4isseet kurssin. T\u00e4m\u00e4 on sama kuin laskettu todenn\u00e4k\u00f6isyys eli 0.5625.</p> <p>Teht\u00e4v\u00e4</p> <p>Katso, mit\u00e4 tulostuu, mik\u00e4li vaihdat yll\u00e4 olevasta koodista <code>and</code>-operaatiot <code>or</code>-operaatioiksi. Kysymys t\u00e4ll\u00f6in on, ett\u00e4: kuinka todenn\u00e4k\u00f6ist\u00e4 on, ett\u00e4 v\u00e4hint\u00e4\u00e4n toinen opiskelija on l\u00e4p\u00e4issyt kurssin?. </p> <p>Todenn\u00e4k\u00f6isyys ei suinkaan ole 3/4 + 3/4 eli 6/4. Prosentteina t\u00e4m\u00e4 olisi 150 %, mink\u00e4 pit\u00e4isi olla jo intuition perusteella selke\u00e4sti v\u00e4\u00e4r\u00e4 vastaus. Yksi tapa laskea todenn\u00e4k\u00f6isyys on summata <code>both</code> + <code>only_left</code> + <code>only_right</code>, jossa both on ylt\u00e4 tuttu <code>3/4 * 3/4</code>, ja kaksi muuta ovat kumpainenkin <code>3/4 * 1/4</code>. Toinen tapa on k\u00e4ytt\u00e4\u00e4 seuraavaa kaavaa:</p> \\[ P(B_1 \\text{ OR } B_2) = P(B_1) + P(B_2) - P(B_1, B_2) \\] <p>Yll\u00e4 olevat for-loopin lis\u00e4ksi voit laskea saman murtoluvuilla joko k\u00e4sin tai Pythonilla. T\u00e4st\u00e4 esimerkki alla:</p> IPython<pre><code>from fractions import Fraction\n\nb1 = Fraction(3, 4)\nb2 = ...\nboth = ...\neither = b1 + b2 - both\n</code></pre>"},{"location":"algoritmit/bayes/intuitio/#riippuvaiset-tapahtumat","title":"Riippuvaiset tapahtumat","text":"<p>Aiemmin esitellyt tapahtumat ovat kesken\u00e4\u00e4n riippumattomia. Kullakin satunnaisesti vastaan tulevalla opiskelijalla on sama 75 % mahdollisuus kuulua heihin, jotka ovat l\u00e4p\u00e4isseet kurssin. Kun arvioit vastaantulevan opiskelijan kurssin l\u00e4p\u00e4isyn mahdollisuutta, sinun arviointiisi voivat vaikuttaa kuitenkin useat seikat, kuten ett\u00e4 toisella opiskelijalla on kainalossaan The Pizza Bible -kirja. Pizzantekemiseen liittyvien kirjojen lukeminen oletettavasti vaikuttaa kurssin l\u00e4p\u00e4isemisen todenn\u00e4k\u00f6isyyteen.</p> \\[ \\begin{align*}     A &amp;= \\text{ has read the pizza bible } \\\\     B &amp;= \\text{ passed the pizza course } \\\\     P(B|A) &amp;= \\text{ 0.96 } \\end{align*} \\] <p>Lausemuodossa t\u00e4m\u00e4 on: \"Kurssin l\u00e4p\u00e4isemisen todenn\u00e4k\u00f6isyys, annettuna ett\u00e4 olet lukenut The Pizza Biblen, on 96 %.\" Kaavassa <code>B</code> on yh\u00e4 sama -kurssin l\u00e4p\u00e4isy. <code>A</code> on tapahtuman ehto. <code>P(B|A)</code> on tapahtuman todenn\u00e4k\u00f6isyys, annettuna ehto. Huomaa, ett\u00e4 t\u00e4ll\u00f6in tapahtumat eiv\u00e4t ole kesken\u00e4\u00e4n riippumattomia vaan nimenomaan riippuvaisia toisistaan.</p> <p>Tip</p> <p>Kuinka paljon kirjan lukeminen siis nostaa kurssin l\u00e4p\u00e4isyn todenn\u00e4k\u00f6isyytt\u00e4? T\u00e4m\u00e4 voidaan laskea seuraavasti:</p> \\[ \\frac{P(B|A)}{P(B)} = \\frac{0.96}{0.75} = 1.28 \\] <p>T\u00e4m\u00e4 tarkoittaa, ett\u00e4 The Pizza Bible kirjan lukeminen nostaa todenn\u00e4k\u00f6isyytt\u00e4 l\u00e4p\u00e4ist\u00e4 kurssi 28 %:lla.</p> Eth\u00e4n sekoita riippumattomia ja riippuvaisia tapahtumia? <p>Oletetaan, ett\u00e4 kirjan lukemisen todenn\u00e4k\u00f6isyys on 50 %. Puolet oppilaista ovat lukeneet kirjan.</p> \\[ P(A) = 0.5 \\] <p>Huomaa, ett\u00e4 et voi laskea n\u00e4iden lukujen avulla vastausta kysymykseen: \"Kuinka todenn\u00e4k\u00f6ist\u00e4 on, ett\u00e4 satunnainen opiskelija on l\u00e4p\u00e4issyt kurssin JA lukenut kirjan\" yksinkertaisella kertolaskulla eli:</p> \\[ P(B, A) = P(B) \\times P(A) = 0.75 \\times 0.5 = 0.375 \\] <p>K\u00e4ytetyn kaavan pit\u00e4isi ottaa huomioon, ett\u00e4 tapahtumat vaikuttavat toisiinsa. Yll\u00e4 oleva kaava toimii riippumattomien tapahtumien kanssa, mutta toisistaan riippuvien tapahtumien kanssa k\u00e4ytet\u00e4\u00e4n seuraavaa kaavaa:</p> \\[ P(B, A) = P(B|A) \\times P(A) = 0.96 \\times 0.5 = 0.48 \\] <p>Ent\u00e4p\u00e4 kysymys \"Kuinka todenn\u00e4k\u00f6ist\u00e4 on, ett\u00e4 satunnainen opiskelija on l\u00e4p\u00e4issyt kurssin TAI lukenut kirjan\"? T\u00e4m\u00e4 voidaan laskea seuraavasti, olettaen ett\u00e4 ujutat <code>P(A,B)</code>-tilalle yll\u00e4 n\u00e4kyv\u00e4n kaavan, jonka tulos on 0.48:</p> \\[ P(B \\text{ OR } A) = P(B) + P(A) - P(B,A) = 0.77 \\]"},{"location":"algoritmit/bayes/intuitio/#todennakoisyyden-selvittaminen","title":"Todenn\u00e4k\u00f6isyyden selvitt\u00e4minen","text":"<p>T\u00e4ysin tasapainoinen kolikko tai noppa on tyypillinen todenn\u00e4k\u00f6isyysmatematiikassa k\u00e4ytetty esimerkki. Arkiel\u00e4m\u00e4n ilmi\u00f6iss\u00e4 jonkin tapahtuman todenn\u00e4k\u00f6isyys pit\u00e4\u00e4 selvitt\u00e4\u00e4 havaintojen perusteella. Esimerkiksi, jos haluat selvitt\u00e4\u00e4, kuinka todenn\u00e4k\u00f6ist\u00e4 on, ett\u00e4 opiskelija l\u00e4p\u00e4isee kurssin, voit k\u00e4ytt\u00e4\u00e4 aiempia kurssisuorituksia. Yll\u00e4 todenn\u00e4k\u00f6isyydeksi on v\u00e4itetty 75 %:n todenn\u00e4k\u00f6isyytt\u00e4. Kuinka t\u00e4h\u00e4n lukuun ollaan p\u00e4\u00e4dytty? Kuvitellaan, ett\u00e4 t\u00e4m\u00e4 on selvitetty kyselytutkimuksella. Kyselyss\u00e4 on kysytty 32 opiskelijalta, ovatko he l\u00e4p\u00e4isseet kurssin.</p> <ul> <li>24 opiskelijaa vastasi kyll\u00e4</li> <li>8 opiskelijaa vastasi ei</li> </ul> <p>Todenn\u00e4k\u00f6isyys voidaan laskea seuraavasti:</p> \\[ P(B) = \\frac{n_{passed}}{n_{total}} = \\frac{24}{32} = 0.75 \\] <p>Warning</p> <p>Huomaathan, ett\u00e4 32 oppilaan otannalla ei voida varmuudella sanoa, ett\u00e4 todenn\u00e4k\u00f6isyys oikeasti on 75 %. N\u00e4in pienell\u00e4 otannalla virhemarginaali on suuri (jotakuinkin \u00b1 15 %, 95% luottamustasolla). N\u00e4in pienell\u00e4 otannalla on vain noin 50 % mahdollisuus, ett\u00e4 todellinen todenn\u00e4k\u00f6isyys on v\u00e4lill\u00e4 70 % - 80 %.</p> <p>Mit\u00e4 suurempi otanta, sit\u00e4 todenn\u00e4k\u00f6isemp\u00e4\u00e4 on, ett\u00e4 todenn\u00e4k\u00f6isyyslukema vastaa ilmi\u00f6n todellista todenn\u00e4k\u00f6isyytt\u00e4. Alla on esitetty kolmen eri otannan todenn\u00e4k\u00f6isyysjakaumat. Huomaa, ett\u00e4 mit\u00e4 suurempi otanta, sit\u00e4 kapeampi jakauma on.</p> <p></p> <p>Kuvio 1. Kolmen eri otannan todenn\u00e4k\u00f6isyysjakaumat. 32 opiskelijan otannalla jakauma on leve\u00e4mpi kuin 100 ja 200 opiskelijan otannoilla.</p> <p>Tilanne on sama kuin jos heit\u00e4t kolikko tai kuusitahoista noppaa. Yksitt\u00e4isell\u00e4 heitolla et voi sanoa, ett\u00e4 kolikon tai nopan todenn\u00e4k\u00f6isyys on 50 % tai 1/6. Mutta kun heit\u00e4t kolikkoa tai noppaa tarpeeksi monta kertaa, todenn\u00e4k\u00f6isyyslukema l\u00e4hestyy todellista todenn\u00e4k\u00f6isyytt\u00e4. Todellinen todenn\u00e4k\u00f6isyys ei v\u00e4ltt\u00e4m\u00e4tt\u00e4 ole tosiel\u00e4m\u00e4n kolikolla tai nopalla tasan 50 % tai tasan 1/6.</p> <p>Yll\u00e4 n\u00e4kyv\u00e4n Kuvio 1:n voit generoida seuraavalla koodilla:</p> IPython<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta\n\nsample_size = [32, 100, 200]\n\nplt.figure(figsize=(10, 6))\n\nfor n in sample_size:\n    # Compute passes and fails\n    passes = int(0.75 * n)\n    fails = n - passes\n\n    # Plot the Beta distribution\n    x = np.linspace(0, 1, 1000)\n    y = beta.pdf(x, passes, fails)\n    plt.plot(x, y, label=f'{n} students ({passes}/{fails} passes/fails)')\n\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"algoritmit/bayes/intuitio/#kaanteinen-todennakoisyys","title":"K\u00e4\u00e4nteinen todenn\u00e4k\u00f6isyys","text":"<p>T\u00e4h\u00e4n asti me tied\u00e4mme todenn\u00e4k\u00f6isyydet:</p> <ul> <li><code>P(A) = 0.5</code><ul> <li>The Pizza Bible kirjan lukemisen todenn\u00e4k\u00f6isyys</li> </ul> </li> <li><code>P(B) = 0.75</code> <ul> <li>-kurssin l\u00e4p\u00e4isemisen todenn\u00e4k\u00f6isyys</li> </ul> </li> <li><code>P(B|A) = 0.96</code><ul> <li>-kurssin l\u00e4p\u00e4isemisen todenn\u00e4k\u00f6isyys, annettuna ett\u00e4 olet lukenut The Pizza Bible kirjan.</li> </ul> </li> </ul> <p>Mutta mit\u00e4 jos haluamme selvitt\u00e4\u00e4, ett\u00e4: \"Kuinka todenn\u00e4k\u00f6ist\u00e4 on, ett\u00e4 opiskelija on lukenut kirjan, annettuna ett\u00e4 h\u00e4n on l\u00e4p\u00e4issyt kurssin?\". T\u00e4m\u00e4 voidaan laskea alla n\u00e4kyv\u00e4ll\u00e4 kaavalla, joka \u2014 kuinka ollakaan \u2014 on Bayesin teoreema:</p> \\[ P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)} = \\frac{0.96 \\times 0.5}{0.75} = 0.64 \\]"},{"location":"algoritmit/bayes/naive/","title":"Naive","text":"<p>P\u00e4\u00e4dyimme edellisess\u00e4 luvussa Bayesin teemassa muotoon, joka n\u00e4kyy alla. T\u00e4ll\u00e4 kertaa olemme tosin korvanneet <code>A</code>:n sanalla belief ja <code>B</code>:n sanalla evidence.</p> \\[ P(\\text{belief}|\\text{evidence}) = \\frac{P(\\text{evidence}|\\text{belief}) \\times P(\\text{belief})}{P(\\text{evidence})} \\] <p>T\u00e4m\u00e4n voisi kirjoittaa my\u00f6s seuraavalla tavalla:</p> \\[ \\text{posterior probability} = \\frac{\\text{likelihood} \\times \\text{prior probability}}{\\text{evidence}} \\] <p>Kuten scikit learn:n dokumentaatio sanoo, jakaja on datasetin suhteen vakio, joten sen voi j\u00e4tt\u00e4\u00e4 pois. Jakajan hylk\u00e4\u00e4minen tekee lopputuloksesta normalisoimattoman, mutta se ei ole ongelma, koska meit\u00e4 kiinnostaa vain suhteellinen todenn\u00e4k\u00f6isyys. Alla olevan kaavan kalan n\u00e4k\u00f6inen symboli luetaan \"proportional to\". T\u00e4m\u00e4 tarkoittaa, ett\u00e4 vasen puoli on suoraan verrannollinen oikean puolen tulon kanssa.</p> \\[ P(\\text{belief}|\\text{evidence}) \\propto P(\\text{evidence}|\\text{belief}) \\times P(\\text{belief}) \\] <p>Voimme my\u00f6s korvata kaavasta sanan <code>belief</code> sanalla <code>spam</code> ja sanan <code>evidence</code> useita datasetiss\u00e4 olevia sanoja korvaavalla <code>x_1, x_2, ..., x_n</code>.</p> \\[ P(spam|x_1, x_2, ..., x_n) \\propto P(x_1, x_2, ..., x_n|spam) \\times P(spam) \\]"},{"location":"algoritmit/bayes/naive/#naive-bayes-luokittelija","title":"Naive Bayes luokittelija","text":"<p>Naive Bayes on Bayesin teoreeman implementaatio, jossa tehd\u00e4\u00e4n naiivi oletus, ett\u00e4 yksitt\u00e4iset datapisteet eiv\u00e4t ole kesken\u00e4\u00e4n riippuvaisia. Eli sanan <code>Viagra</code> esiintyminen viestiss\u00e4 lis\u00e4\u00e4 todenn\u00e4k\u00f6isyytt\u00e4, ett\u00e4 viesti on roskapostia t\u00e4ysin riippumatta siit\u00e4, mit\u00e4 muita sanoja s\u00e4hk\u00f6posti sis\u00e4lt\u00e4\u00e4. Kukin sana osallistuu todenn\u00e4k\u00f6isyyteen itsen\u00e4isesti. T\u00e4m\u00e4n naiivin oletuksen kanssa kaava on:</p> \\[ \\hat{y} = \\arg\\max_{y} P(y) \\prod_{i=1}^{n} P(x_i|y) \\] <p>Saman voisi kirjoittaa Pythonina n\u00e4in:</p> IPython<pre><code>from math import prod\n\nDATASET = [\n    (\"Free Viagra now\", \"Spam\"),\n    (\"A game of golf tomorrow?\", \"Not spam\"),\n]\n\ndef probability_of_word_being_in_class(word, y_val):\n    \"\"\" P(evidence_i | y) \"\"\"\n\n    # N(x_i | y): (1)\n    n_docs = sum(\n        [\n            1 for sentence, y \n            in DATASET \n            if y == y_val and word in sentence.lower()\n        ]\n    )\n\n    # N(y): (2)\n    n_class = len([1 for x, y in DATASET if y == y_val])\n\n    # Laplace smoothing (3)\n    n_docs += 0.5\n    n_class += 1\n\n    return n_docs / n_class\n\ndef predict_class_probability(evidence, y_val):\n    # P(y) (4)\n    prior = sum(\n        [\n            1 for _, y \n            in DATASET \n            if y == y_val\n            ]\n    ) / len(DATASET)\n\n    # product[ P(evidence_i | y) ]\n    likelihood = prod(\n        [\n            probability_of_word_being_in_class(word, y_val) \n            for word in evidence.lower().split()\n        ]\n    )\n\n    return  likelihood * prior\n\n############ TESTILAUSE #############\nevidence = \"Free Viagra of something\"\n\nprobabilities = [\n    predict_class_probability(evidence, \"Not spam\")\n    predict_class_probability(evidence, \"Spam\"), \n]\n\ny_hat_prob, other_prob = max(probabilities), min(probabilities)\ny_hat = probabilities.index(y_hat_prob)\n\nprint(f\"Predicted class: {y_hat} (with probability {y_hat_prob} &gt; {other_prob})\")\n</code></pre> <ol> <li>Kyseisen sanan sis\u00e4lt\u00e4vien viestien m\u00e4\u00e4r\u00e4 luokassa [spam/no spam]</li> <li>Lauseiden m\u00e4\u00e4r\u00e4 datasetiss\u00e4, jotka eduustavat luokkaa [spam/no spam]</li> <li>Smoothing est\u00e4\u00e4 tilanteen, jossa jokin sana ei esiinny yhdess\u00e4k\u00e4\u00e4n viestiss\u00e4. T\u00e4ll\u00f6in todenn\u00e4k\u00f6isyys olisi 0, ja koko lasku menisi nollalla, koska mik\u00e4 tahansa kertaa 0 on 0.</li> <li>Luokan todenn\u00e4k\u00f6isyys [spam/no spam]. Eli mik\u00e4 osuus viesteist\u00e4 on esimerkiksi roskapostia.</li> </ol> <p>Tip</p> <p>Huomaa, ett\u00e4 toteutus on aivan \u00e4\u00e4rimm\u00e4isen yksinkertaistettu ja tarkoitettu pienten datasettien testailuun. Kest\u00e4v\u00e4mm\u00e4n toteutuksen l\u00f6yd\u00e4t kirjasta \"Data Science from Scratch\". Viel\u00e4 paremman toteutuksen l\u00f6yd\u00e4t scikit-learn kirjastosta.</p>"},{"location":"algoritmit/bayes/tehtavat/","title":"\ud83d\udcdd Teht\u00e4v\u00e4t","text":""},{"location":"algoritmit/bayes/tehtavat/#tehtava-vihapuheen-tunnistus","title":"Teht\u00e4v\u00e4: Vihapuheen tunnistus","text":"<p>Kouluta Naive Bayes classifier, joka tunnistaa, onko viesti vihapuhetta vai ei. Raportoi tulokset oppimisp\u00e4iv\u00e4kirjassasi. Valitse joko pienempi tai suurempi datasetti:</p> <ul> <li>Normaali teht\u00e4v\u00e4: gh: hate-speech-and-offensive-language</li> <li>Vaikea teht\u00e4v\u00e4: Data in Brief: suurempi datasetti</li> </ul> <p>Kuten aina, dokumentoi se, mit\u00e4 opit Naive Bayes algoritmista oppimisp\u00e4iv\u00e4kirjaasi.</p> <p>Huomaa, ett\u00e4 sinun ei odoteta k\u00e4ytt\u00e4v\u00e4n edistynytt\u00e4 stemmausta. Tarkoitus on ymm\u00e4rt\u00e4\u00e4, miten Naive Bayes toimii ja miten sit\u00e4 k\u00e4ytet\u00e4\u00e4n. Suosi yksinkertaisia ja ymm\u00e4rrett\u00e4vi\u00e4 menetelmi\u00e4.</p> <p>Warning</p> <p>Lue teht\u00e4v\u00e4nanto dataan liittyviss\u00e4 ongelmissa huolellisesti. Esimerkiksi yll\u00e4 on esitelty, ett\u00e4 sinun tulee tunnistaa, onko viesti vihapuhetta vai ei. Vastaus on siis \"Kyll\u00e4 on\" tai \"Ei ole\". \u00c4l\u00e4 siis tee luokitusta, jossa on useampia luokkia, kuten \"vihapuhe\", \"loukkaava puhe\", \"ei kumpikaan\".</p> <p>Sen sijaan voi olla perusteltua kokeilla, kuinka koulutetun mallin tarkkuus reagoi siihen, sis\u00e4llyt\u00e4tk\u00f6 loukkaavaksi puheeksi luokitellut viestit \"ei kumpikaan\"-luokkaan vai et.</p>"},{"location":"algoritmit/bayes/tehtavat/#vinkkeja","title":"Vinkkej\u00e4","text":""},{"location":"algoritmit/bayes/tehtavat/#kirjastot","title":"Kirjastot","text":"<p>Tulet tarvitsemaan <code>scikit-learn</code>-kirjastoa. Asenna se <code>uv add scikit-learn</code>-komennolla; se lis\u00e4t\u00e4\u00e4n <code>pyproject.toml</code>-tiedostoon. Tutustu ainakin seuraaviin importattuihin luokkiin tai metodeihin:</p> <pre><code>from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB, ComplementNB\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n</code></pre> <p>Huomaa, ett\u00e4 lista ei ole kattava. Kirjoitat oppimisp\u00e4iv\u00e4kirjaa, joten sinun on suositeltavaa tutustua vapaasti aihepiiriin. \u00c4l\u00e4 tyydy vastaamaan kysymykseen vaan ota omistajuus omasta oppimisprosessistasi.</p> <p>Tip</p> <p>Jos tutustut my\u00f6s <code>ml-perusteet-code</code>-repositorion koodiin \u2013 eli siis niihin Allure-testattuihin koodeihin \u2013, kannattaa pohtia, miten kyseisess\u00e4 opettajan tekem\u00e4ss\u00e4 from scratch -koodissa on hoidettu esimerkiksi tekstin vektorisointi. Onko k\u00e4yt\u00f6ss\u00e4 vastaava tekniikka kuin TfidVectorizer tai CountVectorizer? Tekeek\u00f6 malli sen itse vai onko se erillinen luokka/funktio kuten scikitin toteutuksessa?</p>"},{"location":"algoritmit/bayes/tehtavat/#datan-esikasittely","title":"Datan esik\u00e4sittely","text":"<p>Et voi sy\u00f6tt\u00e4\u00e4 dataa sin\u00e4ll\u00e4\u00e4n Naive Bayes -malliin. Sinun on ensin aivan v\u00e4himmill\u00e4\u00e4n muunnettava data numeeriseen muotoon, mutta kenties dataa kannattaa puhdistaa my\u00f6s muutoin? Voit esimerkiksi haluta:</p> <ul> <li>Poista erikoismerkit</li> <li>Tee tekstist\u00e4 lowercase</li> <li>Poista nimimerkit</li> <li>Poista yleisimm\u00e4t sanat (usein termill\u00e4 stop words)</li> <li>Korvaa urlit sanalla \"urlhere\" tai riisutulla domain-nimell\u00e4</li> </ul> <p>Eth\u00e4n tee t\u00e4t\u00e4 ensimm\u00e4isell\u00e4 kielimallin ehdottamalla, todenn\u00e4k\u00f6isesti raskaalla tavalla, vaan suosi jotakin, mink\u00e4 ymm\u00e4rr\u00e4t t\u00e4ysin. Voit tehd\u00e4 sen k\u00e4sin vaikkapa n\u00e4in:</p> <pre><code>import re\nimport pandas as pd\n\ndf = pd.read_csv('path_to_your_dataset.csv')\n\ndef preprocess_text(text):\n    # Replace URLs with the word \"urlhere\"\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"urlhere\", text, flags=re.MULTILINE)\n    # Rest of the processing\n    return text\n\ndf['cleaned_tweet'] = df['tweet'].apply(preprocess_text)\n</code></pre>"},{"location":"algoritmit/distance/kMeans/","title":"k-Means","text":"<p>Aiemmin esitelty\u00e4 kNN:\u00e4\u00e4 ja nyt k\u00e4sitelt\u00e4v\u00e4\u00e4 k-Means algoritmia yhdist\u00e4\u00e4 kirjain <code>k</code>, mutta niiden merkitys on eri. k-Meansin kohdalla <code>k</code> tarkoittaa <code>n_clusters</code> eli klustereiden m\u00e4\u00e4r\u00e4\u00e4, kun taas kNN:ss\u00e4 <code>k</code> tarkoittaa naapureiden m\u00e4\u00e4r\u00e4\u00e4. Toinen merkitt\u00e4v\u00e4 ero algoritmien v\u00e4lill\u00e4 on, ett\u00e4 k-Means ei ole luokittelualgoritmi, vaan klusterointialgoritmi eli siis ohjaamaton koneoppimisalgoritmi. Kukin havainto kuuluu siihen klusteriin, jonka keskipiste on l\u00e4himp\u00e4n\u00e4 havaintoa.</p>"},{"location":"algoritmit/distance/kMeans/#k-means-algoritmi","title":"k-Means algoritmi","text":"<p>k-Means on l\u00e4hes yht\u00e4 simppeli algoritmi kuin k-NN. Aivan kuten k-NN, my\u00f6s k-Meansi\u00e4 voi k\u00e4ytt\u00e4\u00e4 n-ulotteisessa avaruudessa, mutta visualisoinnin vuoksi k\u00e4yt\u00e4mme t\u00e4ss\u00e4 esimerkiss\u00e4 vain 2-ulotteista avaruutta. Koulutuksen vaiheet voidaan tiivist\u00e4\u00e4 seuraavasti:</p> <ol> <li>Valitse <code>k</code> klusteria, joihin <code>m</code> samplea klusteroidaan.</li> <li>Arvo satunnaisesti (x,y) koordinaatit <code>k</code> klusterikeskuksille.</li> <li>Nimit\u00e4 kukin piste kuuluvaksi l\u00e4himp\u00e4\u00e4n klusteriin.</li> <li>Laske keskiarvo (x,y) jokaiselle <code>m</code> havainnolle. Siirr\u00e4 klusterikeskukset n\u00e4ihin.</li> </ol> <p>Vaiheet 3-4 toistetaan kunnes klusterikeskukset kunnes klusterikeskukset eiv\u00e4t en\u00e4\u00e4 muutu (tai jokin muu lopetusehto t\u00e4yttyy).</p>"},{"location":"algoritmit/distance/kMeans/#vaiheet-kuvina","title":"Vaiheet kuvina","text":"<p>Alla esimerkki, joka on rautakoodattu toimimaan tasan 2-ulotteisessa avaruudessa.</p> <p>Data on t\u00e4ysin kuvitteellista, mutta mik\u00e4li se helpottaa, niin voit kuvitella x:n ja y:n arvojen taustalle jonkin ilmi\u00f6n. Esimerkiksi:</p> <ul> <li>x-akseli:  silm\u00e4lasien vahvuus (\u00b1 2.5)</li> <li>y-akseli:  ananas-pizzat\u00e4ytteen tykk\u00e4ysaste (\u00b1 2.5 \u03c3)</li> </ul> <p>Info</p> <p>T\u00e4ll\u00f6in datasetin oikean yl\u00e4laidan ryp\u00e4s, jonka keskipiste on noin \\((1, 1)\\), koostuu kaukon\u00e4k\u00f6isist\u00e4 -pizzan yst\u00e4vist\u00e4. Datasetin perusteella vaikuttaisi, ett\u00e4 ananaksesta tykk\u00e4\u00e4minen pizzan t\u00e4ytteen\u00e4 on kaukon\u00e4k\u00f6ist\u00e4 puuhaa. Kenties datasta puuttuu jokin merkitt\u00e4v\u00e4 piirre, kuten se, ett\u00e4 onko kyseess\u00e4 purkki- vai tuoreananas?</p>"},{"location":"algoritmit/distance/kMeans/#vaihe-1-valitse-k","title":"Vaihe 1: Valitse k","text":"<p>Valitaan klustereiden m\u00e4\u00e4r\u00e4 <code>k</code> eli <code>n_clusters</code>. T\u00e4ss\u00e4 esimerkiss\u00e4 valitaan <code>k=3</code>. Data on generoitu siten, ett\u00e4 me satumme tiet\u00e4m\u00e4\u00e4n klustereiden m\u00e4\u00e4r\u00e4n etuk\u00e4teen. T\u00e4ss\u00e4 yksinkertaisessa esimerkiss\u00e4 se olisi my\u00f6s n\u00e4ht\u00e4viss\u00e4 paljaalla silm\u00e4ll\u00e4 katsomalla pistekuvaajaa.</p>"},{"location":"algoritmit/distance/kMeans/#vaihe-2-arvo-satunnaiset-klusterikeskukset","title":"Vaihe 2: Arvo satunnaiset klusterikeskukset","text":"<p>Huomaa, ett\u00e4 alla olevan kuvaajan (Kuvio 1) pisteet kuuluvat <code>-1</code> klusteriin, mik\u00e4 piirret\u00e4\u00e4n t\u00e4ss\u00e4 tapauksessa harmaana pisteen\u00e4. Sen sijaan kullakin klusterilla on on id, joka on kokonaisluku <code>range(k)</code> eli t\u00e4ss\u00e4 tapauksessa <code>0, 1, 2</code>. Klusterien lokaatio on t\u00e4ysin satunnainen.</p> <p></p> <p>Kuvio 1: Klusterikeskukset satunnaisesti valittuna. Sallimme t\u00e4ss\u00e4 esimerkiss\u00e4 arvonnan k\u00e4ytt\u00e4\u00e4 mit\u00e4 tahansa arvoja v\u00e4lill\u00e4 <code>-3 ... 3</code> eli jopa pisteavaruuden ulkopuolelta.</p>"},{"location":"algoritmit/distance/kMeans/#vaihe-3-laske-etaisyydet","title":"Vaihe 3: Laske et\u00e4isyydet","text":"<p>Klusteriin kuuluvan pisteen m\u00e4\u00e4ritt\u00e4miseksi lasketaan et\u00e4isyys jokaisen klusterin keskipisteen ja pisteen v\u00e4lill\u00e4. Piste kuuluu siihen klusteriin, jonka keskipiste on l\u00e4himp\u00e4n\u00e4 pistett\u00e4. T\u00e4m\u00e4 tehd\u00e4\u00e4n alla olevalla koodilla:</p> IPython<pre><code>@dataclass\nclass Point:\n    x: float\n    y: float\n    cluster: int = -1\n\n@dataclass\nclass Centroid:\n    x: float\n    y: float\n    cluster: int\n\ndef assign_points_to_centroids(points: list[Point], centroids: list[Centroid]):\n\n    for point in points:\n        # Euclidean distances to the three centroids\n        distances = [\n            ((point.x - centroid.x)**2 + (point.y - centroid.y)**2)**0.5 \n            for centroid in centroids\n        ]\n\n        # Fetch the centroid with the smallest distance\n        indx = distances.index(min(distances))\n        closest = centroids[indx]\n\n        # Assign the point to the closest centroid\n        point.cluster = closest.cluster\n\n    return points\n</code></pre> <p>Tip</p> <p>Koska datalla ei ole varsinaisesti \"labelia\", k\u00e4yt\u00e4mme jatkossa sanaa \"cluster\" viittaamaan klusteriin, johon piste kuuluu, ja v\u00e4lttelemme sanan \"label\" k\u00e4ytt\u00f6\u00e4.</p>"},{"location":"algoritmit/distance/kMeans/#vaihe-4-laske-uudet-klusterikeskukset","title":"Vaihe 4: Laske uudet klusterikeskukset","text":"<p>Kun jokainen piste on m\u00e4\u00e4ritetty kuuluvaksi l\u00e4himp\u00e4\u00e4n klusteriin, lasketaan jokaisen klusterin pisteiden keskiarvo. T\u00e4m\u00e4 keskiarvo on uusi klusterikeskus. T\u00e4m\u00e4 tehd\u00e4\u00e4n alla olevalla koodilla:</p> IPython<pre><code>def compute_new_centroid_locations(points: list[Point], centroids: list[Centroid]):\n\n    for centroid in centroids:\n        # Subset of points matching the i'th cluster\n        cluster_points = [point for point in points if point.cluster == centroid.cluster]\n\n        # Mean of columns (x and y)\n        n = len(cluster_points)\n\n        # If this cluster was too far away from any points, move it to the middle of ALL points\n        # This is to avoid divizion by zero\n        if n == 0:\n            centroid.x = sum([point.x for point in points]) / len(points)\n            centroid.y = sum([point.y for point in points]) / len(points)\n            continue\n\n        new_x = sum([point.x for point in cluster_points]) / n\n        new_y = sum([point.y for point in cluster_points]) / n\n\n        # Stopping condition\n        if new_x == centroid.x and new_y == centroid.y:\n            print(f\"We have met the stopping condition.\")\n            return None\n\n        centroid.x = new_x\n        centroid.y = new_y\n\n    return centroids\n</code></pre> <p>Alla olevassa kuvassa (Kuvio 2) n\u00e4kyy klusterikeskukset ja niiden lokaatiot ensimm\u00e4isen iteraation j\u00e4lkeen. Klusterikeskukset ovat siirtyneet l\u00e4hemm\u00e4s klusteriin kuuluvia pisteit\u00e4. Huomaa, ett\u00e4 vihre\u00e4 klusteri (cluster 2) arvottiin ep\u00e4onniseen paikkaan, ja sit\u00e4 l\u00e4himp\u00e4n\u00e4 ei ollut yksik\u00e4\u00e4n piste. T\u00e4m\u00e4n vuoksi klusterikeskus siirtyi keskelle kaikkia pisteit\u00e4 (ks. yll\u00e4 olevasta koodista <code>n == 0</code>-kohta). </p> <p>Tip</p> <p>Vaihtoehtoinen tapa ratkaista ongelma olisi arpoa klusterikeskukset l\u00e4hemm\u00e4s kuuluvia pisteavaruuden keskustaa. Yksi monista tavoista on se, ett\u00e4 ottaa <code>random.sample(points, k)</code>-funktiolla k-kappaletta satunnaisia pisteit\u00e4 ja asettaa ne klusterikeskuksiksi.</p> <p></p> <p>Kuvio 2: Klusterikeskukset siirtyneet l\u00e4hemm\u00e4s klusteriin kuuluvia pisteit\u00e4. Ruksi edustaa vanhaa lokaatiota, pystyneli\u00f6 uutta lokaatiota.</p>"},{"location":"algoritmit/distance/kMeans/#vaihe-n-toista-kunnes-klusterikeskukset-eivat-enaa-muutu","title":"Vaihe N: Toista kunnes klusterikeskukset eiv\u00e4t en\u00e4\u00e4 muutu","text":"<p>Huomaa <code>compute_new_centroid_locations</code>-funktion palauttama <code>None</code>, joka kertoo, ett\u00e4 klusterikeskukset eiv\u00e4t en\u00e4\u00e4 muuttuneet. T\u00e4m\u00e4 on meid\u00e4n lopetusehto. Voimme siis kouluttaa algoritmin loppuun muutoin loppumattomassa silmukassa. J\u00e4rkev\u00e4 koodaaja laittaisi my\u00f6s <code>max_iter</code>-parametrin, joka est\u00e4isi silmukan jatkumisen loputtomiin. Se puuttuu t\u00e4st\u00e4 esimerkist\u00e4.</p> IPython<pre><code>while True:\n    # Take a deep copy for plotting purposes\n    prev_centroids = deepcopy(centroids)\n\n    # Compute \n    points = assign_points_to_centroids(points, centroids)\n\n    centroids = compute_new_centroid_locations(points, centroids)\n\n    if centroids is None:\n        break\n\n    plot_data(points, prev_centroids, centroids)\n</code></pre> Klikkaa plot_data koodi esiin IPython<pre><code>def plot_data(\n    points:list[Point], \n    centroids:list[Centroid], \n    future_centroids:list[Centroid]=None):\n\n    # Set up canvas\n    plt.figure(figsize=(8, 6))\n\n    color_mapping = {0: 'blue', 1: 'orange', 2: 'green', -1: 'grey'}\n\n    # Display the points\n    sns.scatterplot(\n        x=[point.x for point in points],\n        y=[point.y for point in points],\n        hue=[point.cluster for point in points],\n        palette=color_mapping,\n    )\n\n    # Display the centroids\n    sns.scatterplot(\n        x=[centroid.x for centroid in centroids],\n        y=[centroid.y for centroid in centroids],\n        hue=[centroid.cluster for centroid in centroids],\n        palette=color_mapping,\n        s=200,\n        marker=\"X\",\n        edgecolor=\"black\",\n        legend=False\n    )\n\n    if future_centroids is not None:\n        # Display the soon-to-be-centroid locations\n        sns.scatterplot(\n            x=[centroid.x for centroid in future_centroids],\n            y=[centroid.y for centroid in future_centroids],\n            hue=[centroid.cluster for centroid in future_centroids],\n            palette=color_mapping,\n            marker=\"D\", \n            s=100, \n            edgecolors=\"k\",\n            legend=False\n        )\n\n        # Plot dotted lines between current and future centroids\n        for i, centroid in enumerate(centroids):\n            plt.plot([centroid.x, future_centroids[i].x], \n                    [centroid.y, future_centroids[i].y], \n                    \"k--\", lw=1)\n\n    plt.show()\n</code></pre> <p>Alla olevassa kuvassa n\u00e4kyv\u00e4t kaikki iteraatiot (mukaan lukien aiempi), jotka n\u00e4ill\u00e4 l\u00e4ht\u00f6arvoilla k\u00e4ynnistetty silmukka tuotti.</p> <p></p> <p>Kuvio 3: Kaikki iteraatiot, jotka k-Means algoritmi tuotti. Jokainen klusterikeskus on merkitty ristill\u00e4 ja neli\u00f6ll\u00e4. Kannattaa avata kuva uudessa v\u00e4lilehdess\u00e4 suurempana.</p>"},{"location":"algoritmit/distance/kMeans/#evaluointi","title":"Evaluointi","text":"<p>Huomaa, ett\u00e4 todellisen klusteroinnin tapauksessa emme voi evaluoida mallin suoritusta vertaamalla sit\u00e4 oikeaan ratkaisuun, koska kyseess\u00e4 on ohjaamaton oppiminen. Jos meill\u00e4 olisi havaintojen luokat jo tiedossa, meill\u00e4 ei olisi syyt\u00e4 k\u00e4ytt\u00e4\u00e4 koko algoritmia. T\u00e4ss\u00e4 esimerkiss\u00e4 me kuitenkin tied\u00e4mme, mik\u00e4 on havaintojen alla piilev\u00e4 logiikka, joten voimme verrata ennustettuja pisteiden klustereita oikeisiin klustereihin.</p> <p>Koodina sen voi tehd\u00e4 k\u00e4yt\u00e4nn\u00f6ss\u00e4 n\u00e4in:</p> IPython<pre><code># Compare the labels to the predictions\ncorrect = 0\nfor a, b in zip(predicted_points, actual_points):\n    if a.cluster == b.cluster:\n        correct += 1\n\nprint(f\"Accuracy: {correct / len(points) * 100:.2f}%\")\n</code></pre> <p>Kuvana se n\u00e4ytt\u00e4isi t\u00e4lt\u00e4:</p> <p></p> <p>Kuvio 4: Vertailu alkuper\u00e4isten klustereiden ja ennustettujen klustereiden v\u00e4lill\u00e4. Vasemmalla on alkuper\u00e4inen datasetti, oikealla ennustettu, ja keskell\u00e4 on n\u00e4iden RGB difference.</p> <p>Warning</p> <p>Muista kuitenkin, ett\u00e4 \"accuracyn\" laskeminen klusteroinnin tapauksessa on hieman kyseenalaista monestakin syyst\u00e4. </p> <ul> <li>Ilmiselv\u00e4 syy on jo yll\u00e4 mainittu, eli jos k\u00e4yt\u00e4mme klusterointialgoritmia, niin meill\u00e4 ei ole oikeaa vastausta. </li> <li>Toinen syy on, ett\u00e4 vaikka meill\u00e4 onkin dummy-dataa, niin klustereiden j\u00e4rjestys on t\u00e4ysin sattumanvarainen ja riippuu siit\u00e4, minne klusterikeskukset arvotaan. Olisi siis t\u00e4ysin mahdollista, ett\u00e4 k-NN olisi arponut vasemman alalaidan klusterinkeskukseksi vihre\u00e4n klusterin.</li> <li>Me olisimme voineet p\u00e4\u00e4tt\u00e4\u00e4 segmentoida meid\u00e4n pisteet kolmen klusterin sijasta kahteen, nelj\u00e4\u00e4n tai johonkin muuhun <code>k=n</code>-m\u00e4\u00e4r\u00e4\u00e4n. N\u00e4m\u00e4 eiv\u00e4t olisi sen enemp\u00e4\u00e4 oikeita tai v\u00e4\u00e4ri\u00e4 vastauksia; me saisimme vain segmenttej\u00e4, joissa on enemm\u00e4n tai v\u00e4hemm\u00e4n varianssia.</li> </ul> <p>Teht\u00e4v\u00e4</p> <p>Tutustu my\u00f6s muihin klusterointialgoritmeihin, kuten <code>DBSCAN</code>, v\u00e4hint\u00e4\u00e4n pintaraapaisuna. Hyv\u00e4 paikka aloittaa on Scikit-Learn: Clustering.</p>"},{"location":"algoritmit/distance/knn/","title":"k-NN","text":"<p>Jos aiemmat algoritmit, Naive Bayes sek\u00e4 p\u00e4\u00e4t\u00f6spuut ja niist\u00e4 koostuvat mets\u00e4t, ovat tuntuneet yksinkertaisilta, niin k-Nearest Neighbors (k-NN) on viel\u00e4 yksinkertaisempi. Naive Bayesin sis\u00e4ltyy tilastollista vaikeutta, p\u00e4\u00e4t\u00f6spuiden monimutkaisuus tulee informaatiotieteen k\u00e4sitteist\u00e4 sek\u00e4 rekursiivisesta rakenteesta (eli Python-funktioista, jotka kutsuvat itse itse\u00e4\u00e4n). kNN on niin yksinkertainen, ett\u00e4 sen voi selitt\u00e4\u00e4 k\u00e4yt\u00e4nn\u00f6ss\u00e4 kahdella kuvalla. T\u00e4ss\u00e4 on ensimm\u00e4inen:</p>"},{"location":"algoritmit/distance/knn/#k-nn-algoritmi","title":"k-NN algoritmi","text":""},{"location":"algoritmit/distance/knn/#2-ulotteinen-avaruus","title":"2-ulotteinen avaruus","text":"<p>Kuvio 1: Kaksiulotteinen pistekaavio, jossa on kaksi luokkaa, sininen ja oranssi. Kuvassa on N pistett\u00e4, joista noin puolet ovat sinisi\u00e4 ja noin puolet punaisia. Kuvassa on my\u00f6s kaksi uutta pistett\u00e4, yksi sininen ja yksi oranssi.</p> <p>Yll\u00e4 olevassa kuvassa (Kuvio 1) on kaksi suuretta, toinen x-akselilla ja toinen y-akselilla. Pisteen v\u00e4ri on sen label.</p> <p>Algoritmi ei varsinaisesti parametrisoi mit\u00e4\u00e4n, vaan se tallentaa kaikki datan pisteet muistiin. Kun uusi piste tulee, se etsii <code>k</code> l\u00e4hint\u00e4 pistett\u00e4 ja katsoo, mit\u00e4 luokkaa ne ovat. T\u00e4m\u00e4 prosessi on demonstroitu seuraavassa kuvassa.</p> <p></p> <p>Kuvio 2: Uuden, aikaisemmin n\u00e4kem\u00e4tt\u00f6m\u00e4n havainnon luokittelu tehd\u00e4\u00e4n laskemalla et\u00e4isyys aivan kaikkiin datapisteisiin. Luokaksi annetaan l\u00e4himm\u00e4n kolmen koulutusdatassa olleen label (olettaen ett\u00e4 k=3).</p> <p>Uusi piste saa saman luokan kuin sen l\u00e4himmill\u00e4 pisteill\u00e4 on. Algoritmin toimintaperiaate on siis \u00e4\u00e4rimm\u00e4isen simppeli: \"Katso, keiden vieress\u00e4 seisot, ja tied\u00e4t, kuka olet!\". Luokittelu perustuu siis t\u00e4ysin euklidiseen et\u00e4isyyteen. T\u00e4ss\u00e4 (ks. Kuvio 2) tapauksessa <code>k</code> on 3, joten uusi sininen piste saa oranssin luokan, koska sen kaksi kolmesta l\u00e4himm\u00e4st\u00e4 pisteest ovat oransseja. Luku <code>k</code> on siis hyperparametri, joka m\u00e4\u00e4ritt\u00e4\u00e4, kuinka monta l\u00e4hint\u00e4 pistett\u00e4 otetaan huomioon, ja se on aina pariton luku.</p>"},{"location":"algoritmit/distance/knn/#n-ulottoinen-avaruus","title":"N-ulottoinen avaruus","text":"<p>Huomaa, ett\u00e4 k-NN toimii my\u00f6s N-ulotteisessa avaruudessa.</p> <p></p> <p>Kuvio 3: Kolmiulotteinen pistekaavio ei eroa 2-ulotteisesta muuten kuin siin\u00e4, ett\u00e4 siin\u00e4 on kolmas ulottuvuus, joka otetaan et\u00e4isyytt\u00e4 laskiessa huomioon. Nelj\u00e4s ulottuvuus olisi huomattavan vaikea visualisoida.</p> <p>Teht\u00e4v\u00e4</p> <p>Tutki, mihin viittaa termi \"curse of dimensionality\". Mit\u00e4 et\u00e4isyydelle tapahtuu n-ulotteisessa avaruudessa, kun n kasvaa?</p>"},{"location":"algoritmit/distance/knn/#pythonilla","title":"Pythonilla","text":"<p>Huomaa, ett\u00e4 mallin kouluttaminen on vain datan tallentamista. Mallin k\u00e4ytt\u00e4minen ennustamiseen on vain et\u00e4isyyksien laskemista ja sorttaamista. T\u00e4m\u00e4 tekee k-NN:st\u00e4 eritt\u00e4in nopean algoritmin pienell\u00e4 datalla, mutta eritt\u00e4in hitaan suurella datalla (ainakin ilman optimointeja). Kaava et\u00e4isyyden laskemiseen on seuraava:</p> \\[ d(p,q) = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + \\ldots + (p_n - q_n)^2} \\] <p>Eli pisteiden \\((4,3)\\) ja \\((6,3)\\) et\u00e4isyys on:</p> \\[ d((4,3), (6,3)) = \\sqrt{(4-6)^2 + (3-3)^2} = \\sqrt{(-2)^2 + 0^2} = \\sqrt{4} = 2 \\] <p>Oletetaan, ett\u00e4 meill\u00e4 on seuraavanlainen datasetti, joka koostuu n-dimensionaalisista pisteist\u00e4 (<code>Point([1, 2, 3, ..., n_val], label=\"sininen\")</code>). T\u00e4m\u00e4 nykyinen toteutus koostuu tasan 2-ulotteisista pisteist\u00e4. Katso datasetin muodostus koodista alta:</p> IPython<pre><code>from dataclasses import dataclass\n\n@dataclass\nclass Point:\n    features: list[float]\n    label: str|None\n\n@dataclass\nclass Neighbor:\n    distance: float\n    label: str\n\ndata = [\n    Point([1, 2], 'sininen'),\n    Point([2, 3], 'sininen'),\n    Point([3, 4], 'sininen'),\n    Point([4, 5], 'oranssi'),\n    Point([5, 6], 'oranssi'),\n    Point([6, 7], 'oranssi'),\n]\n</code></pre> <p>Antaen t\u00e4m\u00e4n datan, voimme ennustaa ennen n\u00e4kem\u00e4tt\u00f6m\u00e4n pisteen <code>(x=4.5, y=5.5)</code> luokan seuraavasti:</p> IPython<pre><code>from collections import defaultdict\n\ndef euclidean_distance(a:Point, b:Point):\n    if len(a.features) != len(b.features):\n        raise ValueError('Points must have the same number of dimensions')\n    return sum((x - y)**2 for x, y in zip(a.features, b.features))**0.5\n\ndef majority_vote(neighbors:list[Neighbor]):\n    votes = defaultdict(int)\n    for neighbor in neighbors:\n        votes[neighbor.label] += 1\n    return max(votes, key=votes.get)\n\ndef predict(data:list[Point], new_point:Point, k:int, verbose=True):\n    distances = []\n    for point in data:\n        closest = Neighbor(euclidean_distance(point, new_point), point.label)\n        distances.append(closest)\n    distances.sort(key=lambda x: x.distance)\n    closest_k = distances[:k]\n\n    if verbose:\n        print(f\"The {k} closest neighbors are:\")\n        for neighbor in closest_k:\n            print(f\"Distance: {neighbor.distance:.2f}, Label: {neighbor.label}\")\n\n    # Count the votes and return most common\n    return majority_vote(closest_k)\n\n\nnew_point = Point([4.5, 5.5], None)\nprint(predict(data, new_point, k=3))\n</code></pre>"},{"location":"algoritmit/distance/knn/#vertailu-muihin-algoritmeihin","title":"Vertailu muihin algoritmeihin","text":"<p>T\u00e4m\u00e4 on ensimm\u00e4inen algoritmi, jonka kanssa olemme tarkastelleen pistekuvaajaa (scatter plot). Nyt on hyv\u00e4 aika pys\u00e4hty\u00e4 ja mietti\u00e4, kuinka muut kurssilla mainitut luokittelualgoritmin jakaisivat samaa avaruutta eri luokkiin.</p>"},{"location":"algoritmit/distance/knn/#decision-tree","title":"Decision Tree","text":"<p>Jos kouluttaisit yll\u00e4 n\u00e4kyneell\u00e4 (Kuvio 1) datalla Decision Tree, mieti, kuinka avaruutta jaettaisiin kunkin kysymyksen kohdalla. Kenties kysymykset olisivat?</p> <ul> <li>Onko x &gt; 3.8?<ul> <li>T\u00e4m\u00e4 jakaisi graafin vasen-oikeaan siten, ett\u00e4 vasemmalla puolella olisi vain 3 oranssia, oikealla vain 4 sinist\u00e4.</li> </ul> </li> </ul> <p>Jos x oli t\u00e4t\u00e4 suurempi, kysymys olisi kenties:</p> <ul> <li>Onko y &gt; 4.3?<ul> <li>T\u00e4m\u00e4 jakaisi graafin yl\u00f6s-alas siten, ett\u00e4 ylh\u00e4\u00e4ll\u00e4 olisi vain 3 oranssia, alhaalla pelkki\u00e4 sinisi\u00e4.</li> </ul> </li> </ul> <p>Decision Tree jakaisi siis avaruutta kahteen osaan, ja niit\u00e4 taas kahteen osaan, kunnes lopulta laatikkoon kuuluu vain yhdenv\u00e4risi\u00e4 pisteit\u00e4. Diagonaalinen jako on siis ongelmallinen, joten Decision Tree ei ole paras valinta t\u00e4h\u00e4n ongelmaan ilman ett\u00e4 datalle suoritetaan jokin muunnos, kuten dimensionaalisuuden v\u00e4hent\u00e4minen (PCA).</p>"},{"location":"algoritmit/distance/knn/#naive-bayes","title":"Naive Bayes","text":"<p>Naive Bayes sen sijaan arvioi kunkin pisteen kohdalla, kuinka todenn\u00e4k\u00f6ist\u00e4 on, ett\u00e4 uusi havainto kuuluu luokkaan kunkin sen piirteen perusteella. Esimerkiksi havainnon x-piirteen arvo 6 on merkitt\u00e4v\u00e4sti todenn\u00e4k\u00f6isempi oranssi kuin sininen, koska oransseja on enemm\u00e4n korkeilla x-arvoilla.</p>"},{"location":"algoritmit/distance/knn/#kaikki-visualisoituna","title":"Kaikki visualisoituna","text":"<p>Algoritmien ero tulee selv\u00e4ksi, kun piirr\u00e4mme kuvaajaan kunkin algoritmin luokittelurajat. Kuviossa 3 on esitetty Naive Bayes, Decision Tree ja k-NN:n luokittelurajat k\u00e4ytt\u00e4en dataa, joka on luotu Scikit Learn:n <code>make_circles</code>-funktiolla.</p> <p></p> <p>Kuvio 4: Naive Bayes, Decision Tree ja k-NN:n luokittelurajat. Naive Bayes on ympyr\u00e4n muotoinen, Decision Tree on jaettu kuin Yhdysvallat osavaltioihin, ja k-NN on ep\u00e4s\u00e4\u00e4nn\u00f6llinen. Kannattaa klikata kuva auki uuteen v\u00e4lilehteen, jotta se n\u00e4kyy suurempana.</p>"},{"location":"algoritmit/distance/tehtavat/","title":"\ud83d\udcdd Teht\u00e4v\u00e4t","text":""},{"location":"algoritmit/distance/tehtavat/#tehtava-automaattivaiheet-pt-2","title":"Teht\u00e4v\u00e4: Automaattivaiheet (Pt. 2)","text":"<p>K\u00e4yt\u00e4 samaa datasetti\u00e4 kuin viime viikolla. Kouluta k-NN-luokittelumalli, joka ennustaa, onko kyseess\u00e4 automaattiauto. </p> <p>Huomaa, ett\u00e4 k-NN on et\u00e4isyyksiin perustuva algoritmi. Muistahan siis normalisoida tai standardoida data ennen koulutusta! </p>"},{"location":"algoritmit/distance/tehtavat/#vinkit","title":"Vinkit","text":""},{"location":"algoritmit/distance/tehtavat/#k-arvo","title":"k-arvo","text":"<p>Kokeile eri k-arvoja ja dokumentoi, mik\u00e4 k-arvo tuottaa parhaan tuloksen. T\u00e4ss\u00e4 auttaa ns. elbow method, jossa piirret\u00e4\u00e4n k-arvojen ja mallin tarkkuuden v\u00e4linen k\u00e4yr\u00e4.</p>"},{"location":"algoritmit/distance/tehtavat/#hyodynna-vanhaa-tietoa","title":"Hy\u00f6dynn\u00e4 vanhaa tietoa","text":"<p>Muistathan vertailla mallin suorituskyky\u00e4 aiemmalla viikolla kouluttamaasi Decision Tree tai Random Forest -malliin.</p>"},{"location":"algoritmit/distance/tehtavat/#tehtava-varikartta","title":"Teht\u00e4v\u00e4: V\u00e4rikartta","text":"<p>Teht\u00e4v\u00e4 on luoda skripti, joka luo Tikkurilan v\u00e4rikartan kartaisen \\(k\\) v\u00e4rin kartan. Idea on, ett\u00e4 jos haluaisit maalata huoneen annetun kuvan v\u00e4reill\u00e4, malli klusteroi v\u00e4rit, joita sinun tulisi k\u00e4ytt\u00e4\u00e4. T\u00e4m\u00e4n viikon aikana sinun tulee tutustua RGB- ja HSV-v\u00e4rimalleihin, jos ne eiv\u00e4t ole sinulle entuudestaan tuttuja. Koneoppimisessa on kovin tyypillist\u00e4, ett\u00e4 sinun tulee tutustua uusiin k\u00e4sitteisiin, ja ymm\u00e4rt\u00e4\u00e4, miten ne vaikuttavat mallin toimintaan. Jos k\u00e4site on aivan vieras, voit aloittaa vaikkapa v\u00e4rislidereit\u00e4 s\u00e4\u00e4t\u00e4m\u00e4ll\u00e4 esimerkiksi colorizer.org -sivustolla. </p> <p>Skriptin luomaa v\u00e4rikarttaa voisi my\u00f6hemmin k\u00e4ytt\u00e4\u00e4 esimerkiksi:</p> <ul> <li>Huoneen v\u00e4riteeman m\u00e4\u00e4rittelyyn (vrt. Tikkurilan v\u00e4rikartat)</li> <li>Web-sivuston graafisessa ohjeistuksessa</li> <li>Elokuvan v\u00e4rim\u00e4\u00e4rittelyn ohjenuorana</li> </ul>"},{"location":"algoritmit/distance/tehtavat/#esimerkki","title":"Esimerkki","text":"<p>Kuva 1: Terry Kearneyn kuva, otsikolla 5 A Day, on tekij\u00e4noikeusvapaa kuva ja se on ladattavissa Flickr-palvelusta: Terry Kearney: 5 A Day.</p> <p>Jos k\u00e4ytt\u00e4j\u00e4 valitsee yll\u00e4 n\u00e4kyv\u00e4n kuvan (Kuva 1) ja <code>k=4</code> arvon, syntyy alla n\u00e4kyv\u00e4 liuska (Kuva 2).</p> <p></p> <p>Kuva 2: Yll\u00e4 n\u00e4kyv\u00e4n Kuvio 1:n v\u00e4rit jaettuna nelj\u00e4\u00e4n klusteriin RGB:t\u00e4 k\u00e4ytt\u00e4en. T\u00e4st\u00e4 naiivista esimerkist\u00e4 puuttuu vihre\u00e4 kokonaan. Kenties sen kuuluisi mahtua kyytiin?</p> <p>Tip</p> <p>Sivutuotteena mallin luokkia voi k\u00e4ytt\u00e4\u00e4 siihen, ett\u00e4 luot posterisoidun version valokuvasta. T\u00e4m\u00e4 efekti, posterize, l\u00f6ytyy tyypillisist\u00e4 kuvank\u00e4sittelyty\u00f6kaluista kuten Photoshop tai GIMP. Voit tutustua GIMP:n dokumentaatiosta Color Tools: 5.9 Posterize.</p> <p></p> <p>Kuva 3: Posterisoitu kuva kasattuna takaisin (200,200) muotoon. Jos k\u00e4yt\u00e4t RGB-arvoja, lopputulos ei v\u00e4ltt\u00e4m\u00e4tt\u00e4 edusta sit\u00e4, kuinka ihminen jakaisi v\u00e4rit. Miss\u00e4 on kuvasta esimerkiksi oikean yl\u00e4laidan vihre\u00e4t hedelm\u00e4t? Ehk\u00e4 Hue-arvoa pit\u00e4isi painottaa?</p>"},{"location":"algoritmit/distance/tehtavat/#vinkit_1","title":"Vinkit","text":""},{"location":"algoritmit/distance/tehtavat/#opencv","title":"OpenCV","text":"<p>Voit k\u00e4ytt\u00e4\u00e4 kuvank\u00e4sittelyyn valitsemaasi valmista kirjastoa kuten <code>PIL</code> tai <code>opencv</code>. Lis\u00e4\u00e4 kirjasto tutulla <code>uv add opencv-python</code> komennolla. Alla lyhyt esimerkki OpenCV:n k\u00e4yt\u00f6st\u00e4:</p> <pre><code>import cv2\nimport matplotlib as plt\n\nimg = cv2.imread(\"to/path/fruit.jpg\")\n\n# OpenCV uses RGB channel ordering. Pyplot assumes RGB.\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n# Resize image to make calculations faster.\nimg = cv2.resize(img, (200, 200))\n\n# In dataset, each pixel is an observation\nh, w, _ = img.shape\nX = img.reshape((h * w, 3))\n\n# Display the image\nplt.show(img)\n</code></pre>"},{"location":"algoritmit/distance/tehtavat/#rgb-vs-hsv","title":"RGB vs. HSV","text":"<p>RGB on yleinen tapa esitt\u00e4\u00e4 v\u00e4rit, mutta se ei v\u00e4ltt\u00e4m\u00e4tt\u00e4 ole paras valinta, kun haluat luokitella v\u00e4rej\u00e4. </p> <p>Kenties haluat harkita esimerkiksi HSV:t\u00e4 (Hue, Saturation, Value)? Huomaa, ett\u00e4 pelkk\u00e4 Hue-arvon k\u00e4ytt\u00f6 ei v\u00e4ltt\u00e4m\u00e4tt\u00e4 johda haluttuun lopputulokseen. Esimerkiksi punainen ja vaaleanpunainen ovat pelkk\u00e4\u00e4 Hue-lukemaa tuijottaen sama v\u00e4ri (eli vaaleanpunainen on punaisen s\u00e4vy, engl. tint). Voi kuitenkin olla j\u00e4rkev\u00e4\u00e4 painottaa Hue-arvoa enemm\u00e4n kuin Saturation- ja Value-arvoja. Kuinka tekisit t\u00e4m\u00e4n?</p> <p>Muutos tapahtuu seuraavanlaisella snippetill\u00e4:</p> <pre><code>hsv_img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n</code></pre> <p>Huomaa, ett\u00e4 kuva pysyy samana: vain esitystapa muuttuu.</p>"},{"location":"algoritmit/distance/tehtavat/#hsv-syklisena","title":"HSV syklisen\u00e4","text":"<p>Hue on luonteeltaan syklinen ja sen lukema edustaa asteita. T\u00e4m\u00e4 tarkoittaa, ett\u00e4 asteet 0 ja 360 ovat sama v\u00e4ri. N\u00e4m\u00e4 skaalan \u00e4\u00e4rip\u00e4\u00e4t ovat sama punaisen s\u00e4vy. Koska k-Means perustuu et\u00e4isyyksiin, t\u00e4m\u00e4 aiheuttaa ongelmia l\u00e4heisyysmittauksessa.  Kannattaa lukea lis\u00e4\u00e4 esimerkiksi Medium-artikkelista Axel Kud: Why We Need Encoding Cyclical Features.</p> <p>Vertaus kelloon</p> <p>Tilanne on sama kuin jos vuorokaudenajan kuvaisi tunteina: 0 tuntia ja 24 tuntia ovat sama aika, eli keskiy\u00f6. T\u00e4st\u00e4 huolimatta ihmisen kokemana aikana ajat <code>[23, 1]</code> ovat l\u00e4hemp\u00e4n\u00e4 toisiaan kuin vaikkapa <code>[8,16]</code>. Jos ajan ymm\u00e4rt\u00e4\u00e4 lineaarisena arvona, tilanne on toinen: klo 23 ja 01 ovat 22 tunnin p\u00e4\u00e4ss\u00e4 toisistaan.</p> <p>Yksi ratkaisu t\u00e4h\u00e4n on tehd\u00e4 Hue-arvosta kaksi erillist\u00e4 piirrett\u00e4: <code>sin_hue</code> ja <code>cos_hue</code>. T\u00e4m\u00e4 syklinen enkoodaus onnistuu seuraavanlaisesti Pythonissa:</p> <pre><code>H_max = 180 # may vary\n\ndf['sin_hue'] = np.sin(df['hue'] * (2 * np.pi / H_max))\ndf['cos_hue'] = np.cos(df['hue'] * (2 * np.pi / H_max))\n</code></pre> <p>Arvo <code>H_max</code> on tyypillisesti 360, mutta arvo voi riippua k\u00e4ytetyst\u00e4 kirjastosta. Esimerkiksi OpenCV k\u00e4ytt\u00e4\u00e4 Hue-arvoa, joka on v\u00e4lill\u00e4 0-179 (jotta se mahtuu 8-bittiseen arvoon).</p>"},{"location":"algoritmit/distance/tehtavat/#rgb-vs-lab","title":"RGB vs. LAB","text":"<p>Voit tutustua my\u00f6s LAB- eli <code>CIE L*a*b*</code>-v\u00e4rimalliin, joka on suunniteltu siten, ett\u00e4 se vastaa jossain m\u00e4\u00e4rin ihmisen v\u00e4rin\u00e4k\u00f6\u00e4. Dimensiot <code>a</code> ja <code>b</code> kuvastavat skaaloja <code>punainen-vihre\u00e4</code> ja <code>sininen-keltainen</code>. T\u00e4m\u00e4 tarkoittaa, ett\u00e4 LAB-mallissa v\u00e4rit ovat jo valmiiksi kahdessa eri ulottuvuudessa, joten et\u00e4isyysmittaus toimii suoraan. <code>L</code> arvo on kirkkaus. Voit muuntaa RGB:n LAB:iin OpenCV:n avulla seuraavasti:</p> <pre><code>lab_img = cv2.cvtColor(img, cv2.COLOR_RGB2Lab)\n</code></pre>"},{"location":"algoritmit/distance/tehtavat/#kokeile-eri-kuvilla","title":"Kokeile eri kuvilla","text":"<p>Lis\u00e4haasteena on kokeilla skripti\u00e4 hieman v\u00e4hemm\u00e4n v\u00e4rikk\u00e4ille kuville. Vihjeen\u00e4 voit yritt\u00e4\u00e4 luoda filtterin, joka p\u00e4\u00e4st\u00e4\u00e4 vain tietyt ehdot t\u00e4ytt\u00e4v\u00e4t pikselit k-Means algoritmille asti. Kenties HSV:n arvot Saturation ja Value auttavat?</p> <p>Ideaalitilanne on, ett\u00e4 jos sy\u00f6t\u00e4t mustataustaisen kuvan, jossa on 5 selke\u00e4sti eri v\u00e4rist\u00e4 palleroa, musta ei tule valituksi vaan v\u00e4ripallerot. Kenties t\u00e4ss\u00e4 voisi auttaa jokin maskifiltteri, joka ignooraa todella tummat tai vaaleat pikselit, tai alle 50 % saturaation arvot?</p>"},{"location":"algoritmit/linear/gradient_descent/","title":"Gradient descent","text":"<p>Edellisess\u00e4 luvussa k\u00e4sittelimme Hill Climbing algoritmia. Algoritmin koulutus koostui n-m\u00e4\u00e4r\u00e4st\u00e4 iteraatioita, joissa kussakin arvottiin sattumanvaraiset muutokset painoille ja laskettiin niiden perusteella mallin virhe. Eik\u00f6 oliskin varsin k\u00e4tev\u00e4\u00e4, jos satunnaisuuden sijasta voisimme laskea, mihin suuntaan painoja tulisi muuttaa. T\u00e4m\u00e4n meille mahdollistaa Gradient Descent algoritmi!</p>"},{"location":"algoritmit/linear/gradient_descent/#vertailu-hill-climbingiin","title":"Vertailu Hill Climbingiin","text":"<p>Gradient Descent -algoritmin koulutuksen vaiheet my\u00f6t\u00e4ilev\u00e4t Hill Climbing -algoritmin vaiheita, mutta painojen muutokset lasketaan tavalla, joka esitell\u00e4\u00e4n t\u00e4ss\u00e4 dokumentissa. Verrataan n\u00e4it\u00e4 vaiheita Hill Climbing -algoritmin vastaaviin taulukkomuodossa.</p> <p>Muistutuksena aiemmat vaiheet olivat:</p> <ol> <li>Alusta kertoimet satunnaisesti</li> <li>Laske virhe</li> <li>Lis\u00e4\u00e4 kertoimiin satunnaisluku (v\u00e4lill\u00e4 \u00b11.0)</li> <li>Laske virhe</li> <li>Jos virhe pienenee, hyv\u00e4ksy muutos</li> <li>Toista 3-5 kunnes pys\u00e4ytyskriteeri t\u00e4yttyy</li> </ol> Vaihe Hill Climbing Gradient Descent 1 Alusta kertoimet satunnaisesti Alusta kertoimet satunnaisesti 2 Ennusta ja laske MSE/SSE Ennusta ja laske MSE/SSE 3 Lis\u00e4\u00e4 kertoimiin satunnaisluku (v\u00e4lill\u00e4 \u00b11.0) Laske gradientti 4 Laske virhe Kerro gradientti oppimisnopeudella 5 Hyv\u00e4ksy tai hylk\u00e4\u00e4 uudet kertoimet P\u00e4ivit\u00e4 kertoimet 6 Toista 3-5 kunnes pys\u00e4ytyskriteeri t\u00e4yttyy Toista 2-5 kunnes pys\u00e4ytyskriteeri t\u00e4yttyy <p>Huomaa, ett\u00e4 vaiheet ovat p\u00e4\u00e4osin samat, mutta seuraavan termit saattavat olla sinulle vieraita: osittaisderivaatta, gradientti ja oppimisnopeus. N\u00e4ist\u00e4 kaksi ensimm\u00e4ist\u00e4 ovat lainatut integraalilaskennasta. T\u00e4m\u00e4 ei ole matematiikan kurssi, joten emme derivoi yht\u00e4k\u00e4\u00e4n funktiota k\u00e4sin emmek\u00e4 t\u00e4ten tarvitse derivointis\u00e4\u00e4nt\u00f6j\u00e4. Keskitymme ilmi\u00f6n ymm\u00e4rt\u00e4miseen intuition tasolla.</p>"},{"location":"algoritmit/linear/gradient_descent/#kulmakertoimen-selvittaminen","title":"Kulmakertoimen selvitt\u00e4minen","text":""},{"location":"algoritmit/linear/gradient_descent/#diabetes-ja-deltametodi","title":"Diabetes ja deltametodi","text":"<p>Tutustumme algoritmin toimintaan Scikit-learn kirjaston Diabetes datasetin avulla, joka sis\u00e4lt\u00e4\u00e4 10 piirrett\u00e4 (age, sex, bmi, ...). Aloitamme arpomalla kertoimet satunnaisesti (Vaihe 1). Kertoimiksi tai painoiksi valikoituvat alla n\u00e4kyv\u00e4ss\u00e4 taulukossa olevat arvot. Mallin ennuste teht\u00e4isiin n\u00e4iden painojen avulla (<code>y = w1*x1 + w2*x2 + ... + w10*x10</code>). Vektori <code>w</code> on tuttuun tapaan yht\u00e4 pitk\u00e4 kuin havaintomatriisin kukin rivi eli featureiden m\u00e4\u00e4r\u00e4 (+ bias).</p> age sex bmi bp s1 ... 0.50 -0.14 0.65 1.52 -0.23 ... <p>Tutustutaan kulmakerotimeen yhden piirteen avulla. T\u00e4m\u00e4 piirre on <code>weights[2]</code> eli bmi eli painoindeksi (engl. body mass index). Pid\u00e4mme toistaiseksi muut piirteet vakiona. N\u00e4in voimme laskea, kuinka mallin virhe (MSE) muuttuu, kun vaihdetaan arvoa valitulla v\u00e4lill\u00e4, joka on t\u00e4ss\u00e4 esimerkiss\u00e4 etuk\u00e4teen p\u00e4\u00e4tetty v\u00e4li <code>-100</code> ja <code>180</code>. T\u00e4m\u00e4 vaihteluv\u00e4li on valittu siten, ett\u00e4 graafista tulisi t\u00e4ss\u00e4 tapauksessa mukavan symmetrinen.</p> IPython<pre><code>def mse_by_weight_range(feature_index, X, y, weights, values):\n    mse_values = []\n    weights_copy = weights.copy()\n\n    for value in values:\n        weights_copy[feature_index] = value\n        y_hat = X.dot(weights_copy)\n        mse = mean_squared_error(y, y_hat)\n        mse_values.append(mse)\n\n    return mse_values\n\nfeature_index = 2                      # bmi\nvalues = np.linspace(-100, 180, 100)   # x-axis values\nmse_values = mse_by_weight_range(\n    feature_index, X, y, weights, values\n)\n</code></pre> <p>Jos palautuneista <code>mse_values</code>-arvoista piirret\u00e4\u00e4n kuvaaja, saadaan paraabeli, joka kuvaa virheen muutosta <code>bmi</code>-arvon painokertoimen muuttuessa.</p> <p></p> <p>Kuvio 1: Mallin virheen muutos painoindeksin painokertoimen muuttuessa. Nykyinen arvottu painokerroin, <code>0.65</code>, on merkattu kuvaajaan punaisena katkoviivana.</p> <p>Warning</p> <p>Huomaa, ett\u00e4 vaikka kuvaajan perusteella vaikuttaa, ett\u00e4 <code>weights[2] == 45</code> minimoi virheen, niin olisi ep\u00e4optimaalista loikata suoraan kuopan pohjalle. Yht\u00e4 muuttujaa arvioidessa muut pidet\u00e4\u00e4n vakiona - mutta k\u00e4yt\u00e4nn\u00f6ss\u00e4 ne vaikuttavat kokonaisuuteen. Suunta on kuitenkin todenn\u00e4k\u00f6isesti oikea. T\u00e4t\u00e4 varten oppiminen tapahtuu pienin askelin. T\u00e4m\u00e4 oppimisen nopeus on <code>learning_rate</code> ja se esitell\u00e4\u00e4n my\u00f6hemmin.</p> <p></p> <p>Kuvio 2: Kuvio 1:n l\u00e4hikuvaaja, jossa n\u00e4kyy paremmin virheen muutos. Huomaa, ett\u00e4 x-akseli kattaa nyt vain arvot <code>0.6-0.7</code></p> <p></p> <p>Kuvio 3: Kuvio 2:n l\u00e4hikuvaaja, jossa esitell\u00e4\u00e4n pieni delta (<code>+ 0.01</code>), ja sen vaikutus virheeseen. Virhefunktion kulmakertoimen voisi laskea my\u00f6s n\u00e4in. Pieni delta on valittu sattumanvaraisesti.</p> <p>Huomaa, ett\u00e4 vaikka Kuviossa 3 k\u00e4yr\u00e4 n\u00e4ytt\u00e4\u00e4 ihmissilm\u00e4lle suoralta, se on yh\u00e4 kaareva. Mit\u00e4 pienemm\u00e4n deltan avulla laskemme muutoksen, sit\u00e4 tarkemman arvon saamme kulmakertoimesta. Alla olevassa taulukossa n\u00e4kyy kulmakertoimen laskeminen eri deltoilla. Ensimm\u00e4inen sarake on siis askeleen koko, joka otetaan Kuvion 3 x-akselilla. Valitut askeleet ovat kymmenesosia toisistaan (<code>10 ** -2</code>, <code>10 ** -3</code>, <code>10 ** -4</code>, <code>10 ** -5</code>).</p> Weight delta MSE delta Slope 0.01000 -0.88411 -88.410579813 0.00100 -0.08842 -88.419579813 0.00010 -0.00884 -88.420479769 0.00001 -0.00088 -88.420569591 <p>Info</p> <p>Derivointi on seuraavan otsikon aihe, mutta otetaan aikahyppy tulevaisuuteen toistaiseksi vieraan <code>magic()</code>-funktion toiminnallisuuden avulla. Toistaiseksi riitt\u00e4\u00e4, ett\u00e4 hyv\u00e4ksyt, ett\u00e4 <code>slope = magic()</code>-rivi palauttaa kulmakertoimen kunkin havainnon n\u00e4k\u00f6kulmasta. Kyseess\u00e4 on osittaisderivaatta <code>bmi</code>-muuttujan suhteen.</p> IPython<pre><code>def magic(X, y, weights):\n    y_hat = X.dot(weights)\n    slope = 2 * X.T.dot(y_hat - y) / len(X)\n    return slope\n\nslope = magic(X, y, weights)[feature_index]\nprint(f\"Slope is: {slope:.9f}\")\n</code></pre> stdout<pre><code>Slope is: -88.420579813\n</code></pre>"},{"location":"algoritmit/linear/gradient_descent/#feikkidata-ja-derivointi","title":"Feikkidata ja derivointi","text":"<p>Diabetes-datasetiss\u00e4 on merkitt\u00e4v\u00e4 m\u00e4\u00e4r\u00e4 muuttujia (10 kpl). Vaihdetaan yhden muuttujan ja kahdeksan havainnon <code>X.shape == (8, 1)</code> esimerkkiin matematiikan helpottamiseksi. Kun <code>X:\u00e4\u00e4n</code> lis\u00e4t\u00e4\u00e4n bias, sen muodoksi tulee <code>(8, 2)</code>. Alla koodi, jolla data on generoitu, ja X_bias-matriisi taulukkona.</p> IPython<pre><code>import numpy as np\n\n# Generate reproducible noise\nnp.random.seed(42)\nnoise_delta = 0.05\nnoise = np.random.normal(0, noise_delta, 8).round(2)\n\n# Data\nX = np.array(list(range(8))).reshape(8, -1)\n\n# Target (1)\ny = -0.25*X + 1.5\ny = y.flatten() + noise\n\n# Add bias\nX = np.c_[np.ones(X.shape[0]), X]\n\n# Manually set wrong weights (2)\nw = np.array([-0.5, 0.5])\n\n# Predict y_hat\ny_hat = np.dot(X, w)\n</code></pre> <ol> <li>Huomaa, ett\u00e4 <code>y</code> on generoitu suoraan kaavalla <code>y = -0.25x + 1.5</code>.</li> <li>Painot ovat valittu sattumanvaraisesti. Oikeat painot olisivat <code>[1.5, -0.25]</code>, koska <code>y = -0.25x + 1.5</code>.</li> </ol> Havainnon # Bias (x_0) Feat (x_1) y y_hat Ensimm\u00e4inen 1.0 0.0 1.52 -0.50 Toinen 1.0 1.0 1.24 0.00 Kolmas 1.0 2.0 1.03 0.50 Nelj\u00e4s 1.0 3.0 0.83 1.00 Viides 1.0 4.0 0.49 1.50 Kuudes 1.0 5.0 0.24 2.00 Seitsem\u00e4s 1.0 6.0 0.08 2.50 Kahdeksas 1.0 7.0 -0.21 3.00 <p>Note</p> <p>Huomaa, ett\u00e4 emme skaalaa piirteit\u00e4, koska t\u00e4m\u00e4 on yksinkertainen esimerkki. Eth\u00e4n toimi n\u00e4in oikeassa el\u00e4m\u00e4ss\u00e4!</p> <p>Ennuste on luonnollisesti v\u00e4\u00e4r\u00e4, koska painot ovat hyvin kaukana siit\u00e4, mit\u00e4 niiden pit\u00e4isi olla. Piirre itsess\u00e4\u00e4n tulisi kertoa <code>-0.25</code>:lla, mutta se kerrotaan <code>0.5</code>:lla. Bias tulisi kertoa <code>1.5</code>:lla, mutta se kerrotaan <code>-0.5</code>:lla. T\u00e4m\u00e4 on hyv\u00e4 esimerkki siit\u00e4, miten painot vaikuttavat ennusteeseen.</p> <p></p> <p>Kuvio 4: Kuvaaja, jossa on feikkidatan datapisteet sinisin\u00e4 ympyr\u00f6in\u00e4, matemaattinen ideaali punaisena viivana, ja ennuste sinisen\u00e4 viivana. Ennuste on merkitty sinisell\u00e4 viivalla ja oikeat arvot punaisilla pisteill\u00e4.</p>"},{"location":"algoritmit/linear/gradient_descent/#verifioidaan-deltametodilla","title":"Verifioidaan deltametodilla","text":"<p>Voimme laskea kulmakertoimen yll\u00e4 opitulla tavalla, eli tehd\u00e4\u00e4n pieni muutos painokertoimeen ja lasketaan virheen muutos. T\u00e4m\u00e4 laskenta on alla piilotetussa solussa.</p> Koodi: Kulmakertoimen laskenta IPython<pre><code>def mse(y, y_hat):\n    return np.mean((y - y_hat)**2)\n\ndef compute_slope_using_delta(X, y, w, delta=0.0000001):\n    slopes = {}\n    for i in range(len(w)):\n        w_delta = w.copy()\n        w_delta[i] += delta\n        y_hat = predict(X, w_delta)\n        slope = (mse(y, y_hat) - mse(y, predict(X, w))) / delta\n        slopes[i] = round(slope, 2)\n    return slopes\n\ncompute_slope_using_delta(X_bias, y, w)\n</code></pre> stdout<pre><code>{0: 1.2, 1: 12.01}\n</code></pre> <p>Tulokseksi syntyy luvut: 1.20 ja 12.01. Ensimm\u00e4inen luku on biasin kulmakerroin ja toinen on piirteen kulmakerroin. Tarvitsemme n\u00e4it\u00e4 jatkossa varmentaaksemme derivoinnin oikeellisuuden.</p>"},{"location":"algoritmit/linear/gradient_descent/#ssemsen-osittaisderivaatat","title":"SSE/MSE:n osittaisderivaatat","text":"<p>Sen sijaan, ett\u00e4 derivoisimme virhefunktion k\u00e4sin, k\u00e4yt\u00e4mme sympy-kirjastoa. K\u00e4yt\u00e4mme virhefunktiona derivoidessa neli\u00f6virheiden summaa (SSE, Sum of Squared Errors). T\u00e4st\u00e4 voi my\u00f6hemmin muuttaa MSE:n jakamalla sen havaintojen m\u00e4\u00e4r\u00e4ll\u00e4. Kyseisen kirjaston <code>.diff()</code>-metodi derivoi funktion annetun muuttujan suhteen. T\u00e4ss\u00e4 tapauksessa meill\u00e4 on kaksi muuttujaa, painokertoimet <code>w0</code> ja <code>w1</code>.</p> IPython<pre><code>import sympy as sp\n\ndef solve_partial_derivative_formula():\n\n    y, w0, w1, x0, x1 = sp.symbols('y, w0, w1, x0, x1')\n\n    # Sum of squared errors\n    cost = (y - (w0 * x0 + w1 * x1))**2 # (1)\n\n    print(cost.diff(w0).simplify())\n    print(cost.diff(w1).simplify())\n\nsolve_partial_derivative_formula()\n</code></pre> <ol> <li>Virhefunktio on neli\u00f6summa. Jos haluaisimme t\u00e4m\u00e4n keskiarvon eli MSE:n, lis\u00e4isimme jakolaskun <code>m</code>:ll\u00e4, jossa <code>m</code> on havaintojen m\u00e4\u00e4r\u00e4. Me teemme t\u00e4m\u00e4n my\u00f6hemm\u00e4ss\u00e4 vaiheessa.</li> </ol> stdout<pre><code>2*x0*(w0*x0 + w1*x1 - y)\n2*x1*(w0*x0 + w1*x1 - y)\n</code></pre> <p>Tip</p> <p>Joissakin materiaaleissa SSE/MSE puolitetaan, jotta derivaatasta putoaa <code>2 *</code> pois. Jos n\u00e4et kirjallisuudessa kaavan kyseisess\u00e4 muodossa, t\u00e4ss\u00e4 on syy.</p> <p>Nyt kun tied\u00e4mme derivaatan kaavan, voimme luoda funktion, joka suorittaa kyseisen laskennan.</p> IPython<pre><code>def partial_derivative_full_sse(w, x, y, feature_index, observation_index):\n    # With respect to w0 or w1\n    # as in: 2*x0*(w0*x0 + w1*x1 - y)\n    #    or: 2*x1*(w0*x0 + w1*x1 - y)\n\n    # Long form\n    # derivative = 2 * x[feature_index] * (w[0] * x[0] + w[1] * x[1] - y[observation_index])\n\n    # Short form\n    return 2 * x[feature_index] * (np.dot(w, x) - y[observation_index])\n\nfor obs_i, x in enumerate(X):\n    part_0 = partial_derivative_full_sse(w, x, y, 0, obs_i)\n    part_1 = partial_derivative_full_sse(w, x, y, 1, obs_i)\n\n    print(f\"Observation {i} {x[0]=} {x[1]=} =&gt; {part_0=:.2f}, {part_1=:.2f}\")\n</code></pre> <p>Raa'an tulosteen sijasta esit\u00e4n saman datan taulukkomuodossa, jotta sit\u00e4 on helpompi tulkita. Taulukossa on rivein\u00e4 yksitt\u00e4isten havaintojen derivaatat sek\u00e4 n\u00e4ist\u00e4 koostettu (engl. aggregrated) tulos. Kun t\u00e4m\u00e4 tulos jaetaan havaintojen m\u00e4\u00e4r\u00e4ll\u00e4, saadaan keskiarvo (engl. mean) ja t\u00e4m\u00e4 on osittaisderivaatta.</p> X Derivative w_0 Derivative w_1 [1, 0] -4.04 -0.00 [1, 1] -2.48 -2.48 [1, 2] -1.06 -2.12 [1, 3] 0.34 1.02 [1, 4] 2.02 8.08 [1, 5] 3.52 17.60 [1, 6] 4.84 29.04 [1, 7] 6.42 44.94 :::: :::: :::: TOTAL 9.55 96.08 MEAN (total/8) 1.19 12.01 <p>Tip</p> <p>Huomaa, ett\u00e4 derivaatta tarkoittaa yksinkertaisesti kulmakerrointa. Yhden yksik\u00f6n muutos painokertoimeen <code>w_0</code> aiheuttaa SSE-virheen muutoksen <code>9.55</code>, jos kaikki muu pidet\u00e4\u00e4n vakiona. Sama muutos <code>w_1</code>:een aiheuttaa virheen muutoksen <code>96.08</code>. MSE:n kohdalla n\u00e4m\u00e4 jaettaisiin yksinkertaisesti havaintojen m\u00e4\u00e4r\u00e4ll\u00e4.</p> <p>N\u00e4ist\u00e4 osittaisderivaatoista p\u00e4\u00e4semmekin kiinni termiin <code>gradientti</code>. Gradientti on vektori, joka koostuu osittaisderivaatoista. Eli siis jos otat yll\u00e4 olevasta taulukosta luvut <code>[9.55, 96.08]</code> ja muodostat niist\u00e4 vektorin, se on gradientti SSE:n suhteen. Jos otat taulukosta luvut <code>[1.19, 12.01]</code>, se on gradientti MSE:n suhteen. Gradientti on siis virhefunktion kulmakerroin kunkin painokertoimen suhteen.</p> <p>Note</p> <p>Huomaa, ett\u00e4 n\u00e4m\u00e4 arvot ovat py\u00f6ristysvirheit\u00e4 lukuunottamatta samat kuin yll\u00e4 deltametodilla lasketut arvot.</p>"},{"location":"algoritmit/linear/gradient_descent/#msen-vektorimuotoinen-gradientti","title":"MSE:n vektorimuotoinen gradientti","text":"<p>Silmukoiden sijaan voimme hy\u00f6dynt\u00e4\u00e4 matriisilaskentaa ja laskea gradientin joko muutamassa vaiheessa tai jopa yhdell\u00e4 rivill\u00e4. Alla on kumpikin muoto esiteltyn\u00e4.</p> IPython<pre><code># Note that the derivate is computed as:\n#\n#    multiply each feature...\n#     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# 2 * x[feature_index] * (np.dot(w, x) - y[observation_index])\n#                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n#                               ...with the residuals\ny_hat = X @ w\nresiduals = y_hat - y\nsum_of_partials = 2 * X.T @ residuals\n\n# We need to divide by the number of observations to get the mean\ngradient = sum_of_partials / len(y)\nprint(\"The gradient: \", gradient)\n\n#### OR #######\n# ONE-LINER ! #\n###############\ngradient = 2 * X.T @ (X @ w - y) / len(y)\nprint(\"The gradient with one-liner: \", gradient)\n</code></pre> stdout<pre><code>The gradient:  [ 1.195 12.01 ]\nThe gradient with one-liner:  [ 1.195 12.01 ]\n</code></pre> <p>Nyt kun olemme laskeneet gradientin, voimme hy\u00f6dynt\u00e4\u00e4 sit\u00e4. Meid\u00e4n pit\u00e4\u00e4 kuitenkin viel\u00e4 k\u00e4sitell\u00e4 aiheen viimeinen osa: oppimisnopeus.</p>"},{"location":"algoritmit/linear/gradient_descent/#oppimisnopeus","title":"Oppimisnopeus","text":""},{"location":"algoritmit/linear/gradient_descent/#kakku-metsan-pohjalla","title":"Kakku mets\u00e4n pohjalla","text":"<p>Kuvittele valtava, pime\u00e4 mets\u00e4. On yleisesti tiedossa, ett\u00e4 mets\u00e4n syvimm\u00e4ss\u00e4 kohdassa on kakku . Ja sin\u00e4 tietenkin haluat sen! Teht\u00e4v\u00e4 olisi helppo, jos sinulla olisi kartta ja kompassi. N\u00e4it\u00e4 sinulla ei ole, joten joudut luottamaan aisteihisi.</p> <p>K\u00e4yt\u00e4t navigointiin taskulamppua . Taskulampun paristot ovat v\u00e4hiss\u00e4, joten haluat k\u00e4ytt\u00e4\u00e4 sit\u00e4 mahdollisimman harvoin. Siisp\u00e4 valitset strategian: pys\u00e4hdyt paikoillesi, laitat valon p\u00e4\u00e4lle, ja arvioit, miss\u00e4 suunnassa on alam\u00e4ki. Sammutat valon, k\u00e4velet jonkin matkaa alam\u00e4en suuntaan, ja toistat prosessin.</p> <p></p> <p>Kuvio 5: Kuva henkil\u00f6st\u00e4 navigoimassa pime\u00e4ss\u00e4 mets\u00e4ss\u00e4 taskulampun avulla. DALL-E 3:n n\u00e4kemys.</p> <p>Kun laitat valot p\u00e4\u00e4lle ja arvioit alam\u00e4en suunnan, saat tiet\u00e4\u00e4 <code>gradientin</code>. Kenties rinne viett\u00e4\u00e4 (lat, lon) -suunnassa <code>1.195</code> ja <code>12.01</code> -asteen kulmassa. Kun alat l\u00e4hesty\u00e4 alam\u00e4en syvint\u00e4 kohtaa, m\u00e4ki oletettavasti loiventuu. Mutta kuinka kaukana olet syvimm\u00e4st\u00e4 kohdasta? 100 metri\u00e4? 600 kilometri\u00e4? T\u00e4t\u00e4 et voi tiet\u00e4\u00e4, joten sinun pit\u00e4\u00e4 yksinkertaisesti arvata, kuinka pitk\u00e4n matkan k\u00e4velet taskulampun k\u00e4yt\u00f6n j\u00e4lkeen. T\u00e4m\u00e4 arvaus on oppimisnopeus (engl. learning rate).</p> <p>Question</p> <p>Jos oppimisnopeus on k\u00e4velymatka, niin mik\u00e4 t\u00e4ss\u00e4 kuvitteellisessa esimerkiss\u00e4 olisi hyv\u00e4 vertauskuva virheelle? Kenties kaukana n\u00e4kyvien puunlatvojen korkeus suhteessa sinuun? Tai ehk\u00e4p\u00e4 mets\u00e4n pohjalla olevan kakun hajun voimakkuus?</p> <p>Lopulta koulutus on yksinkertaista: laske gradientti, kerro se oppimisnopeudella, ja p\u00e4ivit\u00e4 painot. Toista kunnes pys\u00e4ytyskriteeri t\u00e4yttyy. </p> IPython<pre><code># This is what we should choose.\nlearning_rate = 0.01\n\nfor _ in range(n_epochs):    \n        gradient = compute_gradient(X, y, w, m)\n\n        # Compute the step : a vector containing step sizes per each weight\n        step = learning_rate * gradient\n\n        # Change the weights based on the step size\n        w -= step\n\n        # Here you would implement a stopping criterion\n</code></pre> <p>Alla 2000 epookin yli koulutettu gradient descent animaationa. Data on yll\u00e4 k\u00e4ytetty\u00e4 feikkidataa. Jos valitset liian suuren oppimisnopeuden, voit ylitt\u00e4\u00e4 kakun ja joutua takaisin mets\u00e4n reunaan. Jos valitset liian pienen oppimisnopeuden, taskulampustasi loppuu paristot ennen kuin l\u00f6yd\u00e4t kakun.</p> <p></p> <p>Kuvio 6: Animaatio Gradient Descent -algoritmin toiminnasta. Valittu learning rate on <code>0.01</code>. Virhe on esitetty SSE:n\u00e4, jotta se ei l\u00e4hestyisi yht\u00e4 nopeasti nollaa kuin MSE.</p> <p></p> <p>Kuvio 7: Kuvaaja, jossa on esitetty virheen muutos koulutuksen aikana 2000 epookin yli. Virhe on esitetty SSE:n\u00e4.</p> <p></p> <p>Kuvio 8: Kuvaaja, jossa on esitetty virhefunktion contour plot, mink\u00e4 viivat muistuttaa mukavasti kartan korkeusk\u00e4yri\u00e4. Kuva on eri esimerkist\u00e4, joten akseleiden arvot ovat v\u00e4\u00e4r\u00e4t.</p>"},{"location":"algoritmit/linear/gradient_descent/#lisaa-opiskeltavaa","title":"Lis\u00e4\u00e4 opiskeltavaa","text":"<p>T\u00e4m\u00e4 materiaali on vain pintaraapaisu Gradient Descent -algoritmiin. Tutustu my\u00f6s seuraaviin aiheisiin:</p> <ul> <li>Dynaamisen oppimisnopeuden k\u00e4ytt\u00f6. Oppimisnopeutta voidaan muuttaa koulutuksen aikana. Voit k\u00e4ytt\u00e4\u00e4 esimerkiksi aluksi suurempaa oppimisnopeutta ja pienent\u00e4\u00e4 sit\u00e4 koulutuksen edetess\u00e4.</li> <li>L1 tai L2 regularisaatio. Regularisaatio on tekniikka, joka auttaa est\u00e4m\u00e4\u00e4n ylifittauksen. L1 ja L2 ovat kaksi yleisint\u00e4 regularisaatiotyyppi\u00e4. Esimerkiksi L2 regularisaatio on yksinkertaisimmillaan sit\u00e4, ett\u00e4 virhefunktioon lis\u00e4t\u00e4\u00e4n <code>cost = cost + reg_rate * np.sum(w**2)</code>, jossa <code>reg_rate</code> on regularisaatiokertoimen arvo. Lis\u00e4\u00e4t siis virheeseen painojen neli\u00f6iden summan eli rankaiset mallia siit\u00e4, jos painot yritt\u00e4v\u00e4t poiketa liikaa nollasta (eli ovat suuria tai pieni\u00e4).</li> <li>Stochastic Gradient Descent. T\u00e4ss\u00e4 esimerkiss\u00e4 k\u00e4ytettiin koko datasetti\u00e4 gradientin laskemiseen. T\u00e4m\u00e4 ei ole realistista, jos dataa on aivan valtavia m\u00e4\u00e4ri\u00e4. Stokastinen Gradient Descent k\u00e4ytt\u00e4\u00e4 yht\u00e4 havaintoa kerrallaan.</li> <li>Mini-Batch Gradient Descent. T\u00e4m\u00e4 on kompromissi kahden edellisen v\u00e4lill\u00e4. Et k\u00e4yt\u00e4 kaikki tai vain yht\u00e4 havaintoa, vaan esimerkiksi 64 satunnaisesti valittua havaintoa kerrallaan.</li> </ul>"},{"location":"algoritmit/linear/hill_climbing/","title":"Hill Climbing","text":"<p>Edellisess\u00e4 luvussa k\u00e4ytimme normaaliyht\u00e4l\u00f6\u00e4, joka sovittaa suoran dataan yhden kaavan ratkaisuna. T\u00e4ss\u00e4 luvussa k\u00e4sitell\u00e4\u00e4n optimointia toisella tavalla: Hill Climbing -algoritmilla, joka pyrkii etsim\u00e4\u00e4n ratkaisua iteratiivisesti eli toistamalla itse\u00e4\u00e4n silmukassa. Hill Climbing on tuskin tuotantok\u00e4ytt\u00f6\u00f6n soveltuva algoritmi, mutta se on hyv\u00e4 esimerkki siit\u00e4, miten optimointi voidaan toteuttaa, ja toimii pohjustuksena Gradient Descent -algoritmille, joka on yleisempi ja tehokkaampi optimointimenetelm\u00e4. Sit\u00e4 k\u00e4sitell\u00e4\u00e4n seuraavassa luvussa.</p>"},{"location":"algoritmit/linear/hill_climbing/#hill-climbing_1","title":"Hill Climbing","text":"<p>Optimointiin voi k\u00e4ytt\u00e4\u00e4 yll\u00e4 olevan kaavan sijasta eri koneoppimisen keinoja, joista yksi on Hill Climbing. Hill Climbing on yksinkertainen algoritmi, joka pyrkii l\u00f6yt\u00e4m\u00e4\u00e4n paikallisen maksimin tai minimin. Se toimii seuraavasti:</p> <ol> <li>Alusta kertoimet satunnaisesti</li> <li>Laske virhe</li> <li>Lis\u00e4\u00e4 kertoimiin satunnaisluku (v\u00e4lill\u00e4 \u00b11.0)</li> <li>Laske virhe</li> <li>Jos virhe pienenee, hyv\u00e4ksy muutos</li> <li>Toista 3-5 kunnes pys\u00e4ytyskriteeri t\u00e4yttyy</li> </ol> <p>Luonnollinen pys\u00e4ytyskriteeri on brute force -menetelm\u00e4n vuoksi <code>max_iter</code> eli iteraatioiden maksimim\u00e4\u00e4r\u00e4.</p> <p>T\u00e4m\u00e4n yksinkertaisen metodin sijasta voisimme k\u00e4ytt\u00e4\u00e4 Gradient Descent -algoritmia, joka on tehokkaampi ja yleisempi menetelm\u00e4 optimointiin. Gradient Descent on algoritmi, joka pyrkii l\u00f6yt\u00e4m\u00e4\u00e4n virhefunktion minimin iteratiivisesti derivoimalla funktion ja liikkumalla vastakkaiseen suuntaan gradientin suhteen pieni askel kerrallaan. T\u00e4h\u00e4n tutustutaan hyvin pintapuolisesti seuraavassa luvussa, jossa k\u00e4sitell\u00e4\u00e4n n-uloitteista lineaarista regressiota (engl. multivariate linear regression).</p> <p>Note</p> <p>Huomaa, ett\u00e4 yll\u00e4 olevalla algoritmilla on mit\u00e4tt\u00f6m\u00e4t mahdollisuudet onnistua, jos piirteit\u00e4 ei ole normalisoitu. T\u00e4m\u00e4 johtuu siit\u00e4, ett\u00e4 suuret arvot voivat dominoivat virhefunktiota ja est\u00e4\u00e4 algoritmin l\u00f6yt\u00e4m\u00e4st\u00e4 optimaalista ratkaisua. Lis\u00e4ksi valittu satunnaisluku on todenn\u00e4k\u00f6isesti liian pieni, mik\u00e4li piirteet edustavat suuria arvoja (kuvittele MSE, jos kentt\u00e4 sis\u00e4lt\u00e4\u00e4 lukuja, kuten <code>233_535.124</code>)</p>"},{"location":"algoritmit/linear/hill_climbing/#ongelman-esittely","title":"Ongelman esittely","text":"<p>Kerrataan viel\u00e4 t\u00e4rke\u00e4t termit:</p> <ul> <li>Virhefunktio. Virhefunktio tai tappiofunktio mittaa mallin ennusteen virheellisyytt\u00e4. Meille on jo aiemmin tullut tutuksi MSE eli keskim\u00e4\u00e4r\u00e4inen neli\u00f6virhe.</li> <li>Optimointi. Optimointi on prosessi, jossa pyrit\u00e4\u00e4n minimoimaan yll\u00e4 mainittua virhefunktiota. T\u00e4ss\u00e4 luvussa k\u00e4yt\u00e4mme Hill Climbing -algoritmia.</li> <li>Parametrit. Parametrit ovat mallin kertoimia, jotka m\u00e4\u00e4ritt\u00e4v\u00e4t mallin k\u00e4ytt\u00e4ytymisen. Esimerkiksi lineaarisessa regressiossa kertoimet <code>a</code> ja <code>b</code> m\u00e4\u00e4ritt\u00e4v\u00e4t suoran kulmakertoimen ja vakiotermin. N\u00e4m\u00e4 painot m\u00e4\u00e4ritt\u00e4v\u00e4t mallin ennusteen. Yll\u00e4 oleva <code>.fit(X, y)</code> laskee n\u00e4m\u00e4 kertoimet.</li> </ul>"},{"location":"algoritmit/linear/hill_climbing/#parametrit-w","title":"Parametrit (W)","text":"<p>Keskityt\u00e4\u00e4n hetkeksi parametreihin eli painokertoimiin. Aiemmassa luvussa k\u00e4sittelimme yksinkertaista lineaarista regressiota, jossa malli oli muotoa \\(y = b + wx\\) (engl. univariate regression). T\u00e4ss\u00e4 luvussa datamme on monimuuttujaista (engl. multivariate). Tarkemmin ottaen meill\u00e4 on kaksi selitt\u00e4v\u00e4\u00e4 muuttujaa: k\u00e4\u00e4rmeen pituus senttimetrein\u00e4 ja s\u00e4\u00e4n l\u00e4mp\u00f6tila puremahetkell\u00e4. Satunnaiset kolme rivi\u00e4 dataa n\u00e4ytt\u00e4v\u00e4t t\u00e4lt\u00e4:</p> K\u00e4\u00e4rmeen mitta S\u00e4\u00e4 (\u00b0C) Sairasloma 78.38 32.55 116 300.00 35.00 196 208.11 0.00 5 ... ... ... <p>Info</p> <p>Tavoitteena on siis oppia generalisoimaan, ett\u00e4 jos  puree sinua, s\u00e4\u00e4n ollessa 25.2 \u00b0C ja k\u00e4\u00e4rmeen ollessa 1.5 metri\u00e4 pitk\u00e4, kuinkako monta sairaslomap\u00e4iv\u00e4\u00e4 on odotettavissa.</p> <p>Kahden muuttujan kohdalla suora vaihtuu tasoksi ja sen kaava on:</p> \\[ \\hat{y} = b + w_1x_1 + w_2x_2 \\] <p>Koska muuttujia on kaksi, my\u00f6s kulmakertoimia (tai muuttujien painokertoimia) on kaksi. Kirjain <code>b</code> edustaa vakiotermi\u00e4, joka on sama kuin aiemmin. Koska haluamme, ett\u00e4 my\u00f6s <code>b</code> on optimoitava parametri, lis\u00e4t\u00e4\u00e4n se matriisiin <code>X</code> staattisena numerona yksi. Kun t\u00e4t\u00e4 ykk\u00f6st\u00e4 kertoo mill\u00e4 tahansa painolla, tulos on aina sama kuin paino (koska <code>w = w * 1</code>). Jatkossa matriisi <code>X</code>, kun siihen lis\u00e4t\u00e4\u00e4n vakiotermille oma sarakkeensa, on muotoa:</p> x[0] x[1] x[2] 1 78.38 32.55 1 300.00 35.00 1 208.11 0.00 ... ... ... <p>Jatkossa kutakin kaikkia n\u00e4it\u00e4 kolmea, <code>x[0], x[1], x[2]</code>, kohden on olemassa oma kulmakerroin <code>w[0], w[1], w[2]</code>. Matriisi <code>X</code> no siis kokoa <code>(m, n)</code>, jossa <code>m</code> on piirteiden m\u00e4\u00e4r\u00e4 ja <code>n</code> on havaintojen m\u00e4\u00e4r\u00e4. Vektori <code>w</code> on kokoa <code>(m, 1)</code>.</p> <p>Tip</p> <p>Jatkossa siis <code>b</code> on <code>x[0]</code></p>"},{"location":"algoritmit/linear/hill_climbing/#silmukassa","title":"Silmukassa","text":"<p>T\u00e4m\u00e4n voi siis suorittaa silmukassa, jossa k\u00e4yd\u00e4\u00e4n kukin sample l\u00e4pi, ja kerrotaan sen samplen kukin piirre sit\u00e4 vastaavalla painolla (eli <code>x[0] * w[0] ... x[n] * w[n]</code>). T\u00e4m\u00e4n j\u00e4lkeen kaikki tulokset summataan yhteen ja saadaan ennuste. Koodina se n\u00e4ytt\u00e4\u00e4 t\u00e4lt\u00e4:</p> IPython<pre><code>from random import random\n\nX = [\n    (1, 78.38, 32.55),\n    (1, 300.00, 35.00),\n    (1, 208.11, 0.00),\n    (1, 100.00, 7.22)\n]\ny = ... # Doesn't matter when predicting\n\nm_features = len(X[0])           # 3\nn_samples = len(X)               # 4\nw = [random() for i in range(m)] # Randomize all three\n\ny_hat = []\nfor row in X:\n    y_feat = sum([row[i] * w[i] for i in range(m)])\n    y_hat.append(y_feat\n</code></pre>"},{"location":"algoritmit/linear/hill_climbing/#matriisitulona","title":"Matriisitulona","text":"<p>Koska yll\u00e4 esitelty operaatio on sama kuin matriisin <code>X</code> ja vektorin <code>w</code> v\u00e4linen tulo, voimme korvata silmukoiden k\u00e4yt\u00f6n vektorisoidulla operaatiolla. T\u00e4ss\u00e4 voit k\u00e4ytt\u00e4\u00e4 joko omaa <code>Vector</code>- ja <code>Matrix</code>-luokan toteutusta tai k\u00e4ytt\u00e4\u00e4 NumPy-kirjastoa. J\u00e4lkimm\u00e4inen on nopea ja tehokas tapa, aiempi on hyv\u00e4 tapa n\u00e4hd\u00e4 konepellin alle. </p> \\[ y = Xw \\] <p>K\u00e4yt\u00e4nn\u00f6ss\u00e4 siis:</p> \\[ Xw =\\begin{bmatrix}    1 &amp; x_{1_1} &amp; x_{1_2} &amp; \\cdots &amp; x_{1_m}\\\\    1 &amp; x_{2_1} &amp; x_{2_2} &amp; \\cdots &amp; x_{2_m}\\\\    1 &amp; x_{3_1} &amp; x_{3_2} &amp; \\cdots &amp; x_{3_m}\\\\    \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\    1 &amp; x_{n_1} &amp; x_{n_2} &amp; \\cdots &amp; x_{n_m}\\\\    \\end{bmatrix}    \\dot{}     \\begin{bmatrix}     w_0\\\\     w_1\\\\     w_2\\\\     \\vdots\\\\     w_m     \\end{bmatrix}     =     \\begin{bmatrix}     y_1\\\\     y_2\\\\     y_3\\\\     \\vdots\\\\     y_n     \\end{bmatrix} \\] <p>Jos yll\u00e4 olevan kaavan oikealta puolelta avaa <code>y[1]</code>-ennusteen tulon kaavaksi, se on:</p> \\[ y_1 = 1 \\cdot w_0 + x_{1_1}w_1 + x_{1_2}w_2 + \\cdots + x_{1_m} \\cdot w_m \\] <p>Note</p> <p>Toisin kuin Pythonissa, k\u00e4yt\u00e4mme havaintojen yhteydess\u00e4 <code>1</code>-indeksi\u00e4, eli ensimm\u00e4inen havainto, <code>x[n]</code>, on <code>x[1]</code> eik\u00e4 <code>x[0]</code>.</p> IPython<pre><code>import numpy as np\n\n# Convert to numpy arrays\nX = np.array(X)\nw = np.array(w)\n\n# Prediction is the dot product\ny_hat = X @ w\n\n# Note that\nlen(X) == len(y_hat) == n_samples\n</code></pre>"},{"location":"algoritmit/linear/hill_climbing/#kaarmedatan-esittely","title":"K\u00e4\u00e4rmedatan esittely","text":"<p>Jatkamme saman k\u00e4\u00e4rmeenpuremia k\u00e4sittelev\u00e4n kuvittelevan datan kanssa, joka yll\u00e4 on esiteltyn\u00e4. Havaintoja on yhteens\u00e4 200 kappaletta. Ennen kuin arvomme satunnaiset painot, tarkastellaan hieman dataa. Ensimm\u00e4iset viisi havaintoa n\u00e4ytt\u00e4v\u00e4t t\u00e4lt\u00e4:</p> <pre><code>    x[0],     x[1],        y\n  220.44,    21.14,   180.00\n  204.45,    15.19,   125.00\n  101.48,     6.07,    55.00\n  195.48,    13.35,   129.00\n  184.56,    15.47,   154.00\n  169.73,    10.91,    87.00\n  ...        ...       ...\n</code></pre> <p>Muistutuksena <code>x[0]</code> on  mitta (cm) ja <code>x[1]</code> on s\u00e4\u00e4 (\u00b0C). Y on sairaslomap\u00e4ivien m\u00e4\u00e4r\u00e4. Tavoitteena on siis ennustaa sairaslomap\u00e4ivien m\u00e4\u00e4r\u00e4 k\u00e4\u00e4rmeen pureman j\u00e4lkeen, riippuen k\u00e4\u00e4rmeen pituudesta ja s\u00e4\u00e4st\u00e4. Tarkastellaan hieman oletuksia, mit\u00e4 datasta voidaan p\u00e4\u00e4tell\u00e4. Alla on korrelaatio-matriisi, joka kertoo, kuinka paljon muuttujat korreloivat kesken\u00e4\u00e4n.</p> <p></p> <p>Kuvio 1: Seabornin heatmap-funktiolla plotattu <code>df.corr()</code> -funktion palauttama korrelaatiomatriisi paljastaa numeraalisena arvona, kuinka samat parit korreloivat kesken\u00e4\u00e4n.</p> <p>Korrelaatiomatsiisista on p\u00e4\u00e4telt\u00e4viss\u00e4, ett\u00e4:</p> <ul> <li><code>Temperature</code> &lt;=&gt; <code>y</code> korrelaatio on 0.58<ul> <li>S\u00e4\u00e4n l\u00e4mp\u00f6tila ja sairaslomap\u00e4ivien m\u00e4\u00e4r\u00e4 korreloivat kesken\u00e4\u00e4n</li> <li>Jos sinua purraan helteell\u00e4, saat todenn\u00e4k\u00f6isesti enemm\u00e4n sairaslomap\u00e4ivi\u00e4</li> </ul> </li> <li><code>Snake Length</code> &lt;=&gt; <code>y</code> korrelaatio on 0.62<ul> <li>K\u00e4\u00e4rmeen pituus ja sairaslomap\u00e4ivien m\u00e4\u00e4r\u00e4 korreloivat kesken\u00e4\u00e4n</li> <li>Jos pitk\u00e4 k\u00e4\u00e4rme puree, saat toden\u00e4k\u00f6isesti enemm\u00e4n sairaslomap\u00e4ivi\u00e4</li> </ul> </li> <li><code>Temperature</code> &lt;=&gt; <code>Snake Length</code> korrelaatio on -0.25<ul> <li>L\u00e4mp\u00f6tila ja k\u00e4\u00e4rmeen pituus korreloivat miedon negatiivisesti kesken\u00e4\u00e4n</li> <li>Jos l\u00e4mp\u00f6tila kasvaa, purevan k\u00e4\u00e4rmeen pituus laskee.</li> </ul> </li> </ul> <p></p> <p>Kuvio 2: Scatter 3D -kuvaaja, joka on luotu Plotly Express -kirjastolla. Kuvaajasta on ihmissilmin p\u00e4\u00e4telt\u00e4viss\u00e4, mihin kohtaan taso kuuluisi piirt\u00e4\u00e4.</p>"},{"location":"algoritmit/linear/hill_climbing/#hill-climb-preparaatio","title":"Hill CLimb preparaatio","text":"<p>K\u00e4ytet\u00e4\u00e4n Hill Climbing -algoritmia ensimm\u00e4isen iteraation suorittamiseen. Kuten yll\u00e4 mainittiin, algoritmi aloittaa arpomalla painot satunnaisesti.</p> <pre><code>w = np.array([random() for i in range(m)])\nprint(w)\n</code></pre> intercept (w0) length (w1) temp (w2) 0.78 0.01 0.60"},{"location":"algoritmit/linear/hill_climbing/#datan-skaalaus","title":"Datan skaalaus","text":"<p>K\u00e4\u00e4rmeen mitta on suuruusluokkaa 50-300, s\u00e4\u00e4 on 0-30. Huomaa, ett\u00e4 virhefunktio perustuu et\u00e4isyyden neli\u00f6\u00f6n, joten suuret arvot dominoivat virhefunktiota. T\u00e4m\u00e4n vuoksi on t\u00e4rke\u00e4\u00e4 normalisoida data ennen kuin k\u00e4yt\u00e4mme sit\u00e4: muutoin painotamme k\u00e4\u00e4rmeen mittaa enemm\u00e4n kuin l\u00e4mp\u00f6tilaa.</p> IPython<pre><code>class StandardScaler:\n\n    def standardize(self, X):\n        self.mean = X.mean(axis=0)\n        self.std = X.std(axis=0)\n        return (X - self.mean) / self.std\n\n    def revert(self, X):\n        return X * self.std + self.mean\n\n## Normalize the dataset\nscaler = StandardScaler()\nX_std = scaler.standardize(X)\n</code></pre> <p>Z-score -skaalaus on esitelty jo aiemmin, joten t\u00e4ss\u00e4 materiaalissa emme perehdy sen toimintaa. Luomme luokan (ks. koodi yll\u00e4), joka sek\u00e4 normalisoi ett\u00e4 palauttaa normalisoidun datan alkuper\u00e4iseen muotoon. Skaalattu data n\u00e4ytt\u00e4\u00e4 t\u00e4lt\u00e4:</p> <pre><code>    x[0],     x[1],        y\n    1.13,     0.93,   180.00\n    0.74,    -0.16,   125.00\n   -1.82,    -1.82,    55.00\n    0.52,    -0.49,   129.00\n    0.24,    -0.11,   154.00\n   -0.12,    -0.94,    87.00\n    2.00,     0.46,   165.00\n</code></pre>"},{"location":"algoritmit/linear/hill_climbing/#ennusteen-laskeminen","title":"Ennusteen laskeminen","text":"<p>Ensimm\u00e4inen ennuste voidaan laskea siis seuraavasti:</p> \\[ \\begin{align*} \\hat{y}_1 &amp;= (0.79 \\cdot 1) + (0.01 \\cdot 1.13) + (0.60 \\cdot 0.93) \\\\  &amp;= 0.79 + 0.02 + 0.56 \\\\ &amp;= 1.36 \\end{align*} \\] <p>Oikea arvo on 180.00, joten virhe on 178.64.</p> <p>Voimme laskea my\u00f6s seuraavien rivien ennusteet. T\u00e4m\u00e4 hoituu seuraavalla koodilla:</p> IPython<pre><code>def predict(X, w, add_bias=True):\n    if add_bias:\n        bias_column = np.ones(X.shape[0]).reshape(-1, 1)\n        X = np.concatenate((bias_column, X), axis=1)\n    return np.dot(X, w)\n\n\nw = np.array([0.78824801, 0.01379396, 0.60234906])\ny_hat = predict(X_std, w, add_bias=True)\n\nprint_data(X_std, y, y_hat)\n</code></pre> stdout<pre><code>    x[0],     x[1],        y,    y_hat\n    1.13,     0.93,   180.00,     1.36\n    0.74,    -0.16,   125.00,     0.70\n   -1.82,    -1.82,    55.00,    -0.33\n    0.52,    -0.49,   129.00,     0.50\n    0.24,    -0.11,   154.00,     0.73\n   -0.12,    -0.94,    87.00,     0.22\n    2.00,     0.46,   165.00,     1.09\n...\n</code></pre>"},{"location":"algoritmit/linear/hill_climbing/#virheen-laskeminen","title":"Virheen laskeminen","text":"<p>K\u00e4yt\u00e4mme aiemmin tuttua MSE:t\u00e4 virhefunktiona. Se lasketaan seuraavasti:</p> IPython<pre><code>def mse(residuals):\n    return sum([residual**2 for residual in residuals]) / len(residuals)\n\nresiduals = y - y_hat\nprint(f\"MSE: {mse(residuals):.2f}\")\n</code></pre> stdout<pre><code>MSE: 20517.29\n</code></pre>"},{"location":"algoritmit/linear/hill_climbing/#hill-climb-silmukka","title":"Hill Climb Silmukka","text":"<p>Nyt voimme suorittaa Hill Climbing -algoritmin. Olemme suorittaneet ensimm\u00e4iset vaiheet, joten jatkossa ty\u00f6st\u00e4mme useita tuhansia kertoja vaiheet 3-5. TODO-listamme on siis:</p> <ul> <li> Alusta painot satunnaisesti</li> <li> Laske virhe</li> <li> Lis\u00e4\u00e4 painoihin satunnaisluku (v\u00e4lill\u00e4 \u00b11.0)</li> <li> Laske virhe</li> <li> Jos virhe pienenee, hyv\u00e4ksy muutos</li> <li> Toista 3-5 kunnes pys\u00e4ytyskriteeri t\u00e4yttyy</li> </ul> IPython<pre><code>from dataclasses import dataclass\n\n@dataclass\nclass Iteration:\n    i: int\n    MSE: float\n\n@dataclass\nclass HillClimbResult:\n    w: np.array\n    y_hat: np.array\n    iterations: list[Iteration]\n\ndef hill_climb(X, y, max_iter=10_000) -&gt; HillClimbResult:\n    beneficial_iterations = []\n    best_weights = np.array([0.78824801, 0.01379396, 0.60234906])\n    best_predictions = predict(X, best_weights)\n    best_loss = mse(y - best_predictions)\n\n    for i in range(max_iter):\n        candidate_weights = (\n            best_weights \n            + np.random.uniform(-1.0, 1.0, best_weights.shape)\n        )\n        candidate_predictions = predict(X, candidate_weights)\n        candidate_loss = mse(y - candidate_predictions)\n\n        if candidate_loss &lt; best_loss:\n            best_weights = candidate_weights\n            best_predictions = candidate_predictions\n            best_loss = candidate_loss\n            beneficial_iterations.append(Iteration(i, candidate_loss))\n            print(f\"Loss improved at Epoch #{i}: MSE: {candidate_loss:.2f}\")\n\n    return HillClimbResult(best_weights, best_predictions, beneficial_iterations)\n\nresult = hill_climb(X_std, y)\n</code></pre> stdout<pre><code>Loss improved at Epoch #0: MSE: 20495.92\nLoss improved at Epoch #1: MSE: 20444.31\nLoss improved at Epoch #2: MSE: 20340.82\nLoss improved at Epoch #4: MSE: 20304.92\n...\nLoss improved at Epoch #624: MSE: 109.70\nLoss improved at Epoch #641: MSE: 109.69\nLoss improved at Epoch #651: MSE: 109.56\nLoss improved at Epoch #804: MSE: 109.54\n</code></pre> <p>Warning</p> <p>Huomaa, ett\u00e4 tuloste muuttuu joka kerta kun ajat solun uudelleen. T\u00e4m\u00e4 johtuu siit\u00e4, ett\u00e4 painojen perustuu satunnaisuuteen. On siis teoriassa mahdollista, ett\u00e4 l\u00f6yd\u00e4t jo ensimm\u00e4isell\u00e4 iteraatiolla optimaalisen ratkaisun - tai ett\u00e4 osut jatkuvasti huonoihin painoihin, vaikka iteraatioita olisi kymmeni\u00e4 tuhansia. Todenn\u00e4k\u00f6isyys kumpaankin n\u00e4ist\u00e4 skenaarioista on kuitenkin pieni.</p> <p>Nyt voimme tarkastella ennustettuja ja oikeita arvoja:</p> IPython<pre><code>print_data(X_std, y, result.y_hat) # (1)\n</code></pre> <ol> <li>T\u00e4t\u00e4 funktiota ei ole esitelty t\u00e4ss\u00e4 materiaalissa. Voit kokeilla luoda sen itse!</li> </ol> stdout<pre><code>    x[0],     x[1],        y,    y_hat\n    1.13,     0.93,   180.00,   180.19\n    0.74,    -0.16,   125.00,   132.88\n   -1.82,    -1.82,    55.00,    56.75\n    0.52,    -0.49,   129.00,   118.10\n    0.24,    -0.11,   154.00,   134.14\n   -0.12,    -0.94,    87.00,    97.76\n    2.00,     0.46,   165.00,   161.76\n</code></pre>"},{"location":"algoritmit/linear/hill_climbing/#epookit-graafina","title":"Epookit graafina","text":"<p>Lopuksi voimme tarkastella virheen kehityst\u00e4 iteraatiota iteraariolta, koska olemme tallentaneet hy\u00f6dylliset iteraatiot <code>result.iterations</code>-muuttujaan.</p> IPython<pre><code>import matplotlib.pyplot as plt\n\nfor iteration in result.iterations:\n    plt.scatter(iteration.i, iteration.MSE, color='blue', alpha=0.5, s=2)\n\nplt.xlabel('Iterations')\nplt.ylabel('MSE')\nplt.show()\n</code></pre> <p></p> <p>Kuvio 3: Virheen kehitys iteraatioittain. Algoritmi laskee virheen iteraatioiden m\u00e4\u00e4r\u00e4n funktiona.</p> <p>Tuloksen sis\u00e4lt\u00e4m\u00e4t painot, <code>result.w</code>, on lopulta esimerkiksi <code>array([137.42809371,   2.29821156,  42.30948618])</code>. Voimme k\u00e4ytt\u00e4\u00e4 t\u00e4t\u00e4 ennustamaan <code>y_hat</code>-arvot k\u00e4ytt\u00e4en <code>predict</code>-funktiota. T\u00e4m\u00e4 on laskettuna jo valmiiksi <code>result.y_hat</code>-muuttujassa. Lopulta voimme tarkistaa sek\u00e4 MSE:n ett\u00e4 RMSE:n seuraavalla koodilla:</p> IPython<pre><code>mse_value = mse(y - result.y_hat)\nprint(f\"MSE: {mse_value:.2f}\")\nprint(f\"RMSE: {np.sqrt(mse_value):.2f}\")\n</code></pre> stdout<pre><code>MSE: 109.53\nRMSE: 10.47\n</code></pre> <p>Huomaa, ett\u00e4 MSE edustaa \"neli\u00f6p\u00e4ivi\u00e4\", kun taas RMSE eli Root Mean Squared Error edustaa \"p\u00e4ivi\u00e4\". T\u00e4m\u00e4 tarkoittaa, ett\u00e4 ennusteemme on keskim\u00e4\u00e4rin 10.47 p\u00e4iv\u00e4\u00e4 oikeasta arvosta.</p>"},{"location":"algoritmit/linear/hill_climbing/#ennuste-graafina","title":"Ennuste graafina","text":"<p>Lopuksi voimme visualisoida ennusteen ja oikeat arvot. T\u00e4m\u00e4 onnistuu Matplotlib-kirjastolla <code>plt.scatter(y, result.y_hat)</code>-koodia k\u00e4ytt\u00e4en.</p> <p></p> <p>Kuvio 4: Ennusteen ja oikeiden arvojen vertailu. Punainen viiva kuvaa t\u00e4ydellist\u00e4 ennustetta. Oranssi viiva on RMSE:n mukainen virhe (+-10.47 p\u00e4iv\u00e4\u00e4). </p>"},{"location":"algoritmit/linear/logistic/","title":"Logistic","text":"<p>Logistinen regressio on aiemmista luvuista tuttu lineaarinen malli, mutta toisin kuten nimest\u00e4 voisi p\u00e4\u00e4tell\u00e4, se on tarkoitettu luokittelu- eik\u00e4 regressio-ongelmien ratkaisuun. K\u00e4sittelemme t\u00e4ll\u00e4 kurssilla logistista regressiota vain pintapuolisesti, mutta on t\u00e4rke\u00e4\u00e4 huomata, ett\u00e4 se on k\u00e4yt\u00e4nn\u00f6ss\u00e4 neuroniverkon yksinkertaisin muoto. Logistinen regressio on siis yhden neuronin neuroverkko. Tulet siis kohtaamaan t\u00e4m\u00e4n algoritmin (tai v\u00e4hint\u00e4\u00e4n sen monimutkaisempia muunnelmia) syv\u00e4oppimista k\u00e4sittelevill\u00e4 kursseilla.</p> <pre><code>graph LR\n    bias --&gt; model(\"\u03c3(x1 * w1 + ... + xn * wn)\")\n    x_1 --&gt; model\n    x_2 --&gt; model\n    x_n --&gt; model\n    model --&gt; out[y_hat]</code></pre>"},{"location":"algoritmit/linear/logistic/#sigmoid-aktivointifunktio","title":"Sigmoid-aktivointifunktio","text":"<p>Malli luo <code>y_hat</code>-ennusteen samalla tavalla kuin aiemmin, eli <code>X @ w</code>. Erona on, ett\u00e4 t\u00e4m\u00e4 ennuste sy\u00f6tet\u00e4\u00e4n logistiseen funktioon, joka palauttaa arvon v\u00e4lilt\u00e4 0-1. T\u00e4m\u00e4 arvo voidaan tulkita todenn\u00e4k\u00f6isyydeksi, ett\u00e4 havainto kuuluu positiiviseen luokkaan. N\u00e4it\u00e4 logistisia funktioita kutsutaan aktivaatiofunktioiksi, ja niit\u00e4 k\u00e4ytet\u00e4\u00e4n my\u00f6s syv\u00e4oppimisessa. N\u00e4it\u00e4 ovat esimerkiksi sigmoid, relu ja tanh. Logistisessa regressiossa k\u00e4ytet\u00e4\u00e4n tyypillisiss\u00e4 esimerkeiss\u00e4 sigmoidia, joten k\u00e4yt\u00e4mme sit\u00e4 t\u00e4ss\u00e4kin. Neuroverkkojen yhteydess\u00e4 yleisempi\u00e4 ovat muut.</p> <p>T\u00e4m\u00e4 tehd\u00e4\u00e4n, koska logistinen regressio ennustaa luokan todenn\u00e4k\u00f6isyytt\u00e4. Olisi erikoista, jos ennuste olisi esimerkiksi 12.534, kun vaihtoehdot ovat 0 ja 1.</p> <p>Jos <code>z</code> on mallin ennuste, niin logistinen funktio on seuraava:</p> \\[ y = \\sigma(z) = \\frac{1}{1 + e^{-z}} \\] <p>Koodina sama on:</p> IPython<pre><code>def predict(X, w):\n    return X @ w\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ny_hat = sigmoid(predict(X, w))\n</code></pre> <p>Mik\u00e4li valitsemillasi painoilla, <code>w = [jotain, jotain, ...]</code>, syntyy ennuste, jonka aktivoimaton arvo on esimerkiksi 14, niin <code>sigmoid(14)</code> palauttaa arvon <code>0.9999991</code>. On loogista, ett\u00e4 jos oikeat vastaukset ovat skaalassa <code>[0, 1]</code>, ja lineaarinen aktivoimaton ennuste olisi 14-kertainen t\u00e4h\u00e4n n\u00e4hden, niin kyseisell\u00e4 havainnolla on suuri todenn\u00e4k\u00f6isyys kuulua positiiviseen luokkaan. Sigmoid-aktivoinnin j\u00e4lkeen t\u00e4m\u00e4 todenn\u00e4k\u00f6isyys on 99 % t\u00e4ss\u00e4 tapauksessa.</p> <p></p> <p>Kuvio 1. Sigmoid-funktion kuvaaja v\u00e4lill\u00e4 -5...5. Huomaa, ett\u00e4 positiiviset luvut saavat yh\u00e4 positiivisen arvon, mutta luku ei voi koskaan olla 1 tai suurempi. Vastaavasti negatiiviset luvut pysyv\u00e4t negatiivisina, mutta eiv\u00e4t voi olla -1 tai pienempi\u00e4.</p> Koodi diagrammin takana IPython<pre><code>x = np.linspace(-5, 5, 100)\nplt.plot(x, sigmoid(x))\nplt.title(\"Sigmoid activation function\")\n</code></pre> <p>Ennusteen ja aktivoinnin j\u00e4lkeen, voimme laskea virheen ja p\u00e4ivitt\u00e4\u00e4 painot gradientin avulla. T\u00e4m\u00e4 onnistuu samalla tavalla kuin lineaarisessa regressiossa, mutta virheen laskeminen on hieman erilaista. Tappiofunktion vaihtoehtoja on useita, mutta bin\u00e4\u00e4risten luokitteluongelmien yhteydess\u00e4 k\u00e4ytet\u00e4\u00e4n tyypillisesti ristientropiaa (engl. cross-entropy, log loss). Se on MSE:t\u00e4 monimutkaisempi, joten emme derivoi sit\u00e4 k\u00e4sin tai SymPyll\u00e4. K\u00e4yt\u00e4mme kirjallisuudesta l\u00f6ytyvi\u00e4 kaavoja.</p> \\[ \\text{loss} = -\\frac{1}{m} \\sum_{i=1}^{m} y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\] IPython<pre><code>def cross_entropy_cost(y_hat, y, eps=1e-15):\n    # Avoid division by zero\n    y_clip = np.clip(y_hat, eps, 1 - eps)\n\n    # Number of samples\n    m = len(y)\n\n    # Cross-entropy loss\n    loss = -(1/m) * np.sum(\n         y * np.log(y_clip)             # If y is 1\n         + (1 - y) * np.log(1 - y_clip) # If y is 0\n    ) \n\n    return loss\n</code></pre> <p>Kyseisen kaavahirvi\u00f6n gradientin kaava on:</p> \\[ \\frac{\\partial \\text{loss}}{\\partial w} = \\frac{1}{m} X^T (\\hat{y} - y) \\] IPython<pre><code>def gradient_of_cross_entropy(X, y, w):\n    y_hat = predict_proba(X, w)\n    m = len(y)\n\n    return (1/m) * np.dot(X.T, (y_hat - y))\n</code></pre>"},{"location":"algoritmit/linear/logistic/#case-kurjenmiekat","title":"Case: Kurjenmiekat","text":"<p>Scikit-learn kirjaston datasetteihin kuuluu iris-datasetti. Suomeksi iris on kurjenmiekka. Datasetist\u00e4 l\u00f6ytyy kolmen eri kurjenmiekkalajin (iris setosa, iris versicolor, iris virginica) mittauksia. Kukin havainto koostuu nelj\u00e4st\u00e4 piirteen arvosta: sepal length, sepal width, petal length ja petal width. Me k\u00e4yt\u00e4mme vain kahta lajiketta: iris setosa (kaunokurjenmiekka) sek\u00e4 iris versicolor (kirjokurjenmiekka). K\u00e4yt\u00e4mme my\u00f6s vain kahta ensimm\u00e4ist\u00e4 piirrett\u00e4: sepal length ja sepal width eli verholehden pituutta ja leveytt\u00e4.</p> Koodi datan takana IPython<pre><code>import numpy as np\n\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the dataset\niris = datasets.load_iris()\n\n# Keep only 100 first examples and only two features.\nX = iris.data[:100, :2]\ny = iris.target[:100]\n\n# Perform the z-score standardization\nss = StandardScaler()\nX_no_bias = ss.fit_transform(X)\n\n# Set values\nw = np.array([5.67, 2.34, 0.67])\n\n# Add bias to X\nX = np.c_[X_no_bias, np.ones(X_no_bias.shape[0])]\n</code></pre> <p>Arvomme my\u00f6s alkuarvot painoille. T\u00e4ss\u00e4 tapauksessa k\u00e4yt\u00e4mme arvoja <code>[5.67, 2.34, 0.67]</code>. Arvon on tarkoituksenmukaisesti valittu siten, ett\u00e4 se on hyvin kaukana oikeasta arvosta. L\u00e4ht\u00f6tilanne ei ole kolikonheiton parempi.</p> <p></p> <p>Kuvio 2. Kurjenmiekkadatan hajontakaavio. Kuvassa on kaksi lajiketta: iris setosa (sininen) ja versicolor (oranssi). Kuvassa on my\u00f6s valitsemiemme painojen, vektorin <code>w</code>:n, tuottama rajaviiva, joka m\u00e4\u00e4ritt\u00e4\u00e4 kumpaan luokkaan kasvi kuuluu, olettaen ett\u00e4 <code>0.5</code> on kynnys (engl. threshold).</p> <p>Voimme k\u00e4ytt\u00e4\u00e4 tuttuun tapaan gradientteja painojen s\u00e4\u00e4t\u00e4miseen epookki epookilta. Alla on koodi, joka hy\u00f6dynt\u00e4\u00e4 yll\u00e4 luomiamme funktioita:</p> IPython<pre><code>def predict_proba(X, w):\n    return sigmoid(predict(X, w))\n\ndef gradient_descent(X, y, w, learning_rate=0.1, epochs=2000):\n\n        # Store the cost\n        costs = []\n\n        # Loop over the number of epochs\n        for i in range(epochs):\n\n            # Calculate the prediction\n            y_hat = predict_proba(X, w)\n\n            # Calculate the cost using loop\n            cost = cross_entropy_cost(y_hat, y)\n\n            gradient_w = gradient_of_cross_entropy(X, y, w)\n\n            # Update the weights\n            w -= learning_rate * gradient_w\n\n            # Store the cost\n            costs.append(cost)\n\n        return w, costs\n\nw, costs = gradient_descent(X, y, w)\n</code></pre> <p>Osoittautuu, ett\u00e4 painoilla <code>[ 5.67, -4.10,  0.84]</code> syntyy ennnuste, joka minimoi tappiofunktion - tai ainakin valitsemillamme asetuksilla eli 2000 epookilla ja oppimisnopeudella 0.1. Monimutkaisemman datan kohdalla k\u00e4ytt\u00e4isimme verifiointiin erillist\u00e4 testidataa, ja tutkisimme mallin suorituskyky\u00e4 mittareita kuten ROC-k\u00e4yr\u00e4\u00e4 ja h\u00e4mmennysmatriisia k\u00e4ytt\u00e4en. Koska data on poikkeuksellisen simppeli\u00e4, ja oikea vastaus on n\u00e4ht\u00e4viss\u00e4 jokseenkin paljain silmin, voimme tyyty\u00e4 tarkastelemaan mallin tuottamaa rajaviivaa sek\u00e4 koulutusvirheen kehityst\u00e4 epookkien yli.</p> <p></p> <p>Kuvio 3. Koulutusvirheen kehitys epookkien yli. Koulutusvirhe laskee jyrk\u00e4sti ensimm\u00e4isten epookkien aikana, mutta tasoittuu lopulta. Mik\u00e4li virhe ei laskisi, valitsemamme oppimisnopeus olisi liian suuri. Jos se laskisi mit\u00e4tt\u00f6m\u00e4n hitaasti, oppimisnopeus olisi liian pieni.</p> <p></p> <p>Kuvio 4. Hajontakaaviossa raja on k\u00e4\u00e4ntynyt oikeaan suuntaan 2000 epookin p\u00e4\u00e4tteeksi.</p>"},{"location":"algoritmit/linear/normal_equation/","title":"Normaaliyht\u00e4l\u00f6","text":"<p>Edellisiss\u00e4 luvuissa olemme k\u00e4ytt\u00e4neet tilastotieteest\u00e4 tuttuja menetelmi\u00e4, kuten Naive Bayes, luokitteluongeomien ratkomiseen. Aiemmin olemme siis pyrkineet ennustamaan, ett\u00e4 <code>Onko havainto 1 vai 0</code> tai <code>Onko havainto 1, 2, 3, 4 vai 5</code>. T\u00e4ll\u00e4 kertaa ennustamme jatkuvia lukuja, joten vastaus voi olla esimerkiksi <code>5.5</code> tai <code>3.14159</code>. </p> <p>Tip</p> <p>Monista luokitteluun soveltumista algoritmeista, kuten kNN ja Naive Bayes, l\u00f6ytyy my\u00f6s regressio-ongelmiin soveltuvia toteutuksia.</p> <p>Note</p> <p>My\u00f6hemmin tutustumme toiseen lineaariseen malliin, nimelt\u00e4\u00e4n logistinen regressio, joka on luokittelualgoritmi - toisin kuin nimi antaa ymm\u00e4rt\u00e4\u00e4. Kumpaakin n\u00e4it\u00e4 yhdist\u00e4\u00e4 lineaarisuus. Lineaarisuus tarkoittaa, ett\u00e4 malli pyrkii l\u00f6yt\u00e4m\u00e4\u00e4n suoran suoran (tai tason), joka minimoi virheen.</p>"},{"location":"algoritmit/linear/normal_equation/#intuitio","title":"Intuitio","text":"<p>Kuvio 1: Opiskelun ja tenttipisteiden v\u00e4lisen suhteen havainnot.</p> <p>Kuvion 1 kuvitteellissa datasetissa on vain yksi ennustava muuttuja (engl. predictor) tai selitt\u00e4v\u00e4 muuttuja (engl. independent variable), joka kuvastaa opiskeluun k\u00e4ytettyj\u00e4 tunteja. T\u00e4m\u00e4 on x-akseli. Arvo, jota pyrimme jatkossa ennustamaan, on riippuva muuttuja (engl. dependent variable) tai vastaus (engl. response). T\u00e4m\u00e4 on y-akselin arvo.</p> <p></p> <p>Kuvio 2: Opiskelun ja tenttipisteiden havaintoihin sovitettu suora.</p> <p>Kysymys</p> <p>Huomaa, ett\u00e4 malli genelarisoi, eli se pyrkii ennustamaan my\u00f6s sellaisia arvoja, joita se ei ole n\u00e4hnyt. Mallin edustama s\u00e4\u00e4nn\u00f6nmukaisuus on kuitenkin suora; oikea data sis\u00e4lt\u00e4\u00e4 kohinaa, joka selittynee jonkin toisen muuttujan avulla. Mik\u00e4 tai mitk\u00e4 olisivat sopivia selitt\u00e4vi\u00e4 muuttujia opiskelun tapauksessa?</p>"},{"location":"algoritmit/linear/normal_equation/#kahviesimerkki","title":"Kahviesimerkki","text":"<p>Kuvitellaan, ett\u00e4 haluat selvitt\u00e4\u00e4, kuinka paljon -kulutuksesi vaikuttaa Backspace -n\u00e4pp\u00e4imen eli askelpalauttimen k\u00e4ytt\u00f6\u00f6n. Asennat kahvikuppiisi sensorin, joka mittaa, kuinka monta kuppia kahvia juot p\u00e4iv\u00e4ss\u00e4. Lis\u00e4ksi asennat keyloggerin, joka mittaa, kuinka monta kertaa painat backspace-n\u00e4pp\u00e4int\u00e4 minuutissa ollessasi koodaamassa.</p> <p></p> <p>Kuvio 3: Kahvinjuonti ja backspace-n\u00e4pp\u00e4imen k\u00e4yt\u00f6n havainnot pistekuvaajana. Havaintoja on vain 10.</p>"},{"location":"algoritmit/linear/normal_equation/#suora","title":"Suora","text":"<p>Yll\u00e4 totesimme, ett\u00e4 lineaarinen algoritmi pyrkii l\u00f6yt\u00e4m\u00e4\u00e4n suoran, joka minimoi virheen. Suora on matematiikasta tuttu k\u00e4site, ja sen yht\u00e4l\u00f6 on:</p> \\[ y = mx + b \\] <p>Yht\u00e4l\u00f6n muuttujat ovat seuraavat:</p> <ul> <li>\\(y\\) on selitett\u00e4v\u00e4 muuttuja (engl. dependent variable)</li> <li>\\(x\\) on selitt\u00e4v\u00e4 muuttuja (engl. predictor)</li> <li>\\(m\\) on kulmakerroin (engl. slope)</li> <li>\\(b\\) on vakiotermi (engl. intercept, y-intercept)</li> </ul> <p>Saman funktion voi luonnollisesti toteuttaa my\u00f6s Pythonilla. Huomaa, ett\u00e4 me emme viel\u00e4 toistaiseksi tied\u00e4, mitk\u00e4 ovat \\(m\\) ja \\(b\\), tai miten ne lasketaan. T\u00e4h\u00e4n tutustumme seuraavaksi.</p> IPython<pre><code>def predict(x):\n    m = None # We need to find this\n    b = None # as well as this\n    return m * x + b\n</code></pre> <p>Matematiikasta saattaa olla tuttua, ett\u00e4 suoran vakiotermi \\(b\\) on suoraan \\(y\\)-akselin leikkauspiste, ja kulmakerroin \\(m\\) kertoo, kuinka paljon \\(y\\) kasvaa, kun \\(x\\) kasvaa yhdell\u00e4. Pikaisella silm\u00e4yksell\u00e4 vaikuttaa, ihan noin silm\u00e4m\u00e4\u00e4r\u00e4isesti, ett\u00e4 suora voisi leikata y-akselin noin \\(y = 1\\) kohdalla ja kulkea noin \\(m = 1\\) kulmakertoimella: eli kun x kasvaa yhdell\u00e4, y kasvaa yhdell\u00e4. Ei kuitenkaan ihan n\u00e4in jyrk\u00e4sti, joten arvataan seuraavat arvot: \\(y = 0.9x + 1\\). Sy\u00f6tet\u00e4\u00e4n n\u00e4m\u00e4 arvot funktioon ja piirret\u00e4\u00e4n se kuvioon.</p> <p></p> <p>Kuvio 4: Kahvinjuonti ja backspace-n\u00e4pp\u00e4imen k\u00e4yt\u00f6n havainnot sek\u00e4 perstuntumalta valittu suora. Oikean Y:n ja viivan Y:n eroavaisuus on esitetty oranssina katkoviivana.</p> <p>Kuvion 3 perusteella n\u00e4ytt\u00e4\u00e4 silt\u00e4, ett\u00e4 valitsemamme suora on ihan kohtalaisen l\u00e4hell\u00e4, mutta tulevana asiantuntijana olisi hyv\u00e4 kysy\u00e4, ett\u00e4 kuinka l\u00e4hell\u00e4 se on. T\u00e4t\u00e4 varten meid\u00e4n pit\u00e4\u00e4 m\u00e4\u00e4ritell\u00e4 virhe, mik\u00e4 tehd\u00e4\u00e4n seuraavaksi.</p>"},{"location":"algoritmit/linear/normal_equation/#virhe","title":"Virhe","text":"<p>Kuten kuvasta voi silm\u00e4m\u00e4\u00e4r\u00e4isesti p\u00e4\u00e4tell\u00e4, suoraa ei voi ikin\u00e4 sovittaa siten, ett\u00e4 se l\u00e4p\u00e4isisi kaikki pisteet. Kunkin pisteen virhett\u00e4 edustaa j\u00e4\u00e4nn\u00f6s (engl. residuaal) eli Kuvio 3:n oranssit katkoviivat. </p> \\[ residual = y_i - \\hat{y}_i \\] <p>Yksitt\u00e4inen j\u00e4\u00e4nn\u00f6s on siis y:n todellinen arvo miinus y:n ennustettu arvo (\\(\\hat{y}\\) eli \"y hat\"). Intuitio kertoo, toivon mukaan, ett\u00e4 n\u00e4iden j\u00e4\u00e4nn\u00f6sten summan minimointi on hyv\u00e4 tapa l\u00f6yt\u00e4\u00e4 paras suora. T\u00e4t\u00e4 prosessia kutsutaan optimoinniksi. Tarkastellaan yll\u00e4 valitun viivan residuaaleja.</p> <pre><code>x[0]: -0.95\nx[1]: -1.65\nx[2]: +1.48\nx[3]: -0.68\nx[4]: +0.45\nx[5]: -1.45\nx[6]: -3.08\nx[7]: -0.48\nx[8]: +0.45\nx[9]: -1.50\n</code></pre> <p>Virhett\u00e4 voi mitata monella tavalla. T\u00e4ss\u00e4 kappaleessa k\u00e4yt\u00e4mme yleisint\u00e4 eli MSE (Mean Squared Error) eli keskim\u00e4\u00e4r\u00e4inen neli\u00f6virhe. T\u00e4ll\u00f6in meid\u00e4n optimoinnin tavoitteena on pienimm\u00e4n neli\u00f6summan l\u00f6yt\u00e4minen (engl. least squares).</p> \\[ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\] <p>Pythonina sama on luonnollisesti:</p> IPython<pre><code>def mse(residuals):\n    return sum([residual**2 for residual in residuals]) / len(residuals)\n</code></pre> <p>Jos yll\u00e4 n\u00e4kyv\u00e4t residuaalit sy\u00f6tet\u00e4\u00e4n yll\u00e4 olevaan funktioon, saadaan seuraava tulos:</p> <pre><code>x[0]: -0.95 (squared: 0.90)\nx[1]: -1.65 (squared: 2.72)\nx[n]: ...\nx[8]: +0.45 (squared: 0.20)\nx[9]: -1.50 (squared: 2.25)\n= MSE: 2.07\n</code></pre> <p></p> <p>Kuvio 5: Residuaalit neli\u00f6ityin\u00e4 ovat, noh, neli\u00f6it\u00e4. Huomaa, ett\u00e4 kaavion kuvasuhdetta on muutettu verrattuna aiempiin kaavioihin, jotta neli\u00f6t n\u00e4ytt\u00e4isiv\u00e4t oikeasti neli\u00f6lt\u00e4.</p> <p>Neli\u00f6ity virhe penalisoi suurempia virheit\u00e4 enemm\u00e4n kuin pieni\u00e4 virheit\u00e4. T\u00e4m\u00e4 on yksi syy sille, miksi neli\u00f6summaa k\u00e4ytet\u00e4\u00e4n yleisesti lineaarisen regressiomallin optimoinnissa. Toinen syy liittyy derivoitavuuteen, mutta t\u00e4h\u00e4n voit tutua esimerkiksi Andrew Ng:n Machine Learning Specialization -kurssikokonaisuudessa (Coursera).</p> <p>Warning</p> <p>Huomaa, ett\u00e4 regressiomallin evaluatio ei ole yht\u00e4 suoraviivaista kuin luokittelumallin evaluointi. Luokittelumallin evaluointiin riitt\u00e4\u00e4, ett\u00e4 vertaillaan ennustettuja arvoja todellisiin arvoihin: n\u00e4m\u00e4 ovat joko oikeassa tai v\u00e4\u00e4r\u00e4ss\u00e4. Regressiomalli ei ole bin\u00e4\u00e4risesti joko oikeassa tai v\u00e4\u00e4r\u00e4ss\u00e4, vaan se ennustaa jatkuvia arvoja. T\u00e4m\u00e4n vuoksi regressiomallin evaluointiin tarvitaan muita mittareita.</p>"},{"location":"algoritmit/linear/normal_equation/#neliosumman-minimointi","title":"Neli\u00f6summan minimointi","text":""},{"location":"algoritmit/linear/normal_equation/#normaaliyhtalo_1","title":"Normaaliyht\u00e4l\u00f6","text":"<p>Ilkkan Mellin kirjoittaa Tilastollisten menetelmien oppikirjassa (2006), ett\u00e4: \"Neli\u00f6summan minimointi voidaan tehd\u00e4 derivoimalla neli\u00f6summa regressiokertoimien suhteen ja merkitsem\u00e4ll\u00e4 derivaatat nolliksi.\" T\u00e4st\u00e4 syntyv\u00e4 kaava on seuraava:</p> \\[ \\hat{\\beta} = (X^T X)^{-1} X^T y \\] <p>Pythonissa saman voi toteuttaa Numpyn avulla seuraavasti:</p> IPython<pre><code>import numpy as np\nfrom numpy.linalg import inv\n\n# Add the bias term and convert to numpy arrays\nX = np.array([(*x[:-1], 1) for x in data])\nY = np.array(y)\n\n# Calculate the coefficients\ncoefficients = inv(X.T @ X) @ X.T @ Y\n</code></pre> <p>Ja saman voi tietenkin toteuttaa my\u00f6s Scikit-Learnin avulla:</p> IPython<pre><code>from sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression(fit_intercept=False)\nmodel.fit(X, Y)\ncoefficients = model.coef_\n</code></pre> <p>Teht\u00e4v\u00e4</p> <p>Selvit\u00e4, mitk\u00e4 arvot \\(m\\) ja \\(b\\) kahviesimerkiss\u00e4 saavat, kun k\u00e4yt\u00e4t Numpy tai Scikit (tai Scipy linagl) l\u00e4hestymistapaa. Laske my\u00f6s virheiden MSE n\u00e4it\u00e4 arvoja k\u00e4ytt\u00e4en. </p> <p>Datasetti on seuraavanlainen:</p> IPython<pre><code>data = [\n    (0.5, 0.5),\n    (1.0, 0.25),\n    (2.25, 4.5),\n    (3.25, 3.25),\n    (4.5, 5.5),\n    (5.5, 4.5),\n    (6.75, 4.0),\n    (7.75, 7.50),\n    (9.0, 9.55),\n    (10.0, 8.5)\n]\n</code></pre>"},{"location":"algoritmit/linear/normal_equation/#normaaliyhtalon-rajoitukset","title":"Normaaliyht\u00e4l\u00f6n rajoitukset","text":"<p>Normaaliyht\u00e4l\u00f6 on tehokas tapa laskea lineaarisen regressiomallin kertoimet, mutta sill\u00e4 on rajoituksensa. Normaaliyht\u00e4l\u00f6 toimii ep\u00e4vakaasti, jos \\(X^T X\\) ei ole k\u00e4\u00e4ntyv\u00e4. T\u00e4m\u00e4 tilanne syntyy, jos datasetti k\u00e4rsii multikollinearisuusongelmasta (engl. multicollinearity). Multikollinearisuus tarkoittaa, ett\u00e4 kaksi tai useampi selitt\u00e4v\u00e4 muuttujaa korreloivat kesken\u00e4\u00e4n, eli esimerkiksi <code>x[0] == x[1] * 1.5 + 0.02</code>. Iteratiivisten mallien regulaarisaatio (engl. regularization) auttaa t\u00e4h\u00e4n ongelmaan. Toinen apu on tunnistaa n\u00e4m\u00e4 piirteet ja poistaa ne datasta ennen mallin kouluttamista. Yksi tapa tunnistaa ne on korrelaatiomatriisi, toinen hieman kehittyneempi on VIF (Variance Inflation Factor), joka mittaa, kuinka paljon kunkin piirteen varianssi kasvaa, kun muut piirteet otetaan malliin mukaan. T\u00e4h\u00e4n l\u00f6ytyy valmis toteutus <code>statsmodel</code>-kirjastosta: ks. variance_inflation_factor ja avuksi vaikkapa DataCamp tutoriaali.</p> <p>Toinen rajoitus liittyy tietokoneen laskentakapasiteettiin. Yll\u00e4 esitellyss\u00e4 kaavassa <code>X</code>:n tulee mahtua kerralla muistiin. Muut, iteratiiviset algoritmit, kykenev\u00e4t jakamaan datasetin pienempiin osiin (engl. mini-batches) ja voivat t\u00e4ten laskea kertoimet n\u00e4iden osien perusteella. N\u00e4ihin iteratiivisiin tapoihin tutustutaan seuraavaksi, aloittaen todella yksinkertaisesta Hill Climbing -algoritmista ja edeten kohti Gradient Descent -algoritmia. Vaikka <code>X</code> mahtuisikin muistiin, laskennasta tulee hidasta, mik\u00e4li <code>n_feats</code> (eli selitt\u00e4vien muuttujien m\u00e4\u00e4r\u00e4) kasvaa suureksi.</p> <p>Jos pohdit, mit\u00e4 selitt\u00e4vien piirteiden lis\u00e4\u00e4minen k\u00e4yt\u00e4nn\u00f6ss\u00e4 tekee, niin se laittaa mallin sovittamaan n-ulotteiseen dataan n-ulotteisen suoran/tason. 3-ulottiseen malliin sovitettu taso n\u00e4ytt\u00e4\u00e4 esimerkiksi t\u00e4lt\u00e4:</p> <p></p> <p>Kuvio 6: 3-ulotteinen pistekuvaaja.</p> <p>Piirteiden m\u00e4\u00e4r\u00e4n vaikutusta laskenta-aikaan voi testata seuraavalla koodinp\u00e4tk\u00e4ll\u00e4:</p> IPython<pre><code>import datetime\nimport numpy as np\nfrom sklearn import datasets\n\ndef run_normal_equation_n_feats(n_features, n_samples=10_000):\n    # Generate\n    X, y = datasets.make_regression(n_features=n_features, n_samples=n_samples, noise=15)\n\n    # Start the timer\n    start = datetime.datetime.now()\n\n    # Do the math\n    bias = np.ones(((n_samples,1)))\n    X = np.c_[bias, X]\n    theta = np.linalg.inv(X.T @ X) @ X.T @ y\n\n    # Return the time delta between stop and start\n    return (datetime.datetime.now() - start).total_seconds()\n\n# Test with different number of features\nfeature_counts = [100, 1_000, 10_000]\n\n# Store the times\ntimes = []\n\nfor n in feature_counts:\n    start = datetime.datetime.now()\n    times.append(run_normal_equation_n_feats(n))\n</code></pre> <p>Tip</p> <p>Yll\u00e4 n\u00e4kyv\u00e4n normaaliyht\u00e4l\u00f6n vaihtoehtona on QR-dekompositio, joka on tehokkaampi suurille dataseteille. Mik\u00e4li haluat tutustua t\u00e4h\u00e4n, voit lukea esimerkiksi Essential Math for Data Science -kirjasta lis\u00e4\u00e4 (Thomas Nield, 2022, O'Reilly Media).</p>"},{"location":"algoritmit/linear/tehtavat/","title":"\ud83d\udcdd Teht\u00e4v\u00e4t","text":""},{"location":"algoritmit/linear/tehtavat/#tehtava-auton-hinta","title":"Teht\u00e4v\u00e4: Auton hinta","text":"<p>Koulut Linear Regression. K\u00e4yt\u00e4 samaa autodataa aiemmissa teht\u00e4viss\u00e4, mutta t\u00e4ll\u00e4 kertaa ennusta auton hintaa. Koska hinta on tyypillinen ennustettava muuttuja t\u00e4ss\u00e4 datasetiss\u00e4, l\u00f6yd\u00e4t netist\u00e4 referenssitoteutuksia. Eth\u00e4n kuitenkaan kopioi koodia! Kirjoita itse oma koodi, mink\u00e4 jokaisen rivin merkityksen ymm\u00e4rr\u00e4t.</p>"},{"location":"algoritmit/linear/tehtavat/#vinkit","title":"Vinkit","text":""},{"location":"algoritmit/linear/tehtavat/#pipeline","title":"Pipeline","text":"<p>Mik\u00e4li olet aiemmin k\u00e4ytt\u00e4nyt solu solulta etenev\u00e4\u00e4 vapaamuotoista koodia, kokeile t\u00e4ll\u00e4 kertaa k\u00e4ytt\u00e4\u00e4 <code>sklearn</code>-kirjaston <code>Pipeline</code>- tai <code>ColumnTransformer</code>-luokkia. Tee kaikkesi, jotta koodi on helppolukuista ja helposti kehitett\u00e4viss\u00e4. Ideaalitilanteessa esimerkiksi datan esik\u00e4sittely hoituu omalla pipelinell\u00e4, joka voi n\u00e4ytt\u00e4\u00e4 vaikkapa </p> <pre><code>from sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom feature_engine import selection as fs\n\nclass SlugifyColumns(BaseEstimator, TransformerMixin):\n    pass\n\nclass Mapper(BaseEstimator, TransformerMixin):\n    pass\n\nclass SplitGroupType(BaseEstimator, TransformerMixin):\n    pass\n\nclass Whatchamacallit(BaseEstimator, TransformerMixin):\n    pass\n\npipeline = Pipeline(steps=[\n    ('slugify_column_names', SlugifyColumns()),\n    ('likert_to_number', Mapper(variables=[TARGET], mappings=LIKERT_MAPPINGS)),\n    ('split_group_type', SplitGroupType(variables=GROUP_TYPE_COL)),\n    ('drop_columns', fs.DropFeatures(DROP_COLUMNS)),\n    ('sex_normalize', Whatchamacallit(variables=[\"foo\"])),\n])\n\ndf_preprocessed = pipeline.fit_transform(df)\n</code></pre> <p>Huomaa, ett\u00e4 koska kaikki transformaatiot ovat omia luokkiaan, voit helposti lis\u00e4t\u00e4 niit\u00e4 lis\u00e4\u00e4, ja voit helposti uusiok\u00e4ytt\u00e4\u00e4 niit\u00e4 eri pipelinen osissa. T\u00e4m\u00e4 tekee koodista helposti laajennettavaa ja muokattavaa. Niit\u00e4 voi my\u00f6s testata. Alla yksinkertainen k\u00e4sikutoinen testi:</p> <pre><code>sgp_input = pd.DataFrame({\n    'foo': [' AB123 4', ' 2 ', 'C3', 'D4AB'],\n    'other_column': [1, 2, 3, 4]\n})\nsgp = SplitGroupType(variables=['foo'])\nprint(sgp.transform(sgp_input))\n</code></pre> stdout<pre><code>   other_column   foo_letters   foo_numbers\n0             1            AB          1234\n1             2                           2\n2             3             C             3\n3             4           DAB             4\n</code></pre>"},{"location":"algoritmit/linear/tehtavat/#muiden-tehtavien-parantelu","title":"Muiden teht\u00e4vien parantelu","text":"<p>T\u00e4m\u00e4 teht\u00e4v\u00e4 on aiempia todenn\u00e4k\u00f6isesti helpompi, koska datasetti on sinulle jo entuudestaan tuttu, ja Linear Regression on \u00e4\u00e4rimm\u00e4isen yksinkertainen algoritmi. K\u00e4yt\u00e4 t\u00e4ss\u00e4 voitettua aikaa aiempien skriptien ja oppimisp\u00e4iv\u00e4kirjojen parantamiseen. Kenties huomaat oppineesi jotakin uutta Data-osioon liittyvist\u00e4 videoista, jotka antavat sinulle uusia ideoita datan k\u00e4sittelyyn tai mallin suorituskyvyn arviointiin? </p> <p>Kenties huomaat my\u00f6s, ett\u00e4 parin viikon tauon j\u00e4lkeen luettuvuna sinun oma koodisi vaikuttaa vaikealta. Kenties voit kokeilla parantaa my\u00f6s sit\u00e4 Pipelinen avulla?</p>"},{"location":"algoritmit/linear/tehtavat/#tehtava-kyberviha","title":"Teht\u00e4v\u00e4: Kyberviha","text":"<p>Kouluta Logistic Regression -malli, joka ennustaa, onko viesti kybervihaa vai ei. K\u00e4yt\u00e4mme t\u00e4t\u00e4 datasettia: Digital skills among youth: A dataset from a three-wave longitudinal survey in six European countries</p> <p>Sarakkeita on 862, joten olen opettajan roolissa pureskellut hieman dataa valmiiksi. Jupyter Notebook, jolla data on k\u00e4sitelty, sek\u00e4 k\u00e4sitelty data, l\u00f6ytyv\u00e4t kumpikin gh:sourander/ml-perusteet-code-repositoriosta. Polut ovat:</p> <ul> <li>Notebook: <code>src/playground/yskills_dataprep.ipynb</code></li> <li>Data: <code>data/y_skills/ySKILLS_longitudinal_dataset_teacher_processed.csv</code></li> </ul> <p>Tavoitteena on ennustaa <code>RISK101</code>-kent\u00e4n arvo muiden piirteiden avulla. <code>RISK101</code> on bin\u00e4\u00e4rinen muuttuja, joka kertoo, onko henkil\u00f6ll\u00e4 riski kohdata kybervihaa (engl. cyberhate). On \u00e4\u00e4rimm\u00e4isen suositeltavaa ladata alkuper\u00e4inen datasetti ja sen kylki\u00e4isen\u00e4 toimitettava <code>ySKILLS_data_dictionary.xlsx</code>-tiedosto, joka kertoo, mit\u00e4 kent\u00e4t tarkoittavat. On my\u00f6s \u00e4\u00e4rimm\u00e4isen suositeltavaa lukea yll\u00e4 mainittu Jupyter Notebook l\u00e4pi. Tulemme k\u00e4sittelem\u00e4\u00e4n aihetta my\u00f6s live-luennolla.</p>"},{"location":"algoritmit/linear/tehtavat/#vinkit_1","title":"Vinkit","text":""},{"location":"algoritmit/linear/tehtavat/#kirjastot","title":"Kirjastot","text":"<p>Tutustu ainakin seuraaviin importattuihin luokkiin tai metodeihin:</p> <pre><code>from sklearn.linear_model import SGDClassifier, LogisticRegression\n</code></pre> <p>Muista my\u00f6s s\u00e4\u00e4t\u00e4\u00e4 mallin hyperparametreja. Esimerkiksi <code>SGDClassifier</code>-luokalla on useita parametreja, jotka vaikuttavat mallin oppimiseen ja suorituskykyyn. Voit k\u00e4ytt\u00e4\u00e4 <code>GridSearchCV</code>-luokkaa hyperparametrien arvojen haarukointiin. Kuten aiemminkin, muista dokumentoida oppimisp\u00e4iv\u00e4kirjassasi, mit\u00e4 olet tehnyt ja miksi. Varmista, ett\u00e4 ymm\u00e4rr\u00e4t oman koodisi.</p>"},{"location":"algoritmit/linear/tehtavat/#tavoite","title":"Tavoite","text":"<p>Pyri v\u00e4hint\u00e4\u00e4n 75 % tarkkuuteen. Alla classification report, joka on saavutettu <code>LogisticRegression</code> luokalla ilman mink\u00e4\u00e4n sortin \u00e4lyk\u00e4st\u00e4 hyperparametrien s\u00e4\u00e4t\u00f6\u00e4 tai opettajan esik\u00e4sittelem\u00e4n datasetin hiomista.</p> <pre><code>              precision    recall  f1-score   support\n\n         0.0       0.68      0.28      0.39       687\n         1.0       0.77      0.95      0.85      1796\n\n    accuracy                           0.76      2483\n   macro avg       0.73      0.61      0.62      2483\nweighted avg       0.75      0.76      0.73      2483\n</code></pre>"},{"location":"algoritmit/linear/tehtavat/#paivakirjan-parantelu","title":"P\u00e4iv\u00e4kirjan parantelu","text":"<p>T\u00e4m\u00e4 teht\u00e4v\u00e4 saattaa osoittautua helpommaksi kuin arvasit. Nyt onkin hyv\u00e4 aika ottaa kurssikirjat ja muut l\u00e4hteet k\u00e4siin, ja varmistella, ett\u00e4 oppimip\u00e4iv\u00e4kirjasi faktat ovat tikiss\u00e4. Jos t\u00f6rm\u00e4\u00e4t v\u00e4itteisiin, joista olet ep\u00e4varma, kyseenalaista oma tekstisi. Etsi l\u00e4hteist\u00e4, onko tosiaankin asia kuten olet kirjoittanut. Muista l\u00e4hteviitteet!</p>"},{"location":"algoritmit/puut/intuitio/","title":"Intuitio","text":"<p>P\u00e4\u00e4t\u00f6spuu (engl. decision tree) on yksinkertainen, mutta tehokas koneoppimisen ty\u00f6kalu. Puun rakentaminen on suhteellisen edullista (CPU/GPU-aikana) ja kysely on nopeaa. O-luku on <code>O(log N)</code>, koska taustalla on tietorakenne nimelt\u00e4 bin\u00e4\u00e4ripuu (engl. binary tree). P\u00e4\u00e4t\u00f6spuu on p\u00e4\u00e4asiassa luokittelualgoritmi, mutta sit\u00e4 voidaan k\u00e4ytt\u00e4\u00e4 my\u00f6s regressioon. T\u00e4ss\u00e4 materiaalissa keskitymme ensisijaisesti luokitteluun.</p> <p>P\u00e4\u00e4t\u00f6spuu on puun muotoon j\u00e4rjestetty joukko s\u00e4\u00e4nt\u00f6j\u00e4, jotka auttavat ennustamaan tietyn datapisteen luokan. Jokainen solmu (tai oksan haara) testaa tietyn arvon. Jos olet joskus pelannut \"Arvaa kuka?\" tai \"Mik\u00e4 el\u00e4in?\" pelej\u00e4, konsepti on sinulle jo valmiiksi tuttu. Lopulta puu p\u00e4\u00e4ttyy lehtiin, jotka edustavat valmiita vastauksia eli luokkia.</p> <pre><code>graph TD\n    A[Onko el\u00e4in nis\u00e4k\u00e4s?] --&gt;|Kyll\u00e4| B[Onko el\u00e4imell\u00e4 k\u00e4rs\u00e4?]\n    A --&gt;|Ei| C(Varis)\n    B --&gt;|Kyll\u00e4| D(Elefantti)\n    B --&gt;|Ei| E[Esiintyyk\u00f6 el\u00e4int\u00e4 Suomessa?]\n    E --&gt;|Kyll\u00e4| I(Karhu)\n    E --&gt;|Ei| H(Gepardi)</code></pre> <p>Kuvio 1: Mik\u00e4 el\u00e4in?-pelin p\u00e4\u00e4t\u00f6spuu, olettaen ett\u00e4 ainoat sallitut el\u00e4imet ovat: gepardi, karhu, elefantti ja varis.</p> <p>Informaatio on tiedon yll\u00e4tt\u00e4vyytt\u00e4 (engl. surprise). Jos jokin tapahtuma on \u00e4\u00e4rimm\u00e4isen ep\u00e4todenn\u00e4k\u00f6inen, mutta tapahtuu silti, se on yll\u00e4tt\u00e4v\u00e4 (eli sis\u00e4lt\u00e4\u00e4 paljon informaatiota.) Tapahtuma \"N\u00e4in eilen mets\u00e4ss\u00e4 el\u00e4imen\" ei ole laisinkaan yll\u00e4tt\u00e4v\u00e4, mutta \"N\u00e4in eilen mets\u00e4ss\u00e4 leijonan\" on eritt\u00e4in yll\u00e4tt\u00e4v\u00e4.</p> <p>Teht\u00e4v\u00e4</p> <p>Arvaa kuka? -peliss\u00e4 on kiinte\u00e4 m\u00e4\u00e4r\u00e4 hahmoja, joten oletettavasti on l\u00f6ydett\u00e4viss\u00e4 kysymysten sarja, joka johtaa oikeaan vastaukseen. Lue Rafael Prieto Curielin Cracking the Guess Who? board game pohjustukseksi p\u00e4\u00e4t\u00f6spuille.</p>"},{"location":"algoritmit/puut/intuitio/#informaatioteoria","title":"Informaatioteoria","text":"<p>P\u00e4\u00e4t\u00f6spuun rakentaminen perustuu informaatioteoriaan, joka on samalla koko IT-alan perusta. Informaatioteoriaa kehitti Claude Shannon vuonna 1948, ollessaan vasta 21-vuotias. Informaatioteoria on matemaattinen teoria, joka k\u00e4sittelee informaation siirtoa ja varastointia. Noin 50-sivuinen teoria on ladattavissa PDF-muodossa ainakin Harvardin sivuilta. Sana bitti (engl. bit) tulee sanoista binary digit ja on informaation perusyksikk\u00f6. Shannon ei kenties keksinyt sanaa, mutta h\u00e4n k\u00e4ytti sit\u00e4 ensimm\u00e4isen\u00e4 julkaistussa artikkelissaan.</p> <p>Video 1: Informaatioteoriaa k\u00e4sittelev\u00e4 video (Khan Academy).</p>"},{"location":"algoritmit/puut/intuitio/#entropia","title":"Entropia","text":"<p>Entropia on ep\u00e4varmuuden mitta. Jos olet varma jonkin tapahtuman tuloksesta, entropia on nolla. T\u00e4t\u00e4 tilannetta edustaisi esimerkiksi 6-sivuinen noppa, jossa kukin silm\u00e4luku on sama. </p> <p>Kirja Predicting the Unknown: The History and Future of Data Science and Education, kirjoittajana Stylianos Kampakis, vertaa informaatioteorian hy\u00f6tyj\u00e4 viestinn\u00e4ss\u00e4, signaalink\u00e4sittelyss\u00e4 ja tietojenk\u00e4sittelyss\u00e4 tavallisten lauseiden ymm\u00e4rt\u00e4miseen. Kohinan konseptia h\u00e4n selitt\u00e4\u00e4 tilanteella, jossa vain osa viestist\u00e4 saapuu perille. Jos kuulet eritt\u00e4in kohinaisen radiotiedotteen, josta erotat vain sanat \"ja\", \"on\", \"ett\u00e4\" ja \"sek\u00e4\", saat hyvin v\u00e4h\u00e4n informaatiota. Mik\u00e4li vastaanotosta sattumanvaraisesti l\u00e4pi p\u00e4\u00e4sseet sanat ovat \"kansalaisten\" ja \"sis\u00e4tiloissa\", saat enemm\u00e4n informaatiota. Mik\u00e4li l\u00e4hett\u00e4j\u00e4 tiet\u00e4\u00e4 viestiv\u00e4ns\u00e4 kohinaisessa kanavassa, viesti\u00e4 voi toistaa useita kertoja, jotta viestin sis\u00e4lt\u00e4m\u00e4n informaation todenn\u00e4k\u00f6isyys kasvaa.</p> <p>Informaation ja entropian k\u00e4sitteet liittyv\u00e4t my\u00f6s tiedostojen pakkaamiseen. Suuri pakkaamaton tiedosto, joka sis\u00e4lt\u00e4\u00e4 75 % ykk\u00f6si\u00e4 ja 25 % nollia, pakkautuu keskim\u00e4\u00e4rin noin 81 %:iin alkuper\u00e4isest\u00e4 koosta (<code>entropy([0.75, 0.25]) ~= 0.81</code>). Todella v\u00e4h\u00e4n entropiaa sis\u00e4lt\u00e4v\u00e4n merkkijonon, kuten <code>\"spam spam spam spam \"</code> voi pakata yksinkertaisesti <code>\"spam \" * 4</code>.</p> <p>Shannonin artikkelissa sek\u00e4 yll\u00e4 olevassa videossa esitell\u00e4\u00e4n entropian matemaattinen kaava, joka on lauseena muotoa: entropia (<code>H</code>) on symbolien (esim. <code>X = x_1, x_2, ..., x_n</code>) todenn\u00e4k\u00f6isyyksien (<code>p_1, p_2, ..., p_3</code>) ja informaation sis\u00e4ll\u00f6n (engl. information content, surprisal) <code>I(x_i) = log_2(1/p_i)</code> tulojen summa. Kaavassa k\u00e4ytet\u00e4\u00e4n logaritmin kantalukuna lukua 2. Matemaattista syntaksia k\u00e4ytt\u00e4en kaava on:</p> \\[ H(X) = \\sum_{i=1}^{n} P(x_i) \\log_2 \\frac{1}{P(x_i)} \\] <p>Jos korvataan <code>P(x_i)</code>:t eli symbolien todenn\u00e4k\u00f6isyydet <code>p_i</code>, niin:</p> \\[ H(X) = \\sum_{i=1}^{n} p_i \\log_2(\\frac{1}{p_i}) \\] <p>Kaava on useissa l\u00e4hteiss\u00e4 py\u00f6r\u00e4ytetty muotoon, jossa logaritmin eteen on laitettu miinusmerkki, ja ykk\u00f6sell\u00e4 jaettu luku on nostettu logaritmin eteen (<code>-log(x) == log(1/x)</code>). T\u00e4ll\u00f6in kaava on:</p> \\[ H(X) = -\\sum_{i=1}^{n} p_i \\log_2({p_i}) \\] <p>Entropia on siis datan ep\u00e4puhtauden tai ep\u00e4j\u00e4rjestyksen mitta. Tavoitteena on minimoida entropia jakamalla data tavalla, joka erottaa luokat mahdollisimman puhtaasti. Jos sinulla on 6-sivuinen noppa, jossa on kaksi p\u00e4\u00e4kalloikonia ja nelj\u00e4 papukaijaa, eli ( ), entropia olisi:</p> \\[ H(X) = -\\left( \\frac{2}{6} \\log_2 \\frac{2}{6} + \\frac{4}{6} \\log_2 \\frac{4}{6} \\right) = 0.918 \\] <p>Alla on esimerkkitoteutus entropiafunktiosta Python-kielell\u00e4. Huomaa, ett\u00e4 todenn\u00e4k\u00f6isyyksi\u00e4 voi olla useampia, mutta niiden summan tulee olla 1.0. Me k\u00e4sittelemme t\u00e4ll\u00e4 kurssilla vain bin\u00e4\u00e4ripuita, joten todenn\u00e4k\u00f6isyyksi\u00e4 on jatkossa vain kaksi per p\u00e4\u00e4t\u00f6spuun solmu. Alla oleva koodi edustaa Khan Academyn videon esimerkki\u00e4, jossa symbolien <code>ABCD</code> todenn\u00e4k\u00f6isyydet ovat <code>[0.5, 0.25, 0.125, 0.125]</code>.</p> IPython<pre><code>from math import log2\n\ndef entropy(X):\n    H_val = -sum([p * log2(p) for p in X if p &gt; 0])\n    return H_val\n\nprobabilities = [0.5, 0.25, 0.125, 0.125]\nprint(\"H(X) = \", entropy(probabilities))\n</code></pre> <p>Teht\u00e4v\u00e4</p> <p>Kokeile yll\u00e4 olevaa koodia esimerkiksi todenn\u00e4k\u00f6isyyksill\u00e4:</p> <ul> <li><code>[0.5, 0.5]</code></li> <li><code>[0.0, 1.0]</code></li> <li><code>[0.75, 0.25]</code> # &lt;= Eli siis 75 % ykk\u00f6si\u00e4 sis\u00e4lt\u00e4v\u00e4 tiedosto</li> <li><code>[2/6, 4/6]</code> # &lt;= Eli siis p\u00e4\u00e4kalloja 2/6 ja papukaijoja 4/6</li> </ul>"},{"location":"algoritmit/puut/metsa/","title":"Mets\u00e4","text":"<p>P\u00e4\u00e4t\u00f6spuilla on taipumus ylisovittaa (engl. overfit) malli dataan. T\u00e4m\u00e4 tarkoittaa sit\u00e4, ett\u00e4 malli ei onnistu yleist\u00e4m\u00e4\u00e4n (engl. generalize) sen taustalla olevaa ilmi\u00f6t\u00e4 vaan my\u00f6t\u00e4ilee dataa ulkoaopitun tarkasti. Ylisovittamista voi pyrki\u00e4 rajoittamaan Decision Tree:n oppimista parametreilla, kuten puun maksimisyvyydell\u00e4. N\u00e4m\u00e4 parametrit toimivat \"Early Exit\"-s\u00e4\u00e4nt\u00f6in\u00e4 puun koulutuksessa; jos puu yritt\u00e4\u00e4 kasvaa liian syv\u00e4ksi, rekursiivinen prosessi palauttaa lehden.</p> <p>Toinen, hyvin yleinen tapa, parantaa p\u00e4\u00e4t\u00f6spuita ja muita korkean varianssin malleja on k\u00e4ytt\u00e4\u00e4 ensemble-menetelmi\u00e4. Ensemble-menetelm\u00e4t perustuvat usean mallin yhdist\u00e4miseen yhdeksi ennustavaksi malliksi. Mik\u00e4 ensemble koostuu vain ja ainoastaan puista, on t\u00e4lle asialle osuva termi mets\u00e4. Koska mets\u00e4n puut perustuvat satunnaiseen otantaan koulutusdatasta, t\u00e4st\u00e4 mets\u00e4st\u00e4 k\u00e4ytet\u00e4\u00e4n termi\u00e4 Satunnaismets\u00e4 (engl. Random Forest).</p> <pre><code>graph TD\n    A[\"Random Forest\"]\n\n    subgraph \"Decision Tree 3\"\n        D1[Rested?]\n        D1 --&gt;|False|D2[...]\n        D1 --&gt;|True|D3[...]\n    end\n\n    subgraph \"Decision Tree 2\"\n        C1[Speed?]\n        C1 --&gt;|&lt;10.500|C2[...]\n        C1 --&gt;|&gt;=10.500|C3[...]\n    end\n\n    subgraph \"Decision Tree 1\"\n        B1[Speed?]\n        B1 --&gt;|&lt;20.100|B2[...]\n        B1 --&gt;|&gt;=20.100|B3[...]\n    end\n\n    A --&gt; B1\n    A --&gt; C1\n    A --&gt; D1</code></pre> <p>Kuvio 1: Esimerkki satunnaismets\u00e4st\u00e4, joka koostuu kolmesta p\u00e4\u00e4t\u00f6spuusta. Kukin p\u00e4\u00e4t\u00f6spussi saa oman otoksensa koulutusdatasta eli \"pussillisen dataa\".</p> <p>Info</p> <p>Cambridge Dictionary m\u00e4\u00e4rittelee ensemblen: \"a group of things or people acting or taken together as a whole, especially a group of musicians who regularly play together.\"</p> <p>Elokuvissa ja tv-sarjojen kontekstissa ensemble on tarina, jonka p\u00e4\u00e4henkil\u00f6in\u00e4 on useita henkil\u00f6it\u00e4, jotka ovat yhteydess\u00e4 toisiinsa. Esimerkkin\u00e4 useimmat supersankarielokuvat, kuten Avengers ja Justice League, ovat ensemble-tarinoita. My\u00f6s Frendit kuuluvat t\u00e4h\u00e4n.</p>"},{"location":"algoritmit/puut/metsa/#toimintaperiaate","title":"Toimintaperiaate","text":"<p>Jos haluat pureutua syv\u00e4lle ensemble-metodien toimintaan, etsi ja lue Dietterichin julkaisu \"Ensemble Methods in Machine Learning\" (2000). Voit tutustua my\u00f6s Jensenin ep\u00e4yht\u00e4l\u00f6\u00f6n (Jensen's inequality), joka selitt\u00e4\u00e4, miksi keskiarvo useista sampleista on parempi kuin yksitt\u00e4inen sample, mik\u00e4li funktio on alasp\u00e4in kupera eli konveksi (engl. convex).</p> <p>Dietterichin julkaisu osoittaa, ett\u00e4 useiden mallien yhdist\u00e4minen (esimerkiksi <code>n * p\u00e4\u00e4t\u00f6spuut =&gt; mets\u00e4</code>) johtaa parempaan suorituskykyyn kuin yksitt\u00e4inen puu. Lopulta valinta tehd\u00e4\u00e4n demokratian keinoin: jokainen puu \u00e4\u00e4nest\u00e4\u00e4, ja eniten \u00e4\u00e4ni\u00e4 saanut <code>label</code> voittaa.</p> <pre><code>graph TD\n\n    A[\"Random Forest\"]\n    A --&gt; B[\"Decision Tree 1\"]\n    A --&gt; C[\"Decision Tree 2\"]\n    A --&gt; D[\"Decision Tree 3\"]\n    B --&gt; X[\"\u00c4\u00e4nestys\"]\n    C --&gt; X\n    D --&gt; X\n    X --&gt; Y[\"Label\"]</code></pre>"},{"location":"algoritmit/puut/metsa/#satunnaisuus","title":"Satunnaisuus","text":"<p>Satunnaismets\u00e4ss\u00e4 koulutusdatasta otetaan satunnainen otos kullekin puulle. T\u00e4m\u00e4 tarkoittaa, ett\u00e4 jokainen puu on erilainen, ja jokainen puu oppii eri tavalla. T\u00e4ll\u00e4 kurssilla k\u00e4sitell\u00e4\u00e4n kahta seuraavaa tapaa tehd\u00e4 t\u00e4m\u00e4 \"bagging\"-vaihe, jossa ota havainnoista laitetaan pussiin, osa j\u00e4\u00e4 pussin ulkopuolelle:</p> <ul> <li>Random Sample (\"without replacement\"): Koulutusdatasta otetaan arvottu otos, jossa ei voi esiinty\u00e4 samaa havaintoa kahdesti. Havainnot siis sekoitetaan ja niist\u00e4 pidet\u00e4\u00e4n esimerkiksi 70 % per pussi.</li> <li>Bootstrapping (\"with replacement\"): Koulutusdatan otanta arvotaan yksi kerrallaan. T\u00e4m\u00e4 tarkoittaa, ett\u00e4 sama havainto voi esiinty\u00e4 useammin samassa otoksessa. Tilastollisesti on 37 % todenn\u00e4k\u00f6isyys, ett\u00e4 jokin havainto ei esiinny lainkaan otoksessa eli se on \"Out of Bag\" (OOB).</li> </ul> <p>Tip</p> <p>K\u00e4yt\u00e4nn\u00f6ss\u00e4 satunnaisuutta voi lis\u00e4t\u00e4 usealla eri tavalla, kuten vaikuttamalla siihen, kuinka p\u00e4\u00e4t\u00f6spuu valitsee jakokohdan (engl. split point), vaikuttamalla mallin n\u00e4kemien featureiden m\u00e4\u00e4r\u00e4\u00e4n ja niin edelleen. T\u00e4ss\u00e4 materiaalissa keskitymme datasetin jakamiseen eri otoksiin.</p> <p>Yll\u00e4 oleva on todenn\u00e4k\u00f6isesti helpompi n\u00e4ytt\u00e4\u00e4 koodina kuin selitt\u00e4\u00e4 lauseina. Kuvitellaan, ett\u00e4 meill\u00e4 on 10 rivin datasetti, joka n\u00e4ytt\u00e4\u00e4 t\u00e4lt\u00e4:</p> IPython<pre><code>data = [\n    (1, \"...\"),\n    (2, \"...\"),\n    (3, \"...\"),\n    (4, \"...\"),\n    (5, \"...\"),\n    (6, \"...\"),\n    (7, \"...\"),\n    (8, \"...\"),\n    (9, \"...\"),\n    (10, \"...\"),\n]\n</code></pre>"},{"location":"algoritmit/puut/metsa/#python-random-sample","title":"Python: Random Sample","text":"<p>Koska <code>random.sample(iterable, n)</code> palauttaa satunnaisen otoksen <code>n</code> alkioista, voimme k\u00e4ytt\u00e4\u00e4 sit\u00e4 without replacement eli Random-metodin toteuttamiseen:</p> IPython<pre><code>import random\n\ndef sample_without_replacement(data, n):\n        return random.sample(data, n)\n\nsample_without_replacement(data, 3)\n</code></pre> <p>Random choice on k\u00e4yt\u00e4nn\u00f6ss\u00e4 sama kuin sekoittaisi listan ja pit\u00e4isi <code>data[:n]</code>-osan. Palautuva otos, joka voi sis\u00e4lt\u00e4\u00e4 kunkin numeron vain kerran, on esimerkiksi:</p> stdout<pre><code>[\n    (4, \"...\"),\n    (7, \"...\"),\n    (3, \"...\"),\n]\n</code></pre>"},{"location":"algoritmit/puut/metsa/#python-bagging","title":"Python: Bagging","text":"<p>Jos haluamme toteuttaa Bagging-metodin, voimme k\u00e4ytt\u00e4\u00e4 <code>random.choice(iterable)</code>-funktiota, joka palauttaa satunnaisen alkion datasetist\u00e4. Koska <code>choice</code> ei poista valittua alkioita, sama alkio voi esiinty\u00e4 useammin samassa otoksessa:</p> IPython<pre><code>import random\n\ndef sample_with_replacement(data):\n        return [random.choice(data) for _ in range(len(data))]\n\nsample_with_replacement(data)\n</code></pre> <p>Huomaa, ett\u00e4 <code>random.choice()</code> ei poista rivi\u00e4 datasta, joten sama rivi voi tulla vastaan <code>len(data)</code> kertaa. Palautuva otos on esimerkiksi:</p> stdout<pre><code>[\n    (4, \"...\"),\n    (4, \"...\"),\n    (7, \"...\"),\n    (3, \"...\"),\n    (2, \"...\"),\n    (7, \"...\"),\n    (5, \"...\"),\n    (9, \"...\"),\n    (2, \"...\"),\n    (3, \"...\"),\n]\n</code></pre> <p>Lopputuloksena:</p> <ul> <li>Seuraavat luvut esiintyv\u00e4t kahdesti: <ul> <li><code>2, 3, 4, 7</code></li> </ul> </li> <li>Seuraavat luvut eiv\u00e4t esiinny ollenkaan: <ul> <li><code>1, 6, 8, 10</code></li> </ul> </li> </ul> <p>Mik\u00e4li koulutat esimerkiksi 100 eri puuta, jokainen puu saa erilaisen otoksen datasta. Alla viel\u00e4 esimerkki, kuinka t\u00e4t\u00e4 voisi k\u00e4ytt\u00e4\u00e4:</p> IPython<pre><code>import ml.decision_tree as dt # Last lesson's implementation\n\n\ndata = load_imaginery_data(n=1_000_000)\ntrees = []\nsubsets = []\nN_TREE = 9\n\n# Generate N_TREE subsets\nsubsets = [sample_with_replacement(data) for _ in range(N_TREE)]\n\nfor subset in subsets:\n    tree = dt.build_tree(subset)\n    trees.append(tree)\n</code></pre>"},{"location":"algoritmit/puut/metsa/#aanestys","title":"\u00c4\u00e4nestys","text":"<p>Yksinkertaisin tapa p\u00e4\u00e4tt\u00e4\u00e4, mik\u00e4 on lopullinen ennuste, on \u00e4\u00e4nestys. Jokainen puu \u00e4\u00e4nest\u00e4\u00e4, ja eniten \u00e4\u00e4ni\u00e4 saanut <code>label</code> voittaa - eli siis tilastollinen moodi eli useimmiten esiintyv\u00e4 label. Yll\u00e4 k\u00e4ytetty <code>N_TREE</code> on pariton, joten \u00e4\u00e4nestyksess\u00e4 ei voi tulla tasapeli\u00e4.</p> <pre><code># Imaginary test row\ntest_row = (1, 1, 0.12, 0, 1.23, ..., 1)\n\n# Predict using each tree\nlabels = []\nfor tree in trees:\n    label = dt.predict(tree, test_row)\n    labels.append(label)\n\n# Vote (mode)\ny_hat = max(labels, key=labels.count)\n\n# Raise AssertionError if prediction went wrong\nassert y_hat == test_row[-1]\n</code></pre>"},{"location":"algoritmit/puut/metsa/#boosting","title":"Boosting","text":"<p>Boosting-metodit ovat ensemble-metodeja, jotka pyrkiv\u00e4t parantamaan mallia perustuen aiempien mallien virheisiin. T\u00e4m\u00e4 tarkoittaa sit\u00e4, ett\u00e4 jokainen puu oppii edellisen puun virheist\u00e4 ja pyrkii korjaamaan ne.</p> <p>Eli siis:</p> <ul> <li>Forest-tyylinen ensemblen mallit voidaan kouluttaa rinnakkain</li> <li>Boosting-tyylinen ensemblen mallit koulutetaan per\u00e4kk\u00e4in</li> </ul> <pre><code>graph LR\n\n    D[\"Input Data\"]\n\n    subgraph \"Weighted Data\"\n        D1[\"Original data\"]\n        D2[\"Once weighted\"]\n        D3[\"Twice weighted\"]\n    end\n\n    subgraph \"Trees\"\n        X1[\"Decision Tree 1\"]\n        X2[\"Decision Tree 2\"]\n        X3[\"Decision Tree 3\"]\n    end\n\n    O[\"Output Voting\"]\n\n    D --&gt; D1\n    D1 --&gt; X1\n    X1 --&gt; D2\n    D2 --&gt; X2\n    X2 --&gt; D3\n    D3 --&gt; X3\n\n    X1 --&gt; O\n    X2 --&gt; O\n    X3 --&gt; O</code></pre> <p>Kuvio 2: Boosting-menetelm\u00e4, jossa jokainen puu oppii edellisen puun virheist\u00e4. Kaikki puut osallistuvat ennustamiseen, mutta ovat painotettuja.</p> <p>Boosting-metodeja on useita, mutta yksi tunnetuimmista on AdaBoost (adaptive boosting). AdaBoost perustuu siihen, ett\u00e4 koulutetaan <code>Malli X</code>. Tuon <code>Malli X</code>:n virheellisesti ennustamat havainnot painotetaan seuraavassa mallissa <code>Malli X+1</code>. K\u00e4yt\u00e4nn\u00f6ss\u00e4 meid\u00e4n tulisi siis lis\u00e4t\u00e4 dataan paino, joka olisi pohjimmiltaan esimerkiksi <code>1/N</code>, jossa <code>N</code> on havaintojen m\u00e4\u00e4r\u00e4.</p> IPython<pre><code>X = [\n    (1, 1, 12.34),\n    (1, 1, 11.11),\n    ...\n]\n\nbase_weight = 1 / len(X)\n\nX_with_weights = [\n    (1, 1, 12.34, base_weight),\n    (1, 1, 11.11, base_weight),\n    ...\n]\n</code></pre> <p>Ensimm\u00e4inen malli <code>Malli X</code> koulutetaan normaalisti, ja sen virheellisesti ennustamat havainnot painotetaan seuraavassa mallissa <code>Malli X+1</code>. T\u00e4m\u00e4 prosessi jatkuu, kunnes saavutetaan <code>M</code> mallia. Seuraavan mallin data voisi siis olla jotain seuraavaa, olettaen ett\u00e4 <code>x_2</code> on virheellisemmin ennustettu kuin <code>x_1</code>:</p> IPython<pre><code>X_for_next_model = [\n    (1, 1, 12.34, 0.011),\n    (1, 1, 11.11, 0.025),\n    ...\n]\n</code></pre> <p>Muita tunnettuja boosting-metodeja ovat esimerkiksi Gradient Boosting ja XGBoost.</p>"},{"location":"algoritmit/puut/puu/","title":"Puu","text":"<p>Aiemmassa osiossa esitelty entropia on p\u00e4\u00e4t\u00f6spuun rakentamisen keskeinen k\u00e4site. Entropia on datan ep\u00e4puhtauden tai ep\u00e4j\u00e4rjestyksen mitta. Tavoitteena on minimoida entropia jakamalla data tavalla, joka erottaa luokat mahdollisimman puhtaasti. T\u00e4ss\u00e4 osiossa tutustutaan siihen, kuinka kunkin oksan informaation kasvu lasketaan: eli siis, kuinka valitaan paras ominaisuus, jolla data jaetaan.</p> <p>Arvaa kuka?-pelin logiikassa t\u00e4m\u00e4 olisi parhaan kysymyksen valinta. Kuvitellaan, ett\u00e4 olemme luomassa koneoppimismallia, joka pyrkii ennustamaan, ett\u00e4 \"Onko kysyj\u00e4n valitsema hahmo naisoletettu\". Yksinkertaisuuden vuoksi peliss\u00e4 on leikisti vain 5 eri featurea (<code>parta|silm\u00e4lasit|hattu|pitk\u00e4t hiukset|kauluspaita</code>). Alla on kuvitteellinen esimerkki ensimm\u00e4isen kysymyksen valinnasta:</p> <ul> <li> Onko h\u00e4nell\u00e4 parta? (Score: 0.44)</li> <li> Onko h\u00e4nell\u00e4 silm\u00e4lasit? (Score: 0.33)</li> <li> Onko h\u00e4nell\u00e4 hattu? (Score: 0.22)</li> <li> Onko h\u00e4nell\u00e4 pitk\u00e4t hiukset? (Score: 0.11)</li> <li> Onko h\u00e4nell\u00e4 kauluspaita? (Score: 0.00)</li> </ul> <p>Kukin kysymys jakaisi datasetin kahteen osaan: heihin joilla t\u00e4m\u00e4 piirre on, ja heihin, joilta se puuttuu. Meid\u00e4n kuvitteellisen Arvaa kuka?-pelin datasetiss\u00e4 ei ole laisinkaan parrakkaita naisia, mutta on useampi parraton jotakin muuta sukupuolta edustava henkil\u00f6. Jako tapahtuisi seuraavalla tavalla:</p> IPython<pre><code># Input Data\n#\n#        the rest with beards                     women without beards\n#        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \nlabel = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\nbeard = [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n#                            \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n#                       the rest without beards\n\n# Split the data into two partitions by beardedness\nbeardless = [label for label, beard in zip(label, beard) if not beard]\nbearded   = [label for label, beard in zip(label, beard) if beard]\n\n#        the rest without beards\n#             \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510\n# beardless: [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n# bearded:   [0, 0, 0, 0, 0, 0, 0]\n#             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n#            the rest with beards\n\n# Compute the entropy after the split\nscore = some_magic_function(label, beardless, bearded)\nprint(f\"Score: {score:.2f}\")\n</code></pre> stdout<pre><code>Score: 0.44\n</code></pre> <p>Warning</p> <p>Huomaa, ett\u00e4 t\u00e4m\u00e4 esimerkki ei ota kantaa siihen, mik\u00e4 on nainen tai voiko naisella olla partaa. T\u00e4ss\u00e4 Arvaa Kuka?-tyylisen, kuvitteellisen pelin populaatiossa esiintyy vain kahta luokkaa: <code>0</code> ja <code>1</code>, joista luokka <code>1</code> ovat naiseksi identifioituvat henkil\u00f6t. Luokka 0 ovat kaikki muut sukupuolet (\"the rest\").</p> <p>Tarkkasilm\u00e4inen huomaa, ett\u00e4 esimerkiss\u00e4 on k\u00e4ytetty <code>some_magic_function</code>-funktiota, jota ei ole viel\u00e4 esitelty. Se on funktio, joka laskee informaation kasvun (yll\u00e4 ymp\u00e4ripy\u00f6re\u00e4sti \"score\"). Aiemmin esitelty entropia on yksi osa informaation kasvun laskemista. T\u00e4h\u00e4n kaavaan tutustutaan alla.</p>"},{"location":"algoritmit/puut/puu/#informaation-kasvu","title":"Informaation kasvu","text":"<p>Informaation kasvu (engl. information gain) on p\u00e4\u00e4t\u00f6spuun rakentamisen keskeinen k\u00e4site. Se on laskettu erotuksena vanhemman solmun (engl. parent node) entropiasta ja painotetusta keskiarvosta sen lasten (engl. child nodes) entropioista. Mit\u00e4 suurempi informaation kasvu, sit\u00e4 parempi, koska se osoittaa, ett\u00e4 datan jakaminen valitun ominaisuuden perusteella v\u00e4hent\u00e4\u00e4 ep\u00e4varmuutta kohdearvosta. Yll\u00e4 olevassa esimerkiss\u00e4 parrakkuus on hyv\u00e4 valinta, koska se tuottaa suuremman informaation kasvun kuin muut ominaisuudet (kuten silm\u00e4l\u00e4lasit tai hattu).</p> <p>Kaava on:</p> \\[ IG(S, a) = H(S) - H(S|a) \\] <p>T\u00e4m\u00e4 voidaan kirjoittaa auki muotoon:</p> \\[ IG(S, a) = H(S) - \\sum_{v \\in values(a)} \\frac{|S_{v}|}{|S|} H(S_{v}) \\] <p>Miss\u00e4:</p> <ul> <li>\\(H(S)\\) : on vanhemman solmun entropia (eli <code>y</code>-arvojen entropia)</li> <li>\\(a\\) : on valittu ominaisuus eli piirre</li> <li>\\(v \\in values(a)\\) : on ominaisuuden \\(a\\) mahdolliset arvot</li> <li>\\(S_v\\) : on S:n data niiss\u00e4 indekseiss\u00e4, joissa \\(a\\) on \\(v\\) (<code>child1</code> tai <code>child2</code>)</li> <li>\\(H(S_v)\\) : on lasten solmun \\(v\\) entropia</li> </ul> <p>Kaavassa pystypalkit \\(|S|\\) ja \\(|S_v|\\) tarkoittavat joukon kardinaalisuutta, eli kuinka monta alkiota joukossa on. K\u00e4yt\u00e4nn\u00f6ss\u00e4 se on sama asia kuin <code>len(X)</code>. Jakolaskun tulona on siis: \"Kuinka suuri osa piirteen \\(a\\) arvoista on \\(v\\)\". </p> <p>Python-koodina informaation kasvun laskeminen n\u00e4ytt\u00e4\u00e4 t\u00e4lt\u00e4:</p> IPython<pre><code>def class_probabilities(values):  # What's this?(1)\n    n = len(values)\n    zeros = values.count(0)\n    ones = values.count(1)\n    return [zeros / n, ones / n]\n\n# This is the some_magic_function() from the previous example\ndef information_gain(parent, child1, child2):\n\n    # H(S) - Entropy of parent node\n    H_S = entropy(class_probabilities(parent))\n\n    # |S| - Cardinality of parent node\n    n = len(parent)\n\n    # S_v - Data of each child node\n    S_v = [child1, child2]\n\n    # |S_v| - Cardinality of each child node\n    n_S_v = [len(v) for v in S_v]\n\n    # H(X_v) - Entropy of each child node\n    H_S_v = [entropy(class_probabilities(v)) for v in S_v]\n\n    # Formula\n    IG = H_S - sum([n_S_v[i] / n * H_S_v[i] for i in range(len(S_v))])\n\n    return IG\n\n# Dataset X\nX = [\n#   x0, x1, y\n    (1, 1, 0),  #  \u250c\u2500\u2500\u2500\u2500\u2500&gt; x[0] =&gt; 1/3 equals true\n    (1, 1, 1),  #  \u2502\n    (1, 1, 0),  #  \u2518 \u250c\u2500\u2500\u2500&gt; x[1] =&gt; 3/5 equals true\n    (0, 1, 1),  #  \u2500\u2500\u2524\n    (0, 0, 0),  #    \u2502\n    (0, 1, 1),  #  \u2500\u2500\u2518\n    (0, 0, 0)\n]\n\nn_columns = len(X[0]) - 1\n\nfor i in range(n_columns):\n    parent = [row[-1] for row in X]\n    child1 = [row[-1] for row in X if row[i] == 1]  # v in values(a) == 1\n    child2 = [row[-1] for row in X if row[i] == 0]  # v in values(a) == 0\n\n    ig = information_gain(parent, child1, child2)\n    print(f\"Column {i} information gain: {ig:.2f}\")\n</code></pre> <ol> <li>This function returns a list of two probabilities. A probability of class 0 and probability of class 1.     <pre><code>assert class_probabilities([0, 0, 1, 1, 1]) == [0.4, 0.6]\n</code></pre></li> </ol> <p>Koosteena:</p> <ul> <li>Matala entropia (tai korkea puhtaus) on toivottavaa jokaisessa puun solmussa.</li> <li>Korkea informaation kasvu osoittaa, ett\u00e4 valittu ominaisuus tarjoaa hy\u00f6dyllist\u00e4 tietoa datan jakamiseen ja on siksi toivottava p\u00e4\u00e4t\u00f6spuun rakentamiseen.</li> </ul>"},{"location":"algoritmit/puut/puu/#esimerkki-pyoralla-vai-autolla","title":"Esimerkki: Py\u00f6r\u00e4ll\u00e4 vai autolla?","text":"<p>Luodaan p\u00e4\u00e4t\u00f6spuu, joka pyrkii generalisoimaan kulkuv\u00e4lineen valinnan p\u00e4\u00e4t\u00f6spuun avulla. Huomaa, ett\u00e4 t\u00e4ss\u00e4 esimerkiss\u00e4 saatetaan k\u00e4ytt\u00e4\u00e4 joitakin pseudofunktioita, joita ei olla k\u00e4yt\u00e4nn\u00f6ss\u00e4 m\u00e4\u00e4ritelty miss\u00e4\u00e4n. Tarkoituksena on havainnollistaa informaation kasvun laskemista; varsinainen k\u00e4ytt\u00f6valmiimpi koodi on muualla harjoitusteht\u00e4viss\u00e4 esill\u00e4. Tuotantoon soveltuvaa koodia emme kirjoita laisinkaan t\u00e4m\u00e4n kurssin aikana: k\u00e4yt\u00e4 t\u00e4h\u00e4n Scikit-learn-kirjastoa.</p>"},{"location":"algoritmit/puut/puu/#datan-esittely","title":"Datan esittely","text":"<p>Datasettiin kuuluu 4 saraketta (eli piirrett\u00e4, featurea) ja 16 rivi\u00e4 (eli havaintoa, observation). Sarakkeet ovat:</p> <ul> <li>Olen lev\u00e4nnyt (0 = False, 1 = True)</li> <li>M\u00e4\u00e4r\u00e4np\u00e4\u00e4ss\u00e4 on suihku (0 = False, 1 = True)</li> <li>Minimivauhti, jolla ehtisin perille. (x.xx km/h)</li> <li>Havainnossa on menty autolla (0 = False, 1 = True)</li> </ul> <p>Viimeinen sarake on se label, jota me yrit\u00e4mme ennustaa. Datasetti n\u00e4ytt\u00e4\u00e4 t\u00e4lt\u00e4:</p> im_well_rested dst_has_shower required_speed go_by_car 0 0 10.63 1 1 1 71.09 1 0 1 42.95 1 1 0 24.53 1 1 0 53.66 1 0 1 71.79 1 1 0 5.88 0 1 1 57.5 1 0 0 25.32 1 1 0 34.39 1 1 1 56.91 1 1 0 64.6 1 1 1 17.46 0 0 1 87.61 1 1 0 14.2 1 1 1 8.28 0"},{"location":"algoritmit/puut/puu/#datan-luomiskertomus","title":"Datan luomiskertomus","text":"<p>Havaintoavaruuden kolme ensimm\u00e4ist\u00e4 saraketta (eli piirteet) on generoitu satunnaislukuja k\u00e4ytt\u00e4en. Viimeinen sarake, eli label <code>go_by_car</code>, on generoitu seuraavalla logiikalla:</p> IPython<pre><code>transportation_mode = np.where(\n    (required_speed &gt; 20) |\n    ((well_rested == 0) &amp; (required_speed &gt; 15)) |\n    ((has_shower == 0) &amp; (required_speed &gt; 10)),\n    1, 0\n)\n</code></pre> <p>Lauseena <code>go_by_car</code>-sarakkeen logiikka on: mene autolla, jos jokin seuraavista on totta:</p> <ul> <li>Vaadittu nopeus on 20 km/h</li> <li>En ole lev\u00e4nnyt ja vaadittu nopeus on 15 km/h</li> <li>M\u00e4\u00e4r\u00e4np\u00e4\u00e4ss\u00e4 ei ole suihkua ja vaadittu nopeus on 10 km/h</li> </ul>"},{"location":"algoritmit/puut/puu/#juuritason-solmun-valinta","title":"Juuritason solmun valinta","text":"<p>Varsinainen koodi kannattaa kirjoittaa rekursiiviseksi, koska kunkin oksanhaaran kohdalla logiikka on tismalleen sama. T\u00e4ss\u00e4 esimerkiss\u00e4 ty\u00f6 tehd\u00e4\u00e4n kuitenkin k\u00e4sin, jotta vaiheet tulevat selv\u00e4sti esille. Ensimm\u00e4inen solmu valitaan laskemalla informaation kasvu jokaiselle piirteelle ja valitsemalla suurin informaation kasvu.</p> <p>Tip</p> <p>Kuljettu polku selvi\u00e4\u00e4 muuttujan nimest\u00e4. Esimerkiksi <code>root_l_r_r</code> on p\u00e4\u00e4t\u00f6spuun polku, jossa dataa on jaettu ensin vasemmalle, sitten oikealle ja lopuksi taas oikealle.</p> <p>Alla olevassa koodissa <code>find_max_column_information_gain()</code>-funktio laskee informaation kasvun jokaiselle piirteelle (lev\u00e4nneisyys, suihkullisuus ja nopeus) ja palauttaa suurimman informaation kasvun tuottavan sarakkeen indeksin. T\u00e4m\u00e4n j\u00e4lkeen <code>split_data()</code>-funktio jakaa datan valitun piirteen perusteella kahteen osaan.</p> IPython<pre><code>ig_root = dt.find_max_column_information_gain(data)\nprint(f\"Root node: {ig_root}\")\n# Splitting score: IGScore(\n#   column_index=2, \n#   column_type='continuous', \n#   score=0.39279019935806186, \n#   split_point=20.995\n# )\n\nroot_l, root_r = dt.split_data(data, ig_root)\n</code></pre> <p>Tip</p> <p>Toivon mukaan pohdit nyt, ett\u00e4 kuinka ei-bin\u00e4\u00e4risest\u00e4 piirteest\u00e4, nopeudesta, voidaan poimia split point. T\u00e4h\u00e4n olisi monta mahdollista strategiaa, kuten:</p> <ul> <li>Vaihtoehto 1: K\u00e4yt\u00e4 mediaania. T\u00e4m\u00e4 tuskin johtaa optimaaliseen ratkaisuun, mutta on nopea ja helppo tapa jakaa dataa.</li> <li>Vaihtoehto 2: Laske kaikki mahdolliset split pointit. T\u00e4m\u00e4 on kallis operaatio, mutta antaa tarkan kuvan siit\u00e4, mik\u00e4 split point on paras. Jos data olisi esimerkiksi <code>[1.0, 2.0, 3.0, 4.0, 5.0]</code>, mahdolliset split pointit olisivat kaikkien n\u00e4iden lukujen v\u00e4liin asettuvat luvut: <code>[1.5, 2.5, 3.5, 4.5]</code>.</li> </ul> <p>Me valitsemme j\u00e4lkimm\u00e4isen vaihtoehdon.</p>"},{"location":"algoritmit/puut/puu/#tulos","title":"Tulos:","text":"<p>P\u00e4\u00e4t\u00f6spuu on saanut ensimm\u00e4isen haaransa, joka vastaa kysymykseen:</p> <p>Sin\u00e4  : \"Hmm, pit\u00e4isik\u00f6 minun py\u00f6r\u00e4ill\u00e4 vai menn\u00e4 autolla m\u00e4\u00e4r\u00e4np\u00e4\u00e4h\u00e4n?\"</p> <p>Puu  : \"Ah, t\u00e4m\u00e4 riippuu siit\u00e4, kuinka kauas sinun pit\u00e4\u00e4 p\u00e4\u00e4st\u00e4 ja kuinka \u00e4kki\u00e4. Onko vaadittu nopeus suurempi kuin 20.995 km/h?\".</p>"},{"location":"algoritmit/puut/puu/#graafi","title":"Graafi","text":"<pre><code>graph TD\n    ROOT[Nopeus?] --&gt;|#60;20.995| ROOT_L_QUIZ[Seuraava kysymys]\n    ROOT --&gt;|#62;#61;20.995| ROOT_R_LEAF((Autolla))</code></pre> <p>Info</p> <p>Graafissa py\u00f6re\u00e4 solmu tarkoittaa p\u00e4\u00e4t\u00f6ssolmua, joka ei jaa dataa en\u00e4\u00e4. Suorakulmio tarkoittaa kysymyst\u00e4, joka jakaa dataa kahteen osaan. Englanniksi n\u00e4m\u00e4 ovat leaf node, eli siis puun lehti, ja decision node tai branch node, eli siis oksanhaara.</p>"},{"location":"algoritmit/puut/puu/#datan-jaot","title":"Datan jaot","text":"<p>Klikkaa seuraavat laatikot auki n\u00e4hd\u00e4ksesi, mitk\u00e4 rivit ovat osallistuneet kunkin informaation kasvun laskentaan.</p> Vasen oksa im_well_rested dst_has_shower required_speed go_by_car 0 0 10.63 1 1 0 5.88 0 1 1 17.46 0 1 0 14.2 1 1 1 8.28 0 Oikea lehti im_well_rested dst_has_shower required_speed go_by_car 1 1 71.09 1 0 1 42.95 1 1 0 24.53 1 1 0 53.66 1 0 1 71.79 1 1 1 57.5 1 0 0 25.32 1 1 0 34.39 1 1 1 56.91 1 1 0 64.6 1 0 1 87.61 1"},{"location":"algoritmit/puut/puu/#tason-1-solmun-valinta","title":"Tason 1 solmun valinta","text":"<p>Juuritaso, eli taso 0, on nyt luotu. Toinen oksa haarautui suoraan lehteen, joten sit\u00e4 on turha jatkaa. <code>root_r</code> on siis lehti. Seuraavaksi valitaan <code>root_l</code>-solmun lapsista uusi juuritaso.</p> IPython<pre><code>ig_depth_1 = dt.find_max_column_information_gain(root_l)\nprint(f\"Splitting score: {ig_depth_1}\")\n# Splitting score: IGScore(\n#   column_index=1, \n#   column_type='binary', \n#   score=0.4199730940219749, \n#   split_point=0.5\n# )\n\nroot_l_l, root_l_r = dt.split_data(data, ig_depth_1)\n</code></pre>"},{"location":"algoritmit/puut/puu/#tulos_1","title":"Tulos","text":"<p>Jotta voit p\u00e4\u00e4ty\u00e4 t\u00e4h\u00e4n tilanteeseen, sinulla t\u00e4ytyy olla nopeutta v\u00e4hemm\u00e4n kuin 20.995 km/h. Seuraavaksi eniten informaation kasvua paljastavaksi kysymykseksi on osoittautunut, onko m\u00e4\u00e4r\u00e4np\u00e4\u00e4ss\u00e4 suihku.</p>"},{"location":"algoritmit/puut/puu/#graafi_1","title":"Graafi","text":"<pre><code>graph TD\n    ROOT[Nopeus?] --&gt;|#60;20.995| ROOT_L[Suihku?]\n    ROOT --&gt;|#62;#61;20.995| ROOT_R_LEAF((Autolla))\n\n    ROOT_L --&gt;|Ei| ROOT_L_L_QUIZ[Seuraava kysymys]\n    ROOT_L --&gt;|Kyll\u00e4| R_L_R_LEAF((Py\u00f6r\u00e4ll\u00e4))</code></pre>"},{"location":"algoritmit/puut/puu/#datan-jaot_1","title":"Datan jaot","text":"Vasen oksa im_well_rested dst_has_shower required_speed go_by_car 0 0 10.63 1 1 0 5.88 0 1 0 14.2 1 Oikea lehti im_well_rested dst_has_shower required_speed go_by_car 1 1 17.46 0 1 1 8.28 0"},{"location":"algoritmit/puut/puu/#tason-2-solmun-valinta","title":"Tason 2 solmun valinta","text":"<p>Juuritaso ja Taso 1 p\u00e4\u00e4ttyiv\u00e4t kumpikin tilanteeseen, jossa toinen oksa (<code>root_l_r</code>) osoittautui lehdeksi. Meill\u00e4 on taas helppo tilanne edess\u00e4, jossa meid\u00e4n t\u00e4ytyy laskea vain toisen oksan informaation kasvu.</p> IPython<pre><code>ig_depth_2 = dt.find_max_column_information_gain(root_l_l, verbose=True)\nprint(f\"Splitting score: {ig_depth_2}\")\n# Splitting score: IGScore(\n#   column_index=2, \n#   column_type='continuous',\n#   score=0.9182958340544896,\n#   split_point=8.255\n#   )\n\nroot_l_l_l, root_l_l_r = dt.split_data(root_l_l, ig_depth_2)\n</code></pre>"},{"location":"algoritmit/puut/puu/#tulos_2","title":"Tulos","text":"<p>Jotta voit p\u00e4\u00e4ty\u00e4 t\u00e4h\u00e4n tilanteeseen, sinulla t\u00e4ytyy olla nopeutta v\u00e4hemm\u00e4n kuin 20.995 km/h ja m\u00e4\u00e4r\u00e4np\u00e4\u00e4ss\u00e4 ei saa olla suihkua. Seuraavaksi eniten informaation kasvua paljastavaksi kysymykseksi on osoittautunut, onko vaadittu nopeus suurempi kuin 8.255 km/h.</p>"},{"location":"algoritmit/puut/puu/#graafi_2","title":"Graafi","text":"<pre><code>graph TD\n    ROOT[Nopeus?] --&gt;|#lt;20.995| ROOT_L[Suihku?]\n    ROOT --&gt;|#62;#61;20.995| ROOT_R_LEAF((Autolla))\n\n    ROOT_L --&gt;|Ei| ROOT_L_L[Nopeus?]\n    ROOT_L --&gt;|Kyll\u00e4| R_L_R_LEAF((Py\u00f6r\u00e4ll\u00e4))\n\n    ROOT_L_L --&gt;|#60;8.255| ROOT_L_L_L_LEAF((Py\u00f6r\u00e4ll\u00e4))\n    ROOT_L_L --&gt;|#62;#61;8.255| ROOT_L_L_R_LEAF((Autolla))</code></pre>"},{"location":"algoritmit/puut/puu/#datan-jaot_2","title":"Datan jaot","text":"Vasen lehti im_well_rested dst_has_shower required_speed go_by_car 1 0 5.88 0 Oikea lehti im_well_rested dst_has_shower required_speed go_by_car 0 0 10.63 1 1 0 14.2 1"},{"location":"algoritmit/puut/puu/#mita-opimme-esimerkista","title":"Mit\u00e4 opimme esimerkist\u00e4?","text":"<p>Huomaa, ett\u00e4 esimerkki koulutettiin vain 16 rivill\u00e4 dataa. N\u00e4in suppealla datalla on luonnollista, ett\u00e4 p\u00e4\u00e4t\u00f6spuun p\u00e4\u00e4telm\u00e4t eiv\u00e4t aivan edusta todellisuutta. Mik\u00e4 on todellisuus? Todellisuutta ovat ne luonnonvoimat, jotka ohjaavat ilmi\u00f6it\u00e4: t\u00e4ss\u00e4 tapauksessa sit\u00e4 on k\u00e4sin koodattu logiikka, joka esiteltiin yll\u00e4: Datan luomiskertomus.</p> <p>Ent\u00e4p\u00e4 jos generoimme enemm\u00e4n dataa? Alla Decision Tree:n Scikit learn -kirjastolla luotu p\u00e4\u00e4t\u00f6spuu, joka on koulutettu 293 rivill\u00e4 dataa. Koodi on yksinkertainen ja helppo ymm\u00e4rt\u00e4\u00e4, mutta se on my\u00f6s tehokas ja skaalautuva. Voisimme k\u00e4ytt\u00e4\u00e4 my\u00f6s t\u00e4h\u00e4n omaa koodiamme, mutta Scikit-learn tarjoaa mahdollisuuden puun visualisointiin - t\u00e4t\u00e4 ei meid\u00e4n purkkaviritelm\u00e4st\u00e4 l\u00f6ydy. Huomaa, ett\u00e4 tarkkuus on kummassakin kuitenkin sama! T\u00e4m\u00e4n voit testata my\u00f6hemmin koodiharjoituksien avulla.</p> IPython<pre><code>import matplotlib.pyplot as plt\nimport ml.decision_tree as dt\n\nfrom pathlib import Path\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\ndata_train = dt.read_jsonl(Path(\"../../data/bike_or_car/293_train.jsonl\"))\ndata_test = dt.read_jsonl(Path(\"../../data/bike_or_car/100_test.jsonl\"))\n\nX_train = [x[:3] for x in data_train] # First 3 columns\ny_train = [x[-1] for x in data_train] # Last column\n\nX_test = [x[:3] for x in data_test] # Same as above\ny_test = [x[-1] for x in data_test] # But for test data\n\n# Build tree (aka fit the model)\nclf = DecisionTreeClassifier(criterion=\"entropy\") # Default: Gini.\nclf.fit(X_train, y_train)\n</code></pre> <p>T\u00e4m\u00e4n j\u00e4lkeen voimme visualisoida p\u00e4\u00e4t\u00f6spuun seuraavasti:</p> IPython<pre><code>plt.figure(figsize=(10, 8))\nplot_tree(clf, filled=True)\n</code></pre> <p></p> <p>Kuvio 1: P\u00e4\u00e4t\u00f6spuu, joka on luotu Scikit-learn-kirjastolla. Graafissa neli\u00f6n taustav\u00e4ri indikoi enemmist\u00f6luokkaa, joka on kyseisess\u00e4 solmussa. Sininen viittaa autoon, oranssi py\u00f6r\u00e4\u00e4n.</p> <p>Huomaa, ett\u00e4 nyt p\u00e4\u00e4t\u00f6spuu on l\u00f6yt\u00e4nyt luvut, jotka ovat kohtalaisen l\u00e4hell\u00e4 meid\u00e4n m\u00e4\u00e4rittelem\u00e4tt\u00f6m\u00e4n todellisuuden avainlukuja. Mallista ei l\u00f6ydy tarkkaa <code>20 km/h</code> arvoa, mutta l\u00e4helt\u00e4 liippaava <code>19.605 km/h</code> l\u00f6ytyy. Kuinka l\u00e4helle totuutta p\u00e4\u00e4stiin <code>15 km/h</code> ja <code>10 km/h</code> arvojen kanssa? Lue t\u00e4m\u00e4 graafista.</p> <p>Mallin tarkkuutta voit testata 100 rivill\u00e4 testidataa, jotka on ladattu <code>data_test</code>-muuttujaan (ja siit\u00e4 <code>X_test</code> ja <code>y_test</code>-muuttujiin). T\u00e4m\u00e4 onnistuu seuraavalla koodilla:</p> <pre><code>from sklearn.metrics import confusion_matrix\n\n# Predict\ny_pred = clf.predict(X_test)\n\n# Confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\ntn, fp, fn, tp = conf_matrix.ravel()\n\nprint(f\"Confusion matrix:\")\nprint(f\"TP: {tp}, FN: {fn}\")\nprint(f\"FP: {fp}, TN: {tn}\")\nprint(f\"Accuracy: {(tp + tn) / (len(y_test))}\")\n</code></pre> <p>H\u00e4mmennysmatriisi on seuraavanlainen:</p> Ennuste Autolla Ennuste Py\u00f6r\u00e4ll\u00e4 Autolla 71 0 Py\u00f6r\u00e4ll\u00e4 1 28 <p>Kysymys</p> <p>Mik\u00e4 on t\u00e4m\u00e4n mallin tarkkuus?</p> <ul> <li>0.99</li> <li>0.97</li> <li>0.95</li> <li>Jokin muu, mik\u00e4?</li> </ul> <p>Kysymys</p> <p>Miksi yksi False Positive on False Positive? </p> <ul> <li>Kyseinen v\u00e4\u00e4rin ennustettu havainto n\u00e4ytt\u00e4\u00e4 t\u00e4lt\u00e4: <code>(1, 1, 19.95, 0)</code>. Kuinka l\u00f6yt\u00e4isit t\u00e4m\u00e4n koodin avulla? </li> <li>Miss\u00e4 ennuste menee vikaan? Nime\u00e4 oksa, joka on selke\u00e4sti v\u00e4\u00e4r\u00e4ss\u00e4. </li> <li>Pohdi, kuinka t\u00e4t\u00e4 tulosta voisi parantaa.</li> </ul>"},{"location":"algoritmit/puut/tehtavat/","title":"\ud83d\udcdd Teht\u00e4v\u00e4t","text":""},{"location":"algoritmit/puut/tehtavat/#tehtava-automaattivaihteet-pt-1","title":"Teht\u00e4v\u00e4: Automaattivaihteet (Pt. 1)","text":"<p>Kouluta Decision Tree ja/tai Random Forest -luokittelumalli. Teht\u00e4v\u00e4n\u00e4si on luoda koneoppimismalli, joka pyrkii ennustamaan muiden kenttien avulla, onko kyseess\u00e4 automaattiauto. Datan voit ladata alkuper\u00e4isest\u00e4 l\u00e4hteest\u00e4\u00e4n, Kaggle: Car Features and MSRP tai repositoriosta, jossa sit\u00e4 jakavat edelleen sit\u00e4 k\u00e4ytt\u00e4neet. N\u00e4ist\u00e4 yksi on: Github: Car-Prices-Prediction</p> <p>Huomaa, ett\u00e4 datasetti\u00e4 on l\u00e4ht\u00f6kohtaisesti k\u00e4ytetty MSRP:n (Manufacturer's Suggested Retail Price) ennustamiseen, mutta t\u00e4ss\u00e4 teht\u00e4v\u00e4ss\u00e4 keskitymme vain automaattiauton tunnistamiseen. K\u00e4yt\u00e4mme siis eri ennustettavaa muuttujaa kuin useimmissa Kaggle-koodeissa, joita tulet l\u00f6yt\u00e4m\u00e4\u00e4n datasettiin liittyen. T\u00e4m\u00e4 ei poista sit\u00e4, ett\u00e4 saatat saada hyvi\u00e4 ideoita datan k\u00e4sittelyyn muista repositorioista.</p> <p>K\u00e4\u00e4nn\u00e4 arvot esimerkiksi seuraavalla tavalla kahdeksi luokaksi:</p> Alkuper\u00e4inen arvo (str) Uusi arvo (bool) N samplea AUTOMATIC 1 8266 MANUAL 0 2935 AUTOMATED_MANUAL 0/1 626 DIRECT_DRIVE (drop) 68 UNKNOWN (drop) 56 <p>Pudota siis rivit, joissa Transmission Type on <code>DIRECT_DRIVE</code> tai <code>UNKNOWN</code>. Sen sijaan arvon <code>AUTOMATED_MANUAL</code> voit k\u00e4\u00e4nt\u00e4\u00e4 joko 0:ksi tai 1:ksi. Kokeile, kumman kanssa saat paremman tuloksen. Arvo <code>1</code> viittaa siis <code>True</code> arvoon, eli automaattiautoon.</p> <p>Jos k\u00e4yt\u00e4t jotakin muuta jakoa, kuten luokittelet <code>UNKNOWN</code> tai <code>DIRECT_DRIVE</code> vaihteistot manuaaliseksi, dokumentoi se oppimisp\u00e4iv\u00e4kirjassasi ja perustele valintasi.</p> <p>Tip</p> <p>Dataa on k\u00e4ytetty useissa eri projekteissa, koska se l\u00f6ytyy Kagglest\u00e4. Mik\u00e4li et tied\u00e4, mist\u00e4 aloittaa eksploratiivinen data-analyysi ja featureiden muokkaus tai valinta, voit etsi\u00e4 apua Kagglesta. Valtaosa esimerkeist\u00e4 ennustaa hintaa, mutta t\u00e4st\u00e4 huolimatta datan k\u00e4sittely on muiden muuttujien osalta samanlaista. Jos et halua ladata dataa Kagglesta, voit ladata sen my\u00f6s esimerkiksi Github: Car-Prices-Prediction -repositoriosta, mist\u00e4 l\u00f6ytyy my\u00f6s esimerkki\u00e4 datan k\u00e4sittelyst\u00e4.</p>"},{"location":"algoritmit/puut/tehtavat/#vinkkeja","title":"Vinkkej\u00e4","text":""},{"location":"algoritmit/puut/tehtavat/#kirjastot","title":"Kirjastot","text":"<p>Aivan kuten aiemmassa teht\u00e4v\u00e4ss\u00e4, my\u00f6s t\u00e4ss\u00e4 tulet tarvisemaan <code>scikit-learn</code>-kirjastoa. T\u00e4m\u00e4 p\u00e4tee kaikkiin kurssin teht\u00e4viin. Kenties seuraavista l\u00f6ytyy apuja?</p> <pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\n</code></pre>"},{"location":"algoritmit/puut/tehtavat/#one-hot-encoding","title":"One Hot Encoding","text":"<p>Kent\u00e4t <code>Make</code> ja <code>Model</code> ovat kategorisia muuttujia, joilla on todella suuri m\u00e4\u00e4r\u00e4 uniikkeja arvoja. T\u00e4m\u00e4 p\u00e4tee my\u00f6s muihin kenttiin, mutta n\u00e4m\u00e4 kent\u00e4t ovat erityisen ongelmallisia, joten keskityt\u00e4\u00e4n niihin t\u00e4ss\u00e4 vinkiss\u00e4.</p> <p>Tip</p> <p>Kuinka paljon? Selvit\u00e4! Tee eksploratiivinen data-analyysi datasetille.</p> <p>Kent\u00e4t eiv\u00e4t ole numeerisia vaan kategorisia. Ne on muunnettava numeeriseen muotoon. Toisin kuin vaikkapa \"Transmission Type\", jossa on vain muutama uniikki arvo, n\u00e4iden kenttien arvot omaavat suuren granulariteetin eli uniikkien arvojen m\u00e4\u00e4r\u00e4n. T\u00e4m\u00e4 tuskin on mallillle hyv\u00e4ksi: mallin tulee oppia generalisoimaan ongelma, joten on ongemallista, jos datasetiss\u00e4 on vaikkapa vain kolme Bugatti Veyronia.</p> <p>Kenties sinun siis kannattaa k\u00e4sitell\u00e4 data siten, ett\u00e4 k\u00e4yt\u00e4t one hot encoding -menetelm\u00e4\u00e4, joka muuttaa kategoriset muuttujat numeeriseen muotoon siten, ett\u00e4 jokainen uniikki arvo saa oman sarakkeensa, mutta pid\u00e4t vain n yleisint\u00e4. T\u00e4m\u00e4n voi tehd\u00e4 monella tavalla, mutta yksi tapa on OneHotEncoder(..., min_frequency=10), jolloin kaikki ne arvot, jotka esiintyv\u00e4t alle 10 kertaa, muutetaan luokan <code>infrequent</code> edustajaksi.</p> <p>T\u00e4h\u00e4n soveltuu esimerkiksi seuraava luokka:</p> <pre><code>from sklearn.preprocessing import OneHotEncoder\n</code></pre> <p>Tip</p> <p>Lis\u00e4haasteena sarakkeilla on keskin\u00e4inen, toiminnallinen riippuvuus (Make \u2192 Model). T\u00e4m\u00e4 one-to-many suhde, jossa Make on Modelin yl\u00e4luokka, voi vaikuttaa siihen, kuinka One Hot Encoding kannattaa toteuttaa.</p> <p>Keksitk\u00f6 tavan yhdist\u00e4\u00e4 n\u00e4iden tiedot? Jos keksit, muistathan vertailla mallin suorituskyky\u00e4 ennen ja j\u00e4lkeen t\u00e4m\u00e4n yhdist\u00e4misen. Onko mallin tarkkuus parempi vai huonompi? Vai pit\u00e4isik\u00f6 jompi kumpi kentt\u00e4 pudottaa kokonaan?</p>"},{"location":"algoritmit/puut/tehtavat/#tulkittavuus","title":"Tulkittavuus","text":"<p>Puumallien yksi etu, varsinkin neuroverkkoihin n\u00e4hden, on niiden tulkittavuus (engl. interpretability, explainability). Tutustu seuraaviin tapoihin tulkita eri kenttien vaikutusta mallin ennusteeseen:</p> <ul> <li><code>feature_importances_</code>-attribuutti, joka kertoo, kuinka paljon kukin piirre vaikuttaa mallin ennusteeseen.</li> <li><code>plot_tree</code>-funktio, joka piirt\u00e4\u00e4 puun rakenteen ja n\u00e4ytt\u00e4\u00e4, miten eri kent\u00e4t vaikuttavat ennusteeseen.</li> </ul> <p>Voit kokeilla my\u00f6s haastaa itse\u00e4si edistyneemmill\u00e4 tavoilla, kuten SHAP ja sen violin summary plot, mutta \u00e4l\u00e4 keskity niihin liikaa. Muista, ett\u00e4 oppimisp\u00e4iv\u00e4kirjan merkinn\u00e4n aihe ovat puumallit.</p>"},{"location":"data/datasetti/","title":"Datasetti","text":"<p>Info</p> <p>T\u00e4m\u00e4 osio, \"Data\", on tarkoitettu k\u00e4ytett\u00e4v\u00e4ksi yhdess\u00e4 muiden osioiden kanssa. Lue aiheet kerran l\u00e4pi nyt, mutta palaa aiheisiin muiden osioiden yhteydess\u00e4.</p> <p>Koneoppimisen yhteydess\u00e4 data on usein raakaa ja vaatii esik\u00e4sittely\u00e4 ennen kuin se voidaan sy\u00f6tt\u00e4\u00e4 koneoppimismalliin. Esik\u00e4sittely voi sis\u00e4lt\u00e4\u00e4 datan puhdistamista, skaalaamista, muuttujien valintaa ja muuta. Termill\u00e4 \"datasetti\" tarkoitetaan jotakin kokoelmaa dataa. Data ei v\u00e4ltt\u00e4m\u00e4tt\u00e4 ole esimerkiksi taulukkomuodossa - mutta usein se joko on, tai se muokataan taulukkomuotoon ennen koneoppimismallin soveltamista.</p>"},{"location":"data/datasetti/#jarjestyneisyys","title":"J\u00e4rjestyneisyys","text":""},{"location":"data/datasetti/#strukturoitu-data","title":"Strukturoitu data","text":"<p>Aloitetaan strukturoidusta datasta, koska sit\u00e4 on helppo k\u00e4sitell\u00e4 ja ymm\u00e4rt\u00e4\u00e4. Strukturoitu data on dataa, joka on j\u00e4rjestetty taulukkomuotoon. Taulukko koostuu riveist\u00e4 ja sarakkeista. Tyypillisesti strukturoitu data on tallennettu tietokantaan (esim. PostgreSQL, MySQL, SQLite) tai tietovarastoon (esim. Snowflake, Redshift). Se voi olla my\u00f6s taulukkolaskentaohjelman taulukossa (esim. Excel, Google Sheets), mutta t\u00e4ll\u00f6in voitaisiin v\u00e4itell\u00e4 siit\u00e4, tippuuko se osittain strukturoidun datan kategoriaan (ks. alla). Tietokannat ovat tyypillisesti vahvasti tyyppisi\u00e4, kun taas taulukkolaskentaohjelmat ovat usein heikosti tyyppisi\u00e4. Esimerkiksi tietokannassa jokaisella sarakkeella on tyyppi (esim. <code>INTEGER</code>, <code>VARCHAR</code>, <code>DATE</code>), kun taas taulukkolaskentaohjelmassa sarakkeen tyyppi voi muuttua dynaamisesti. Mik\u00e4li kent\u00e4n \"Pituus\" arvo voi olla merkkijono <code>182 cm</code>, tai kvalitatiivinen arvo <code>pitk\u00e4</code>, data ei sis\u00e4ll\u00e4 varsinaista lukkoon ly\u00f6ty\u00e4 struktuuria.</p> Selitt\u00e4v\u00e4 a Selitt\u00e4v\u00e4 b Selitt\u00e4v\u00e4 c Selitett\u00e4v\u00e4 muuttuja [y] 1.01 512 R 1 1.10 124124 Python 0 0.99 42 R 1 0.97 N/A R 1 1.00 96141 Python 0"},{"location":"data/datasetti/#havainnot","title":"Havainnot","text":"<p>Havainnot ovat rivej\u00e4 taulukossa. Huomaa, ett\u00e4 kukin rivi vastaa yht\u00e4 havaintoa. Havainnot ovat datasetin yksitt\u00e4isi\u00e4 tapauksia. Yksi rivi voi edustaa esimerkiksi yht\u00e4 henkil\u00f6\u00e4, yht\u00e4 autoa, yht\u00e4 ajomatkaa, myyntitapahtumaa tai mit\u00e4 tahansa muuta. Saman datasetin rivit edustavat kuitenkin samaa tyyppi\u00e4. Yll\u00e4 olevan taulukon ensimm\u00e4inen rivi olisi siis:</p> <pre><code>row_x0: tuple = (\n    1.01, 512, 'R', 1\n)\n</code></pre>"},{"location":"data/datasetti/#muuttujat","title":"Muuttujat","text":"<p>Muuttujat ovat datasetin sarakkeita, toiselta nimelt\u00e4\u00e4n pystyrivej\u00e4, jotka kuvaavat havaintoja. Ne ovat siis havaintojen piirteit\u00e4 tai dimensioita. Strukturoidun datan tapauksessa muuttujalla on jokin tyyppi, kuten <code>INTEGER</code>, <code>VARCHAR</code> tai <code>DATE</code>. Tyypin lis\u00e4ksi arvo voi useimmiten olla my\u00f6s <code>NULL</code> tai <code>N/A</code>, mik\u00e4 tarkoittaa, ett\u00e4 arvoa ei ole saatavilla.</p> <ul> <li>numeerisia<ul> <li>Pituus</li> <li>Paino</li> <li>Huoneiden m\u00e4\u00e4r\u00e4</li> </ul> </li> <li>kategorisia<ul> <li>Matkustajaluokka</li> <li>Kotimaa</li> </ul> </li> </ul> <p>Data jaetaan usein selitt\u00e4vien ja selitett\u00e4v\u00e4n mukaan kahteen matriisiksi <code>X</code> ja vektoriksi <code>y</code>. <code>X</code> sis\u00e4lt\u00e4\u00e4 selitt\u00e4v\u00e4t muuttujat ja <code>y</code> selitett\u00e4v\u00e4n muuttujan. T\u00e4m\u00e4 on t\u00e4rke\u00e4\u00e4, koska koneoppimismallit vaativat tietynlaista sy\u00f6tett\u00e4. Esimerkiksi lineaarinen regressio vaatii <code>X</code>-matriisin ja <code>y</code>-vektorin.</p> IPython<pre><code>X: list[tuple] = [\n    (1.01, 512, 'R'),\n    (1.10, 124124, 'Python'),\n    (0.99, 42, 'R'),\n    (0.97, None, 'R'),\n    (1.00, 96141, 'Python')\n]\n\ny: list[int] = [1, 0, 1, 1, 0]\n</code></pre> <p>On t\u00e4rke\u00e4\u00e4 huomata, ett\u00e4 on ihmisen p\u00e4\u00e4t\u00f6s, ett\u00e4 mik\u00e4 muuttuja on selitt\u00e4v\u00e4 ja mitk\u00e4 ovat selitt\u00e4vi\u00e4. Kukaan ei est\u00e4 meit\u00e4 ottamasta yll\u00e4 n\u00e4kyv\u00e4\u00e4 datasetti\u00e4 ja kouluttamasta teko\u00e4ly\u00e4, joka pyrkii kolmen muun sarakkeen arvon perusteella ennustamaan arvon <code>Python</code> tai <code>R</code>.</p> <p>On yll\u00e4tt\u00e4v\u00e4nkin tyypillist\u00e4, ett\u00e4 jokin data puuttuu. Paras tapa korjata <code>NULL</code>-ongelma on tehostaa datan ker\u00e4ysprosessia. Emme el\u00e4 t\u00e4ydellisess\u00e4 maailmassa, joten valitettavasti meid\u00e4n on kuitenkin opittava k\u00e4sittelem\u00e4\u00e4n puuttuvaa dataa. Puuttuvaa dataa voidaan k\u00e4sitell\u00e4 monin eri tavoin, kuten poistamalla vialliset havainnot, t\u00e4ytt\u00e4m\u00e4ll\u00e4 puuttuvat arvot keskiarvolla tai mediaanilla. Puuttuvan datan paikkauksessa tai poistamisessa pit\u00e4\u00e4 olla kuitenkin tarkkana: arvon puuttuminen voi olla merkityksellist\u00e4. Eli siis se, ett\u00e4 arvo puuttuu, voi olla itsess\u00e4\u00e4n tietoa, joka auttaa ennustamaan selitett\u00e4v\u00e4\u00e4 muuttujaa.</p> <p>Teht\u00e4v\u00e4</p> <p>Kuvittele, ett\u00e4 ty\u00f6skentelet yritykselle, joka valmistaa e-lukulaitteita. Vikatilanteessa ohjelma l\u00e4hett\u00e4\u00e4 virheraportin valmistajan palvelimille (mik\u00e4li k\u00e4ytt\u00e4j\u00e4 on antanut t\u00e4h\u00e4n suostumuksen). Tutkit n\u00e4iden vikatilanteiden trace dataa. Kaikki muut kent\u00e4t sis\u00e4lt\u00e4v\u00e4t aina jotakin tietoa, mutta kentt\u00e4 <code>MOTHERBOARD_FIRMWARE</code> on ajoittain <code>NULL</code>. Lasket, ett\u00e4 n\u00e4it\u00e4 rivej\u00e4 on datasetiss\u00e4 10 %. Kuinka alkaisit selvitt\u00e4\u00e4, ihan maalaisj\u00e4rke\u00e4 k\u00e4ytt\u00e4en, voiko <code>MOTHERBOARD_FIRMWARE</code>-kent\u00e4n puuttuminen olla merkityksellist\u00e4?</p>"},{"location":"data/datasetti/#osittain-strukturoitu-data","title":"Osittain strukturoitu data","text":"<p>Osittain strukturoitu data (engl. semi-structured data) on dataa, joka ei ole t\u00e4ysin taulukkomuotoista, mutta sill\u00e4 on kuitenkin selke\u00e4 koneluettava skeema. Esimerkiksi HTML-tiedostot ovat osittain strukturoitunutta dataa. Alla on esimerkki hyvin, hyvin yksinkertaisesta HTML-sivusta:</p> index.html<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n    &lt;head&gt;\n        &lt;title&gt;Pizza e-Bible&lt;/title&gt;\n    &lt;/head&gt;\n    &lt;h1&gt;Pizza Ingredients&lt;/h1&gt;\n    &lt;ul&gt;\n        &lt;li&gt;Egg&lt;/li&gt;\n        &lt;li&gt;Ham&lt;/li&gt;\n        &lt;li&gt;Spam&lt;/li&gt;\n    &lt;/ul&gt;\n&lt;/html&gt;\n</code></pre> <p>Yll\u00e4 n\u00e4kyv\u00e4n kaltaista dataa voi louhia koneellisesti esimerkiksi Wikipediasta. Osittain strukturoitu data vaatii huomattavasti enemm\u00e4n k\u00e4sittely\u00e4 kuin strukturoitu data. T\u00e4m\u00e4 k\u00e4sittely on luonnollisesti virheherkk\u00e4 prosessi. Datan skeema voi ajan kanssa el\u00e4\u00e4. Esimerkiksi jos Wikipedian sivun HTML-rakenne muuttuu, koneellinen louhinta voi ep\u00e4onnistua, tai voit p\u00e4\u00e4ty\u00e4 kirjoittamaan v\u00e4\u00e4r\u00e4\u00e4 tietoa tietokantaan.</p> <p>Info</p> <p>Tiedostop\u00e4\u00e4te tai tiedoston formaatti ei itsess\u00e4\u00e4n tee datasta kokonaan tai osittain strukturoitua. Esimerkiksi JSON-tiedosto voi olla t\u00e4ysin strukturoitua dataa, jos se on koneellisesti kirjoitettu skeemaa noudattaen.</p>"},{"location":"data/datasetti/#epastrukturoitu-data","title":"Ep\u00e4strukturoitu data","text":"<p>Data, joka on t\u00e4ysin vailla struktuuria (engl. unstructured data), on kaikkein haastavinta k\u00e4sitell\u00e4. Sanat t\u00e4ysin vailla struktuuria eiv\u00e4t suinkaan tarkoita, ett\u00e4 kyseess\u00e4 olisi t\u00e4ysin kaoottinen data. Yksitt\u00e4inen havainto voi olla esimerkiksi \u00e4\u00e4nitiedosto, kuva, video tai t\u00e4ysin vapaamuotoinen teksti. Tiedostolla itsell\u00e4\u00e4n, kuten <code>.JPG</code>-tiedostolla, on oma rakenne, mutta informaatio tiedostossa on ep\u00e4strukturoitua. Esimerkiksi kuvan pikseli xy-koordinaatissa <code>(100, 100)</code> ei itsess\u00e4\u00e4n kerro mit\u00e4\u00e4n siit\u00e4, onko kyseess\u00e4 kissa vai koira. Kuvan sis\u00e4lt\u00f6\u00e4 tulee siis kuvailla jollakin muulla tavalla numeraalisesti.</p> <p>Mik\u00e4li koulutat teko\u00e4ly\u00e4, joka pyrkii tunnistamaan onko kuvassa kissa vai koira, sinun dataset voi vaikkapa hakemistorakenne:</p> <pre><code>dataset/\n    cat/\n        DSC_0123.jpg\n        web_fluffy.jpg\n        ...\n    dog/\n        IMG_0473.jpg\n        P129719088.jpg\n        ...\n</code></pre> <p>Se, kuuluuko kuva hakemistoon <code>cat/</code> vai <code>dog/</code>, on datasetin selitett\u00e4v\u00e4 muuttuja. Kuvan pikseleist\u00e4 muodostetaan my\u00f6hemmin selitt\u00e4vi\u00e4 muuttujia. T\u00e4h\u00e4n soveltuvat eri feature descriptorit eli piirrekuvaajat, kuten HOG, SIFT, SURF, ORB, LBP, jne.</p> <p>Huomaa, ett\u00e4 saman datan voi tallentaa tietokantaan, mutta se ei tee datasta sen enemp\u00e4\u00e4 strukturoitua. Alla esimerkki, jossa kullakin kuvalla on <code>id</code>, <code>label</code> ja <code>data</code>, joista j\u00e4lkimm\u00e4isin on bin\u00e4\u00e4rimuodossa tallennettu kuva.</p> id label data 1 cat 0x111001010...00111 2 dog 0x001001010...11010 ... ... ..."},{"location":"data/datasetti/#vektorit-ja-matriisit","title":"Vektorit ja matriisit","text":"<p>Yll\u00e4 datasetti on kuvailtu ihmiselle helpossa taulukkomuodossa. Koneoppimismallien tapauksessa data k\u00e4sitell\u00e4\u00e4n yleens\u00e4 matriiseina ja vektoreina. Vektorisoidut operaatiot ovat tehokkaampia kuin silmukat. Tyypillisesti koneoppimisessa k\u00e4ytet\u00e4\u00e4n <code>numpy</code>-kirjastoa edustamaan matriisia <code>X</code> ja vektoria <code>y</code>. Matriisi <code>X</code> on kaksiulotteinen taulukko, jossa rivit ovat havaintoja ja sarakkeet ovat muuttujia. Vektori <code>y</code> on yksiulotteinen taulukko, jossa on selitett\u00e4v\u00e4n muuttujan arvot. Alla on ylemp\u00e4\u00e4 dokumentista tuttu datasetti <code>X</code> ja <code>y</code> <code>numpy</code>-muodossa.</p> IPython<pre><code>import numpy as np\n\nX: np.ndarray = np.array([\n    [1.01, 512, 0],\n    [1.10, 124124, 1],\n    [0.99, 42, 0],\n    [0.97, 0, 0],\n    [1.00, 96141, 1]\n])\n\ny: np.ndarray = np.array([1, 0, 1, 1, 0])\n</code></pre> <p>T\u00e4ll\u00e4 kurssilla suositaan Pythonin listoja, tupleja ja sanakirjoja, koska kurssilla pyrit\u00e4\u00e4n ymm\u00e4rt\u00e4m\u00e4\u00e4n, kuinka algoritmit toimivat. On hyv\u00e4 olla kuitenkin tietoinen, ett\u00e4 usein koneoppimiseen liittyviss\u00e4 matemaattisissa kaavoissa <code>x</code> ja <code>y</code> ovat vektoreita ja matriiseja ja laskuoperaatiot suoritetaan vektorisoituina operaatioina. </p>"},{"location":"data/datasetti/#matemaattinen-kaava","title":"Matemaattinen kaava","text":"<p>Alla n\u00e4kyv\u00e4ss\u00e4 kaavassa <code>X</code> on matriisi, <code>w</code> on vektori ja <code>y_hat</code> on vektori. Se, mit\u00e4 t\u00e4m\u00e4 kaava varsinaisesti tekee esitell\u00e4\u00e4n my\u00f6hemmin. Keskityt\u00e4\u00e4n t\u00e4ss\u00e4 yhteydess\u00e4 syntaksin ymm\u00e4rt\u00e4miseen.</p> \\[ \\hat{y} = Xw \\] <p>Saman voisi kirjoittaa auki alla n\u00e4kyv\u00e4ll\u00e4 tavalla, olettaen ett\u00e4 X on muodoltaan \\(3 \\times 2\\) ja w on pituudeltaan \\(2\\). Vektorin \\(w\\) voi my\u00f6s kuvitella \\(2 \\times 1\\) matriisiksi. Koot voivat olla mit\u00e4 tahansa, kunhan matriisin <code>(m, n)</code> ja vektorin <code>(n)</code> koot t\u00e4sm\u00e4\u00e4v\u00e4t <code>n</code>:n eli sarakkeiden lukum\u00e4\u00e4r\u00e4n osalta.</p> \\[ \\begin{bmatrix} \\hat{y}_{1} \\\\ \\hat{y}_{2} \\\\ \\hat{y}_{3} \\\\ \\end{bmatrix} = \\begin{bmatrix} x_{1,1} &amp; x_{1,2} \\\\ x_{2,1} &amp; x_{2,2} \\\\ x_{3,1} &amp; x_{3,2} \\\\ \\end{bmatrix} \\begin{bmatrix} w_{1} \\\\ w_{2} \\\\ \\end{bmatrix} \\] <p>Matriisin ja vektorin tulo lasketaan siten, ett\u00e4 otetaan kunkin rivi ja kerrotaan se vektorilla, eli otetaan n\u00e4iden pistetulo. Esimerkiksi:</p> \\[ \\begin{bmatrix} \\hat{y}_{1} \\\\ \\hat{y}_{2} \\\\ \\hat{y}_{3} \\\\ \\end{bmatrix} = \\begin{bmatrix} 11 &amp; 22 \\\\ 21 &amp; 22 \\\\ 31 &amp; 22 \\\\ \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\\\ \\end{bmatrix} = \\begin{bmatrix} 11 \\cdot 1 + 22 \\cdot 2 \\\\ 21 \\cdot 1 + 22 \\cdot 2 \\\\ 31 \\cdot 1 + 22 \\cdot 2 \\\\ \\end{bmatrix} \\] <p>Teht\u00e4v\u00e4</p> <p>Kahden matriisin v\u00e4linen v\u00e4linen pistetulo lasketaan hyvin samalla tavalla. Selvit\u00e4, kuinka t\u00e4m\u00e4 toimii.</p> <p>Tehd\u00e4\u00e4n t\u00e4m\u00e4 viel\u00e4 Pythonissa. Mik\u00e4li haluamme luoda matriisin <code>X</code> ja vektorin <code>y</code> k\u00e4ytt\u00e4m\u00e4tt\u00e4 mit\u00e4\u00e4n kirjastoja tai luokkia, voimme kuvastaan niit\u00e4 seuraavalla tavalla:</p> IPython<pre><code>X = [\n    (11, 22),\n    (21, 22),\n    (31, 22),\n]\n\nw = (1, 2)\n</code></pre> <p>Tulon voisi laskea silmukkaa k\u00e4ytt\u00e4en seuraavalla tavalla:</p> IPython<pre><code>def dot(u, v):\n    return sum(u_i * v_i for u_i, v_i in zip(u, v))\n\ny_hat = [dot(x, w) for x in X]\n</code></pre> <p>Numpy\u00e4 k\u00e4ytt\u00e4en mik\u00e4\u00e4n ei muutu, paitsi ett\u00e4 <code>X</code> ja <code>y</code> ovat <code>numpy</code>-matriisi ja -vektori, jolloin voimme hy\u00f6dynt\u00e4\u00e4 numpy-kirjaston metodeja.</p> IPython<pre><code>import numpy as np\n\nX = np.array(\n[\n    (11, 22),\n    (21, 22),\n    (31, 22),\n]\n)\n\nw = np.array([1, 2])\n\n# Option 1: Using the dot method\ny_hat = X.dot(w)\n\n# Option 2: Using the matmul operator\ny_hat = X @ w\n</code></pre> <p>Warning</p> <p>Huomaa, ett\u00e4 <code>X * w</code> on eri asia kuin <code>X @ w</code>. Ensimm\u00e4inen n\u00e4ist\u00e4 on <code>element-wise multiplication</code>, jossa kunkin alkion kohdalla kerrotaan vastaava alkio. J\u00e4lkimm\u00e4inen on matriisitulo, jossa otetaan kunkin rivin pistetulo vektorin kanssa. LateX-kaavoissa <code>u * v</code> olisi \\(u \\odot v\\) ja <code>X @ w</code> olisi \\(Xw\\).</p>"},{"location":"data/datasetti/#vektorit-ilman-numpya","title":"Vektorit ilman Numpy\u00e4","text":"<p>Koneoppimisessa k\u00e4ytet\u00e4\u00e4n usein <code>numpy</code>-kirjastoa. On hyv\u00e4 ymm\u00e4rt\u00e4\u00e4, ett\u00e4 vektori on k\u00e4yt\u00e4nn\u00f6ss\u00e4 vain tuple tai lista, joka reagoi matemaattisiin operaatioihin m\u00e4\u00e4r\u00e4tyll\u00e4 tavalla: esimerkiksi kahden vektorin summa on <code>element-by-element sum</code>. Vastaavasti matriisia voidaan k\u00e4sitell\u00e4 listana vektoreita varsinkin siin\u00e4 tapauksessa, ett\u00e4 matriisi edustaa datasetti\u00e4.</p> <p>Helppo tapa tutustua vektoreiden toimintaa on toteuttaa vektori- ja matriisiluokat itse. Suorituskyvyss\u00e4 natiivi Python-toteutus h\u00e4vi\u00e4\u00e4 merkitt\u00e4v\u00e4sti C-kieleen nojaavalle <code>numpy</code>-kirjastolle, mutta ymm\u00e4rrys vektorien ja matriisien toiminnasta paranee. Alla on esimerkki vektoriluokasta, joka tukee pistetuloa ja elementtikohtaista kertolaskua.</p> <p>Voit my\u00f6s yritt\u00e4\u00e4 luoda oman matriisiluokan, joka tukee matriisituloa ja elementtikohtaista kertolaskua. Jos matriisiluokan tarkoitus on olla s\u00e4il\u00f6 datasetille, voit my\u00f6s lis\u00e4t\u00e4 siihen metodeja, jotka tekev\u00e4t datasetin k\u00e4sittelyst\u00e4 helpompaa: eli lis\u00e4t\u00e4 luokkaan sit\u00e4, mit\u00e4 Pandas tekee. Luokka voi esimerkiksi yll\u00e4pit\u00e4\u00e4 sis\u00e4lt\u00e4miens\u00e4 vektorien nimet, tietotyypit ja tilastoja. Luokka voi my\u00f6s tarjota metodeja datasetin jakamiseen koulutus- ja testidataan, datan sekoittamiseen ja datan esik\u00e4sittelyyn.</p> IPython<pre><code>class Vector:\n    def __init__(self, *args: int|float):\n        self.elements = list(args)\n\n    def __iter__(self):\n        return iter(self.elements)\n\n    def __mul__(self, other):\n        return Vector(*[a * b for a, b in zip(self, other)])\n\n    def __matmul__(self, other):\n        return sum(self * other)\n\n    def __eq__(self, other):\n        return [a == b for a, b in zip(self, other)]\n\nu = Vector(1, 2, 3)\nv = Vector(4, 5, 6)\n\nu * v == Vector(4, 10, 18)\nu @ v == 32\n</code></pre> <p>Teht\u00e4v\u00e4</p> <p>Tutustu <code>ml/vector.py</code>-tiedostossa olevaan <code>Vector</code>-luokkaan. Kokeile importata se esimerkiksi Jupyter Notebookiin ja laskea vektoreiden sek\u00e4 skaalarien v\u00e4lisi\u00e4 operaatioita.</p>"},{"location":"data/enkoodaus/","title":"Enkoodaus","text":"<p>Aiemmin esitettiin, ett\u00e4 koneoppimisessa operaatiot suoritetaan tilastotieteeseen nojaten ja useimmiten vektorisoidussa muodossa. T\u00e4m\u00e4 asettaa k\u00e4yt\u00e4nn\u00f6ss\u00e4 sen vaatimuksen, ett\u00e4 kaikkien muuttujien tulee olla numeerisia. Sanamuotoiset kategoriset muuttujat, kuten arvoavaruus <code>{ \"Cat\", \"Dog\", \"Hamster\" }</code>, eiv\u00e4t ole numeerisia. T\u00e4st\u00e4 syyst\u00e4 kategoriset muuttujat tulee enkoodata numeeriseen muotoon. Kaksi usein k\u00e4ytetty\u00e4 menetelm\u00e4\u00e4 ovat:</p> <ul> <li>Label encoding: Kategoriset muuttujat muutetaan numeeriseen muotoon. Esimerkiksi <code>[Cat, Dog, Dog, Hamster]</code> =&gt; <code>[0, 1, 1, 2]</code>.</li> <li>One-hot encoding:  Kategiruset muuttujat muutetaan useiksi bin\u00e4\u00e4rimuuttujiksi. Esimerkiksi <code>[Cat, Dog, Dog, Hamster]</code> =&gt; <code>[[1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1]]</code>.</li> </ul> <p>Huomaa, ett\u00e4 label encoding tekee jostakin luokasta numerona suuremman kuin toisen. T\u00e4m\u00e4 voi johtaa virheellisiin johtop\u00e4\u00e4t\u00f6ksiin, koska monet koneoppimismallit tulkitsevat suuruusj\u00e4rjestyksen olevan merkityksellinen. Kategoriset muuttujat, kuten t-paidan koko (S, M, L, XL jne.) ovat j\u00e4rjestyksellisi\u00e4 eli ordinaalisia. T\u00e4ll\u00f6in niiden vaihtaminen numeroiksi voi olla perusteltua. Parempi ratkaisu olisi kuitenkin k\u00e4ytt\u00e4\u00e4 esimerkiksi rinnan ymp\u00e4rysmittaa tai muuta oikeaa mittaustulosta.</p> <p>Kaikki kategoriset muuttujat eiv\u00e4t kuitenkaan ole ordinaalisia - ne ovat nominaalisia. Jos datasetiss\u00e4 on esimerkiksi piirre lempiv\u00e4ri, joka on kokonaislukuina enkoodattuna: <code>0: oranssi</code>ja <code>6: Sininen</code>, koneoppimismalli voi tulkita, ett\u00e4 sininen on kuusi kertaa niin suuri kuin oranssi.</p> <p>Question</p> <p>Mit\u00e4 ongelmia koituisi, jos k\u00e4\u00e4nt\u00e4isit kategorisen lempiv\u00e4rin numeraaliseksi k\u00e4ytt\u00e4en v\u00e4rin hue-arvoa? Hue on numeerinen arvo v\u00e4lill\u00e4 0-360, joka kuvaa v\u00e4rin s\u00e4vy\u00e4.</p> <p>T\u00e4m\u00e4n pohtiminen voi olla helpompaa visuaalisesti. K\u00e4yt\u00e4 esimerkiksi W3Schools: HTML HSL and HSLA Colors-interaktiivista ty\u00f6kalua ja mieti, mit\u00e4 v\u00e4rej\u00e4 edustavat todella pienet ja suuret hue-arvot.</p> <p>Jos k\u00e4yt\u00e4mme Datasetti-materiaalista tuttua datasetti\u00e4, jossa kolmas sarake sis\u00e4lt\u00e4\u00e4 vain ja ainoastaan arvoja \"R\" ja \"Python\". Kyseess\u00e4 on tekstimuotoinen kategorinen muuttuja, ja voimme enkoodata sen bin\u00e4\u00e4riseksi One-hot encoding -menetelm\u00e4ll\u00e4 seuraavasti:</p> Selitt\u00e4v\u00e4 a Selitt\u00e4v\u00e4 b is_python Selitett\u00e4v\u00e4 muuttuja [y] 1.01 512 0 1 1.10 124124 1 0 0.99 42 0 1 0.97 N/A 0 1 1.00 96141 1 0 <p>Warning</p> <p>Huomaa, ett\u00e4 emme luo erikseen kahta eri saraketta: <code>is_r</code> ja <code>is_python</code>. Syy t\u00e4lle on multikollineaarisuuden v\u00e4lttely. Emme halua sarakkeita, jotka ovat t\u00e4ysin korreloituneita kesken\u00e4\u00e4n, koska se voi aiheuttaa ongelmia mallin sovittamisessa.</p>"},{"location":"data/jakauma/","title":"Jakauma","text":"<p>Transformaatiot muokkaavat datan jakautumista (engl. distribution), toisin kuin skaalaukseen k\u00e4ytett\u00e4v\u00e4t menetelm\u00e4t. T\u00e4m\u00e4 on tarpeellista ajoittain, kun data on vinoutunutta (engl. skewed) tai siin\u00e4 on reilusti poikkeamia, mik\u00e4 johtaa . Transformatiivinen muunnos voi auttaa tasaamaan vinoutunutta dataa ja tekem\u00e4\u00e4n siit\u00e4 normaalijakautunutta.</p>"},{"location":"data/jakauma/#vinous-skew","title":"Vinous (skew)","text":"<p>Vinous kuvaa datan jakautumista. Normaalijakautuneessa dataa vinous on nolla. Tosiel\u00e4m\u00e4n datasetit poikkeavat usein normaalijakautuneesta datasta. Vinous voi olla positiivinen tai negatiivinen.</p> <p>Vinouman laskemiseksi k\u00e4ytet\u00e4\u00e4n seuraavaa kaavaa:</p> \\[ g_1 = \\frac{\\frac{1}{n} \\sum (x_i - \\overline{x})^3}{(\\frac{1}{n}\\sum (x_i - \\overline{x})^2)^\\frac{3}{2}} \\] IPython<pre><code>def skew(x):\n    n = len(x)\n    mean = np.mean(x)\n\n    nominator = 1/n * np.sum((x - mean) ** 3)\n    denominator = (1/n * np.sum((x - mean) ** 2)) ** (3/2)\n\n    return nominator / denominator\n</code></pre>"},{"location":"data/jakauma/#huipukkuus-kurtosis","title":"Huipukkuus (Kurtosis)","text":"<p>Huippuus kuvaa jakauman muotoa. Huipukkuuden kaava on seuraava:</p> \\[ g_2 = \\frac{\\frac{1}{n} \\sum (x_i - \\overline{x})^4}{\\frac{1}{n}(\\sum (x_i - \\overline{x})^2)^2} - 3  \\] IPython<pre><code>def kurtosis(x):\n    n = len(x)\n    mean = np.mean(x)\n\n    nominator = 1/n * np.sum((x - mean) ** 4)\n    denominator = (1/n * np.sum((x - mean) ** 2)) ** 2\n\n    return (nominator / denominator) - 3\n</code></pre>"},{"location":"data/jakauma/#palkkaesimerkki","title":"Palkkaesimerkki","text":"<p>Generoidaan satunnaisesti dataa, joka edustaa normaalijakaumaa siten, ett\u00e4 keskiarvo on 30 000 ja keskihajonta 10 000. Lis\u00e4t\u00e4\u00e4n dataan noin 1 % ihmisi\u00e4, joiden keskipalkka on 150 000 ja keskihajonta 50 000. T\u00e4m\u00e4 luo vinoutunutta dataa, jossa vinouma on positiivinen (eli kohti suurempia arvoja).</p> IPython<pre><code>import numpy as np\n\n# Generate 10k people's salaries\nnum_samples = 10_000\n\n# Most people have average salaries\nlow_salaries = np.random.normal(\n    loc=30_000, \n    scale=10_000, \n    size=num_samples\n    )\n\n# Some people have high salaries\nhigh_salaries = np.random.normal(\n    loc=150_000, \n    scale=50_000, \n    size=num_samples // 100\n    ) \n\n# Concatenate the two arrays\nsalaries = np.concatenate((low_salaries, high_salaries))\n\n# Remove round to nearest dollar\nsalaries = np.round(salaries, 0)\n\n# Drop negative or zero salaries\nsalaries = salaries[salaries &gt; 0]\n</code></pre> <p>Syntyv\u00e4 data n\u00e4ytt\u00e4\u00e4 seuraavalta:</p> <p></p> <p>Kuvio 1: Palkkadata, joka on vinoutunut kohti suuria arvoja.</p> <p>Huomaa, ett\u00e4 suurin osa palkoista sijaitsee v\u00e4lill\u00e4 0-60 000. Vasemmalla puolella data leikkautuu nollan kohdalla. Oikealla on hajanaisia suuria palkkoja. Suurin palkka t\u00e4ss\u00e4 tasasetiss\u00e4 on 268 898. Jos datasetist\u00e4 lasketaan vinouma ja huipukkuus, saadaan seuraavat tulokset:</p> REPL<pre><code>&gt;&gt;&gt; skewness(salaries)\n6.17\n&gt;&gt;&gt; kurtosis(salaries)\n62.35\n</code></pre> <p>Data on kuvitteellista, joten se ei tietenk\u00e4\u00e4n noudata tosimaailman monimutkaisuutta, mutta t\u00e4ss\u00e4 esimerkiss\u00e4 vinouma on 6.17 ja huipukkuus 62.35. Vinouma on positiivinen, mik\u00e4 johtuu dataan tarkoituksella upotetuista suurista palkoista. Huipukkuus on korkea johtuen samasta syyst\u00e4: outlierit nostavat huipukkuuden arvoa. Datalla, joka noudattaa keskihajontaa (esim. <code>np.random.normal(size=100_000)</code>) kummatkin arvot ovat teoreettisesti 0.</p> <p>Koneoppimismallit, jotka pyrkiv\u00e4t sovittamaan lineaarisen viivan normaalijakaumaan, k\u00e4rsiv\u00e4t vinoutuneesta datasta. T\u00e4h\u00e4n lukeutuvat varsinkin lineaariset regressiomallit, mutta edes neuroverkot eiv\u00e4t ole immuuneja vinoutuneelle datalle.</p>"},{"location":"data/jakauma/#vinouman-korjaus","title":"Vinouman korjaus","text":"<p>Vinoumaa voi pyrki\u00e4 korjaamaan useilla eri funktioilla. Idea on simppeli: aja kukin arvo jonkin funktion, kuten neli\u00f6juuren, l\u00e4pi. Alla on muutama tyypillinen keino esiteltyn\u00e4.</p>"},{"location":"data/jakauma/#log-transformation","title":"Log transformation","text":"<p>Logaritminen muunnos on yksinkertainen tapa. Kukin arvo asetetaan logaritmiin, jonka kantaluku (engl. base) on yleens\u00e4 10 tai e. Logaritmi palauttaa numeron, joka kuvaa, mihin potenssiin kantaluku on korotettava saadakseen alkuper\u00e4isen arvon. L\u00e4hell\u00e4 nollaa olevat luvut pienenv\u00e4t mit\u00e4tt\u00f6m\u00e4n v\u00e4h\u00e4n, suuret luvut paljon. </p> <p>Korjaus on n\u00e4inkin simppeli:</p> REPL<pre><code>&gt;&gt;&gt; log_salaries = np.log(salaries)\n&gt;&gt;&gt; skew(log_salaries)\n-0.98\n&gt;&gt;&gt; kurtosis(log_salaries)\n8.44\n</code></pre>"},{"location":"data/jakauma/#juuri","title":"Juuri","text":"<p>Toinen yleinen muunnos on neli\u00f6juuri tai kuutiojuuri (engl. square or cube root). Kuutiojuuri palauttaa luvun, joka pit\u00e4isi kertoa kolmesti itsens\u00e4 kanssa saadakseen alkuper\u00e4isen arvon. Eli <code>cbrt(8) = 2</code>, koska \\(2 * 2 * 2 = 8\\). Neli\u00f6juuri toimii samalla tavalla, mutta kantaluku on 2. </p> REPL<pre><code>&gt;&gt;&gt; cbrt_salaries = np.cbrt(salaries)\n&gt;&gt;&gt; skew(cube_salaries)\n0.93\n&gt;&gt;&gt; kurtosis(cube_salaries)\n9.16\n</code></pre>"},{"location":"data/jakauma/#box-cox","title":"Box-Cox","text":"<p>Box-Cox-muunnos on suosittu muutos. Se vaatii parametriksi <code>lambda</code>-arvon, mik\u00e4 pit\u00e4\u00e4 tavalla tai toisella l\u00f6yt\u00e4\u00e4. Jos parhaan tuloksen antava lambdan arvo on jokin muu kuin 0, Box-Cox muunnos on seuraava:</p> \\[ x_{boxcox} = \\frac{x^\\lambda - 1}{\\lambda} \\] <p>Jos parhaaksi arvoksi osoittautui 0, kaava on:</p> \\[ x_{boxcox} = \\log(x) \\] <p>Eli Pythonissa:</p> IPython<pre><code>def custom_boxcox(x, lmbda):\n    if lmbda == 0:\n        return np.log(x)\n    return (x ** lmbda - 1) / lmbda\n</code></pre> <p>Oikean lambdan l\u00f6yt\u00e4minen on haastavaa, joten k\u00e4ytet\u00e4\u00e4n t\u00e4ss\u00e4 yhteydess\u00e4 Scipyn <code>boxcox</code>-funktiota, joka etsii parhaan lambdan automaattisesti. Mik\u00e4li haluat kokeilla funktion toteuttamista itse, kannattanee aloittaa log likelihood -funktion toteutuksesta, johon l\u00f6ytyy osviittaa t\u00e4\u00e4lt\u00e4: SciPy Docs: scipy.stats.boxcox_llf. T\u00e4m\u00e4n kurssin puitteissa voimme tyyty\u00e4 kokeilemaan valmista SciPy-funktiota, kuten alla:</p> REPL<pre><code>&gt;&gt;&gt; from scipy.stats import boxcox\n&gt;&gt;&gt; box_salaries, best_lambda = boxcox(salaries)\n&gt;&gt;&gt; skew(box_salaries)\n0.19\n&gt;&gt;&gt; kurtosis(box_salaries)\n6.90\n</code></pre> <p>Muokattu datasetti voidaan palauttaa takaisin alkuper\u00e4iseen skaalaan k\u00e4ytt\u00e4m\u00e4ll\u00e4 <code>scipy.special.inv_boxcox</code>-funktiota.</p> IPython<pre><code>from scipy.special import inv_boxcox\n\n# Invert\ninverted_salaries = inv_boxcox(boxcox_salaries, best_lambda)\n\n# Compare to original (allowing floating point errors)\nnp.allclose(salaries, inverted_salaries)\n</code></pre> <p></p> <p>Kuvio 2: Palkkadata muunnettuna Box-Cox-muunnoksella.</p> <p>Huomaa, ett\u00e4 ongelma ei ole t\u00e4ysin h\u00e4vinnyt, mutta loiventunut huomattavasti.</p>"},{"location":"data/jakauma/#muut-vaihtoehdot","title":"Muut vaihtoehdot","text":"<p>Jatkuvan datan pakottaminen l\u00e4hemm\u00e4s normaalia jakautumista voi parantaa mallin suorituskyky\u00e4. Muista kuitenkin, ett\u00e4 on olemassa muitakin tapoja k\u00e4sitell\u00e4 vinoutunutta dataa.</p> <p>N\u00e4it\u00e4 ovat muiden muassa:</p> <ul> <li>K\u00e4yt\u00e4 mallia, joka ei ole herkk\u00e4 vinoutuneelle data</li> <li>Ryhmittele data<ul> <li>Pienituloiset, keskituloiset, suurituloiset, ...</li> <li>Tai k\u00e4yt\u00e4 kvantiileja (alin 10 %, seuraava 10 %, ..., ylimm\u00e4t 10 %)</li> </ul> </li> <li>Poista outlierit kokonaan (riski!)</li> </ul>"},{"location":"data/skaalaus/","title":"Skaalaus","text":""},{"location":"data/skaalaus/#skaalan-suhteen-herkat-algoritmit","title":"Skaalan suhteen herk\u00e4t algoritmit","text":"<p>Osa koneoppimisalgoritmeista ovat herkki\u00e4 skaalaukselle. T\u00e4m\u00e4 tarkoittaa, ett\u00e4 algoritmin suorituskyky voi rampautua, mik\u00e4li eri piirteiden v\u00e4lill\u00e4 on suuria eroja. Esimerkiksi huoneiden lukum\u00e4\u00e4r\u00e4 on yleens\u00e4 pieni luku, kun taas asunnon hinta dollareita on valtava lukema.</p> <p>Jos mallin virhe perustuu et\u00e4isyyteen, skaalaus on t\u00e4rke\u00e4\u00e4. Esimerkiksi K-means-algoritmi k\u00e4ytt\u00e4\u00e4 et\u00e4isyytt\u00e4 klustereiden muodostamiseen. Jos yksi piirre on suurempi kuin toinen, se vaikuttaa enemm\u00e4n et\u00e4isyyteen. Huomaa, ett\u00e4 skaalauksen tarkka muoto vaihtelee. Kenties piirre puristetaan v\u00e4lille <code>[0,1]</code>, tai kenties se skaalataan siten, ett\u00e4 keskihajonta mahtuu alueelle <code>[-1,1]</code>. N\u00e4ihin tutustutaan alla tarkemmin, mutta tarkistathan aina ett\u00e4 valitsemasi datan esik\u00e4sittelij\u00e4 on se, mit\u00e4 mallisi tarvitsee. Esimerkiksi neuroverkot k\u00e4ytt\u00e4v\u00e4t usein Min-Max skaalausta, joka puristaa datan v\u00e4lille <code>[0,1]</code>.</p> <p>Tip</p> <p>T\u00e4m\u00e4n kurssin puitteissa riitt\u00e4\u00e4 seuraava karkea listaus, jossa  tarkoittaa, ett\u00e4 skaalaus on suositeltavaa ja  tarkoittaa, ett\u00e4 skaalaus ei ole tarpeellista.</p> <ul> <li>Puut </li> <li>Naive Bayes </li> <li>Muut </li> </ul>"},{"location":"data/skaalaus/#skaalauksen-apuvalineet","title":"Skaalauksen apuv\u00e4lineet","text":"<p>Dataa voi kuvata tilastotieteen avulla. Kuvailevia lukuja ovat keskiarvo, mediaani, moodi, varianssi, keskihajonta ja kvartiilit. Datan skaalauksessa tarvitaan tyypillisesti keskiarvoa, varianssia ja keskihajontaa. N\u00e4m\u00e4 lienev\u00e4t jo matematiikasta tuttuja, mutta k\u00e4yd\u00e4\u00e4n ne l\u00e4pi kertauksen vuoksi. Kukin n\u00e4ist\u00e4 esitell\u00e4\u00e4n ensin matemaattisessa muodossa ja sen j\u00e4lkeen Python-koodina. N\u00e4it\u00e4 tarvitaan my\u00f6hemmin skaalausta tehdess\u00e4.</p> <p>T\u00e4m\u00e4 materiaali pohjautuu osin BMC Genomics-sivuston kaavoihin.</p>"},{"location":"data/skaalaus/#keskiarvo-mean","title":"Keskiarvo (mean)","text":"<p>Keskiarvo on kaikkien otannan (engl. sample) lukujen summa jaettuna lukum\u00e4\u00e4r\u00e4ll\u00e4. Huomaa, ett\u00e4 koko populaation keskiarvoa merkataan \\(\\mu\\):lla (lausutaan myy); \\(\\overline{x}\\) on nimenomaan otannan keskiarvo. Jatkossa, kun n\u00e4et <code>x\u0304</code>-symbolin t\u00e4ss\u00e4 dokumentissa, se tarkoittaa keskiarvoa.</p> \\[ {\\overline{x}} = \\frac{\\sum x_{i}}{N} \\] IPython<pre><code>from ml.vector import Vector\n\nx = Vector(-1, 0, 1, 2, 3, 4, 5)\n\ndef mean(x: Vector):\n    return sum(x) / len(x)\n</code></pre>"},{"location":"data/skaalaus/#varianssi","title":"Varianssi","text":"<p>Varianssi kertoo, kuinka paljon data poikkea keskiarvosta. Luku on nostettu neli\u00f6\u00f6n, jotta negatiiviset poikkeamat eiv\u00e4t kumoaisi positiivisia, ja jotta suuret poikkeamat painottuisivat enemm\u00e4n.</p> \\[ s^{2} = \\frac{\\sum \\left( {x_{i} - {\\overline{x}} } \\right) ^{2}}{N - 1} \\] IPython<pre><code>def variance(x: Vector, ddof=1):\n    return sum((x - mean(x))**2) / (len(x) - ddof)\n</code></pre> <p>Miksi - 1?</p> <p>Jakajassa oleva <code>N - 1</code> v\u00e4hent\u00e4\u00e4 populaatiosta yhden asteen vapautta. T\u00e4m\u00e4 yhden vapausaste (engl. degrees of freedom) on k\u00e4yt\u00f6ss\u00e4 otannan (engl. sample) varianssia laskettaessa. Mik\u00e4li N edustaa koko populaatiota, sit\u00e4 ei k\u00e4ytet\u00e4. Huomaa, ett\u00e4 koska jakaja on pienempi, varianssi on suurempi kuin jos jakajana olisi <code>N</code>. Koko populaation varianssin oletetaan siis olevan suurempi kuin otannan varianssin.</p> <p>Jos skaalaat <code>X</code>:\u00e4\u00e4, <code>ddof=0</code> on oikea arvo. Jos skaalaat <code>X_train</code>-dataa, <code>ddof=1</code> on oikea arvo.</p>"},{"location":"data/skaalaus/#keskihajonta","title":"Keskihajonta","text":"<p>Keskihajonta on varianssin neli\u00f6juuri. Se palauttaa neli\u00f6\u00f6n nostetun varianssin takaisin alkuper\u00e4iseen mittayksikk\u00f6\u00f6n. Esimerkiss\u00e4 oletetaan, ett\u00e4 k\u00e4sittelemme otantaa, joten hyv\u00e4ksymme aiemman toteutuksen <code>ddof=1</code> default-arvon.</p> \\[ s = \\sqrt{s^{2}} \\] IPython<pre><code>def std(x: Vector, **kwargs):\n    return variance(x, **kwargs) ** 0.5\n</code></pre>"},{"location":"data/skaalaus/#piirteiden-skaalaus","title":"Piirteiden skaalaus","text":"<p>Piirteiden skaalaus on menetelm\u00e4, jolla yhten\u00e4istet\u00e4\u00e4n eri muuttujien tai piirteiden alue. Tietojenk\u00e4sittelyss\u00e4 sit\u00e4 kutsutaan my\u00f6s datan normalisoinniksi ja se suoritetaan yleens\u00e4 datan esik\u00e4sittelyvaiheessa. Se ottaa datataulukon ja palauttaa uuden taulukon samalla muodolla, mutta skaalattuna valitun menetelm\u00e4n mukaiselle alueelle. Alla on esiteltyn\u00e4 muutama yleinen skaalaukseen liittyv\u00e4 menetelm\u00e4.</p>"},{"location":"data/skaalaus/#keskitys-centering","title":"Keskitys (centering)","text":"<p>Keskitys ei varsinaisesti skaalaa mit\u00e4\u00e4n, mutta se on t\u00e4rke\u00e4 osa alla esiteltyj\u00e4 skaalauksia. Keskityksess\u00e4 lukujen keskiarvo v\u00e4hennet\u00e4\u00e4n jokaisesta arvosta. Toisin sanoen keskiarvo siirret\u00e4\u00e4n nollaan. Mik\u00e4li muuttuja noudattaa normaalijakaumaa, puolet arvoista on positiivisia ja puolet negatiivisia.</p> \\[ {\\widetilde{x}} = x - {\\overline{x}} \\] IPython<pre><code>def center(x):\n    return x - mean(x)\n</code></pre> <p></p> <p>Kuvio 1: Vasemmassa histogrammissa n\u00e4kyy alkuper\u00e4inen data, joka noudattaa suunnilleen normaalijakaumaa. Oikeassa histogrammissa n\u00e4kyy keskitetty data, jossa keskiarvo on 0.</p>"},{"location":"data/skaalaus/#z-score","title":"Z-score","text":"<p>BMC's taulukossa t\u00e4t\u00e4 kutsutaan autoskaalaukseksi (engl. autoscaling). T\u00e4m\u00e4 on yleisin skaalausmenetelm\u00e4. Keskitettu data jaetaan keskihajonnalla, mist\u00e4 lopputuloksena listan lukujen keskiarvo on 0 ja keskihajonta 1. Z-pisteytyksest\u00e4 k\u00e4ytet\u00e4\u00e4n usein termi\u00e4 standardisointi (engl. standardization).</p> \\[ {z} = \\frac{\\widetilde{x}}{s} \\] IPython<pre><code>def z_score(x: Vector):\n    return center(x) / std(x)\n</code></pre> <p></p> <p>Kuvio 2: Vasemmassa histogrammissa on sama data kuin Kuviossa 1. Oikeassa histogrammissa n\u00e4kyy Z-pisteytetty data, jossa keskiarvo on 0 ja keskihajonta on 1.</p> <p>Tip</p> <p>Tulet t\u00f6rm\u00e4\u00e4m\u00e4\u00e4n t\u00e4h\u00e4n usein eri koneoppimisesimerkeiss\u00e4. Mik\u00e4li n\u00e4et jossakin esimerkiss\u00e4 k\u00e4yt\u00f6ss\u00e4 StandardScaler-esik\u00e4sittelij\u00e4n, se on juurikin t\u00e4m\u00e4.</p>"},{"location":"data/skaalaus/#min-max-skaalaus","title":"Min-max skaalaus","text":"<p>Min-max skaalauksessa data skaalataan v\u00e4lille <code>[0,1]</code>. T\u00e4m\u00e4 on meid\u00e4n kurssin kontekstissa eli perinteisess\u00e4 koneoppimisessa hieman Z-scorea eli standardiskaalausta harvinaisempi, mutta se on hy\u00f6dyllinen esimerkiksi neuroverkkojen kanssa. Min-max skaalauksesta k\u00e4ytet\u00e4\u00e4n usein my\u00f6s nimityst\u00e4 normalisointi.</p> \\[ x' = \\frac{ x - min(x) }{ max(x) - min(x) } \\] IPython<pre><code>def min_max(x: Vector):\n    return (x - min(x)) / (max(x) - min(x))\n</code></pre> <p>Tip</p> <p>Huomaa t\u00e4m\u00e4n variantti, jossa skaalaksi voidaan asettaa haluttu alue, esimerkiksi \\([a, b]\\). Alla funktio kirjoitettu siten, ett\u00e4 <code>minmax()</code> viittaa yll\u00e4 n\u00e4kyv\u00e4\u00e4n funktioon:</p> \\[ f(x, a, b) = a + (b - a) \\cdot minmax(x) \\] IPython<pre><code>def minmax_scale(x, a=0, b=1):\n    return a + (b - a) * min_max(x)\n</code></pre>"},{"location":"data/suorituskyky/","title":"Mallin suorituskyky","text":"<p>Koneoppimismallit eiv\u00e4t suinkaan automaattisesti ole t\u00e4ydellisi\u00e4, vaan niiden suorituskyky\u00e4 tulee arvioida ja vertailla muihin malleihin. T\u00e4m\u00e4 vaatii mallintajalta ammattitaitoa sek\u00e4 ty\u00f6kaluja, joilla mallin suorituskyky\u00e4 voidaan arvioida.</p> <p>T\u00e4m\u00e4 dokumentti tulee vastaan niin varhaisessa vaiheessa kurssia, ett\u00e4 sis\u00e4lt\u00f6 saattaa tuntua viel\u00e4 hieman abstraktilta. Palaa t\u00e4h\u00e4n materiaaliin my\u00f6hemmin kunkin koneoppimismallin kohdalla, kun olet valmis arvioimaan mallin suorituskyky\u00e4. On t\u00e4rke\u00e4\u00e4 mietti\u00e4, mist\u00e4 tied\u00e4t, ett\u00e4 malli ennustaa hyvin sellaista dataa, jota se ei ole n\u00e4hnyt ennen.</p> <p>T\u00e4ss\u00e4 dokumentissa k\u00e4sitell\u00e4\u00e4n seuraavia aiheita:</p> <ul> <li>Alisovittaminen (engl. underfitting)</li> <li>Ylisovittaminen (engl. overfitting)</li> <li>Vinouma (engl. bias)</li> <li>Hajonta (engl. variance)</li> </ul> <p>Koneoppimismallin parametrien optimointi on tasapainoilua \u00e4\u00e4rip\u00e4iden v\u00e4lill\u00e4. Tavoitteena on l\u00f6yt\u00e4\u00e4 optimaalinen malli, joka ennustaa hyvin sek\u00e4 opetus- ett\u00e4 testidataa.</p> <p>Warning</p> <p>Huomaa, ett\u00e4 t\u00e4ss\u00e4 luvussa keskityt\u00e4\u00e4n ohjatun oppimisen (engl. supervised learning) malleihin. Ohjaamattoman oppimisen (engl. unsupervised learning) malleissa ei ole koulutus- ja testidataa, joten niiden suorituskyky\u00e4 arviointi on huomatavasti vaikeampaa.</p>"},{"location":"data/suorituskyky/#koulutus-ja-testidata","title":"Koulutus- ja testidata","text":"<p>Ennen kuin pohditaan aihetta yht\u00e4\u00e4n enemp\u00e4\u00e4, on hyv\u00e4 varmistaa, ett\u00e4 tied\u00e4t, mit\u00e4 tarkoitetaan koulutus- ja testidatalla. Koulutusdata on dataa, jolla malli opetetaan. Testidata on dataa, jolla mallin suorituskyky\u00e4 arvioidaan. Testidataa ei saa k\u00e4ytt\u00e4\u00e4 mallin opettamiseen, sill\u00e4 silloin malli ei ole en\u00e4\u00e4 riippumaton. Mik\u00e4li k\u00e4yt\u00e4t Pandas-kirjastoa, testidataa voi olla helppo jakaa koulutusdatasta <code>train_test_split</code> funktiolla:</p> IPython<pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv(\"data.csv\")\nX_train, X_test = train_test_split(data, test_size=0.2)\n</code></pre> <p>Usein testi- ja koulutusdata jaetaan suhteessa 80/20 tai 70/30. T\u00e4m\u00e4 tarkoittaa, ett\u00e4 80% tai 70% datasta k\u00e4ytet\u00e4\u00e4n koulutukseen ja loput testaukseen. T\u00e4m\u00e4 ei ole kiveen hakattu s\u00e4\u00e4nt\u00f6, vaan riippuu datasta ja mallista, kuinka paljon dataa tarvitaan koulutukseen. Data tyypillisesti sekoitetaan ennen sen jakamista. Puhdas Python-ratkaisu datan jakamiseksi olisi:</p> IPython<pre><code>import random\n\nX = [\n    (1, 52.7),\n    (0, 51.3),\n    ...\n    (1, 49.2),\n    (0, 50.1)\n]\n\n# Shuffle\nrandom.shuffle(X)\n\n# Split point\ni = int(len(X) * 0.8)\n\n# Split\nX_train = X[:i]\nX_test = X[i:]\n\n# All samples are included but in two different sets\nassert len(X_train) + len(X_test) == len(X)\n</code></pre>"},{"location":"data/suorituskyky/#ali-ja-ylisovittaminen","title":"Ali- ja ylisovittaminen","text":"<p>Alisovittaminen (engl. underfitting) ja ylisovittaminen (engl. overfitting) ovat ongelmatekij\u00e4\u00e4 konemalleissa, ja jokainen malli etsii tasapainoa n\u00e4iden kahden v\u00e4lill\u00e4.</p> <p></p> <p>Kuvio 1: Yhdist\u00e4 pisteet -piirroskirjassa ihmisen teht\u00e4v\u00e4 on yhdist\u00e4\u00e4 pisteet numeroidusti ja oikein. Koneoppimismallin teht\u00e4v\u00e4 olisi pikemminkin etsi\u00e4 ymp\u00e4ripy\u00f6re\u00e4 kissan muoto annetuilla havainnoilla.</p> <p></p> <p>Kuvio 2: Scikit-learnin dokumentaatiosta poimittu kuva, joka havainnollistaa alisovittamista ja ylisovittamista. Katso kuvan luonut koodi selityksineen: Underfitting vs. Overfitting</p>"},{"location":"data/suorituskyky/#vinouma","title":"Vinouma","text":"<p>Vinouma (engl. bias) on mallin virhe, joka johtuu v\u00e4\u00e4rist\u00e4 oletuksista. Vinouman tapauksessa malli alisovittaa dataa, eli malli ei kykene selitt\u00e4m\u00e4\u00e4n ilmi\u00f6n monimutkaisuutta. Malli on siis liian yksinkertainen datan monimutkaisuuteen n\u00e4hden. Kaikissa maailman malleissa on jokin m\u00e4\u00e4r\u00e4 vinoumaa, hajontaa ja kohinaa. Mallin virheen voi ajatella koostuvan n\u00e4ist\u00e4 tekij\u00f6ist\u00e4:</p> \\[ \\text{Virhe} = \\text{Vinouma} + \\text{Hajonta} + \\text{Kohina} \\] <p>Vinoumaa voidaan mitata ottamalla satunnaisia otantoja datasta (engl. bootstrapping) ja laskemalla mallin virhe. Jos mallin virhe pysyy otannasta huolimatta suurena, malli on vahvasti vinoutunut (engl. biased). T\u00e4st\u00e4 voidaan tulkita, ett\u00e4 my\u00f6s testidatan virhe on suuri. T\u00e4ten koulutusdatan lis\u00e4\u00e4minen ei poista ongelmaa; malli on liian yksinkertainen selitt\u00e4m\u00e4\u00e4n ongelmaa, joten mitenp\u00e4 se voisi? Vinoumaa voidaan korjata lis\u00e4\u00e4m\u00e4ll\u00e4 mallin monimutkaisuutta. Se, miten mallin monimutkaisuutta lis\u00e4t\u00e4\u00e4n, selvi\u00e4\u00e4 kurssin aikana.</p> <p>Alla olevassa kuviossa on tilanne, jossa malli on biased. Koulutusdataa on vain 10 pistett\u00e4, mutta vaikka kouluttaisit mallin enemm\u00e4ll\u00e4 datalla, malli ei kykenisi selitt\u00e4m\u00e4\u00e4n ilmi\u00f6t\u00e4. Ei ole sellaista suoraa viivaa, joka selitt\u00e4isi parabolisen k\u00e4yr\u00e4n. Huomaa, ett\u00e4 vinouma on siis oletus, ett\u00e4 ilmi\u00f6 on suora.</p> <p></p> <p>Kuvio 3: Malli on yh\u00e4 liian yksinkertainen eli alisovittaa dataa. Eri otannat (bootstrapit) tuottavat kaikki suuren virheen.</p> <p>Teht\u00e4v\u00e4</p> <p>Ihminen on er\u00e4\u00e4nlainen koneoppimismalli, joka osaa luokitella n\u00e4kem\u00e4\u00e4ns\u00e4. Tutustu aiheeseen kognitiivinen vinouma ja mieti, miten se suhtautuu koneoppimismallin vinoumaan.</p>"},{"location":"data/suorituskyky/#hajonta","title":"Hajonta","text":"<p>Hajonta (engl. variance) on vinouman vastakohta.  Hajonnan tapauksessa malli ylisovittaa dataa eli se pit\u00e4\u00e4 mit\u00e4tt\u00f6mi\u00e4kin yksityiskohtia merkitt\u00e4vin\u00e4 - jopa pelkk\u00e4\u00e4 kohinaa. Hajonta on mallin virhe, joka johtuu siit\u00e4, ett\u00e4 malli on liian monimutkainen datan monimutkaisuuteen tai m\u00e4\u00e4r\u00e4\u00e4n n\u00e4hden.</p> <p>Hajonnan tunnistaa usein siit\u00e4, ett\u00e4 koulutusdatan virhe on pieni, mutta testidatan virhe on suuri - ja mallin monimutkaisuuden lis\u00e4\u00e4ntyess\u00e4 t\u00e4m\u00e4 ero kasvaa. Mallin parametrit heilahtavat reilusti, jos koulutat mallin useamman kerran saman datan eri subsetilla, koska malli on liian herkk\u00e4.</p> <p>Tip</p> <p>Bias ja variance eiv\u00e4t ole lukuja, joita saisi yht\u00e4 helposti esille kuin vaikkapa tarkkuus tai f1-score. Jos haluat penkoa, voit yritt\u00e4\u00e4 k\u00e4ytt\u00e4\u00e4 mlxtend-kirjaston bias_variance_decomp: Bias-variance decomposition for classification and regression losses-artikkelissa esitelty\u00e4 funktiota.</p>"},{"location":"data/suorituskyky/#regularisointi","title":"Regularisointi","text":"<p>Regularisointi on menetelm\u00e4, jolla voidaan v\u00e4hent\u00e4\u00e4 ylisovittamista. Yleisesti regularisointi on nimenomaan gradient descent -menetelmien yhteydess\u00e4 k\u00e4ytetty termi, mutta muiden kurssin koneoppimisalgoritmien tapauksessa sille l\u00f6ytyy yleisesti jokin vastine, jolla mallin ylisovittamista voidaan v\u00e4hent\u00e4\u00e4. Esimerkiksi k-NN -mallissa voidaan s\u00e4\u00e4t\u00e4\u00e4n <code>k</code>:n arvoa. P\u00e4\u00e4t\u00f6spuualogritmit ovat luontaisesti herkki\u00e4 ja t\u00e4ten ylisovittavia, mutta niiden ylisovittamista voidaan v\u00e4hent\u00e4\u00e4 rajoittamalla puun syvyytt\u00e4 tai rakentamalla useista pienist\u00e4 puista koostuva mets\u00e4. T\u00e4t\u00e4 harjoitellaan t\u00e4m\u00e4n kurssin aikana.</p> <p>Regularisointi on siis yleinen termi, joka tarkoittaa mallin ylisovittamisen v\u00e4hent\u00e4mist\u00e4.</p>"},{"location":"data/suorituskyky/#trade-off","title":"Trade-off","text":"<p>Huomaa, ett\u00e4 ylisovittamisen ja alisovittamisen v\u00e4lill\u00e4 on tasapaino, jota kutsutaan trade-offiksi. Ellei ennustettu malli noudata t\u00e4ydellisesti ilman kohinaa jotakin matemaattista kaavaa, malli on aina v\u00e4kisinkin ali- tai ylisovittava. Tavoitteena on l\u00f6yt\u00e4\u00e4 optimaalinen malli, joka ennustaa hyvin sek\u00e4 opetus- ett\u00e4 testidataa. Jos malli ennustaa hyvin opetusdataa, mutta huonosti testidataa, se on ylisovittava. Jos malli ennustaa huonosti sek\u00e4 opetus- ett\u00e4 testidataa, se on alisovittava. Alla on taulukko, joka kuvaa, miten jompaa kumpaa \u00e4\u00e4rip\u00e4\u00e4t\u00e4 voidaan korjata.</p> Ylisovitus Alisovitus Mallin monimutkaisuus Laske Nosta ...tai regularisaatio Nosta Laske # Muuttujaa Poista muuttujia Tehtaile lis\u00e4\u00e4 muuttujia # Havaintoa Ker\u00e4\u00e4 lis\u00e4\u00e4 havaintoja --- <p>Yll\u00e4 oleva taulukko tarjoaa ratkaisuehdotuksen yli- tai alisovittamisen tapauksessa. My\u00f6s ensemble-menetelm\u00e4t, kuten Random Forest ja Gradient Boosting, ovat hyvi\u00e4 tapoja v\u00e4hent\u00e4\u00e4 ylisovittamista. N\u00e4m\u00e4 menetelm\u00e4t k\u00e4ytt\u00e4v\u00e4t useita malleja, jotka yhdess\u00e4 ennustavat paremmin kuin yksitt\u00e4inen malli.</p> <p></p> <p>Kuvio 5: Kuvassa on esitetty mallin monimutkaisuuden vaikutus virheeseen. Virhe ilmaistaan sanalla \"validation\", mutta voit tulkita sen koulutusdatan virheeksi t\u00e4ss\u00e4 yhteydess\u00e4. Kun mallin monimutkaisuutta lis\u00e4t\u00e4\u00e4n koulutusvirhe laskee. Alisovitus vaihtuu vaihtua ylisovitukseksi, kun yleistett\u00e4vyys heikkenee. Kuvaajassa t\u00e4t\u00e4 ilment\u00e4\u00e4 koulutusvirheen ja testivirheen et\u00e4isyys toisistaan.</p>"},{"location":"data/suorituskyky/#mittariston-valinta","title":"Mittariston valinta","text":"<p>Yll\u00e4 olevissa esimerkeiss\u00e4 k\u00e4ytetty virhe oli <code>MSE</code>. Datalla ei ollut luokkaa vaan juokseva arvo, eli kaikissa yll\u00e4 olevissa kuvissa ongelma oli tyyppi\u00e4 regressio. Luokitteluongelmissa k\u00e4ytet\u00e4\u00e4n erilaisia mittareita. Alla on listattu yleisimm\u00e4t mittarit, joita k\u00e4ytet\u00e4\u00e4n koneoppimismallien suorituskyvyn arvioimiseen kussakin koneoppimismallityypiss\u00e4.</p>"},{"location":"data/suorituskyky/#regressiomallit","title":"Regressiomallit","text":""},{"location":"data/suorituskyky/#mse","title":"MSE","text":"<p>MSE (Mean Squared Error) on virhefunktio, joka laskee keskim\u00e4\u00e4r\u00e4isen virheen neli\u00f6summan. Se on yksi yleisimmist\u00e4 virhefunktioista regressiomalleissa, ja se lasketaan seuraavalla kaavalla:</p> \\[ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - f(x_i))^2 \\] <p>Kyseist\u00e4 virhefunktiota k\u00e4ytet\u00e4\u00e4n Normaaliyht\u00e4l\u00f6 -materiaalissa.</p> <p>Huomaa, ett\u00e4 virhe on nostettu neli\u00f6\u00f6n. Jos ennustat esimerkiksi asunnon hintaa ja MSE on 40,000 euroa, se tarkoittaa, ett\u00e4 nimenomaan neli\u00f6summa virheest\u00e4 on 40,000 euroa. Neli\u00f6 voidaan palauttaa alkuper\u00e4iseen skaalaan ottamalla neli\u00f6juuri virheest\u00e4: <code>sqrt(40_000)</code> palauttaa arvon <code>200</code>, koska <code>200 * 200 = 40_000</code>. Neli\u00f6summaa k\u00e4ytet\u00e4\u00e4n mallin koulutuksessa, mutta alkuper\u00e4iseen skaalaan palautettu <code>RMSE</code> (Root Mean Squared Error) on helpompi ymm\u00e4rt\u00e4\u00e4 evaluaatiovaiheessa. Koneoppimismalli ei siis suinkaan t\u00e4ss\u00e4 tapauksessa ennustanut hintoja 40 000 euroa v\u00e4\u00e4rin, jos virhett\u00e4 k\u00e4sitell\u00e4\u00e4n alkuper\u00e4isess\u00e4 skaalassa (eli euroina eik\u00e4 neli\u00f6euroina). Alkuper\u00e4iseen skaalaan palautettu virhe on 200 euroa, mik\u00e4 on mit\u00e4t\u00f6n ero, olettaen ett\u00e4 asunnot maksavat kymmeni\u00e4 tai satoja tuhansia.</p> <p>Sek\u00e4 MSE:n ett\u00e4 RMSE:n voi laskea my\u00f6s Scikit-Learn kirjaston funktioilla:</p> IPython<pre><code>from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import root_mean_squared_error\n\nX_train, X_test, y_train, y_test = ... # Load data\n\n# Train any regression model here\n#   model = SomeRegressionModel()\n#   model.fit(X_train, y_train)\n#   y_pred = model.predict(X_test)\n\n# Calculate the Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\n\n# These should be equal\nassert rmse == root_mean_squared_error(y_test, y_pred)\n</code></pre>"},{"location":"data/suorituskyky/#r2","title":"R^2","text":"<p>R^2-luku kuvaa selitysastetta (engl. coefficient of determination) prosentteina v\u00e4lill\u00e4 0.00 - 1.00. Jos R^2 on 0.75, se tarkoittaa, ett\u00e4 75% muuttujan vaihtelusta voidaan selitt\u00e4\u00e4 mallilla. Loput ovat virhett\u00e4, jonka selitt\u00e4\u00e4 jokin muu tekij\u00e4.</p> <p>R^2 voidaan laskea seuraavalla kaavalla:</p> \\[ R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}} \\] <p>Jossa: </p> <ul> <li>RSS (Residual Sum of Squares) on virheiden neli\u00f6summa</li> <li>TSS (Total Sum of Squares) on kokonaisneli\u00f6summa. </li> </ul> <p>Huomaa, ett\u00e4 RSS on siis sama kuin MSE, mutta jakolasku j\u00e4tet\u00e4\u00e4n tekem\u00e4tt\u00e4 (eli \"mean\"). Matemaattisina yht\u00e4l\u00f6in\u00e4 RSS ja TSS ovat:</p> \\[ RSS = \\sum_{i=1}^{n} (y_i - f(x_i))^2 \\] \\[ TSS = \\sum_{i=1}^{n} (y_i - mean(y))^2 \\] <p>Pythonina sen voi kirjoittaa alla olevalla tavalla, ja todistaa oikeasi vertaamalla sit\u00e4 Scikit Learnin <code>r2_score</code> -funktion palauttamaan arvoon.</p> <pre><code>import sklearn\nimport numpy as np\n\ndef rss(y_true, y_pred):\n    rss = sum((y_true - y_pred) ** 2)\n    return rss\n\ndef tss(y_true):\n    mean_y = sum(y_true) / len(y_true)\n    tss = sum((y_true - mean_y) ** 2)\n    return tss\n\ndef r_squared(y_true, y_pred):\n    rss_value = rss(y_true, y_pred)\n    tss_value = tss(y_true)\n    r_squared = 1 - (rss_value / tss_value)\n    return r_squared\n\n# Fake data and predictions\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])\n\nr_squared(y_true, y_pred) == sklearn.metrics.r2_score(y_true, y_pred)\n</code></pre>"},{"location":"data/suorituskyky/#luokittelumallit","title":"Luokittelumallit","text":"<p>Koska luokittelumallit ennustavat luokkaa, ei jatkuvaa arvoa, mallin tarkkuutta voidaan arvioida laskemalla oikeita ja v\u00e4\u00e4ri\u00e4 ennusteita kokonaisluvuin. N\u00e4it\u00e4 mittareita ei tehd\u00e4 t\u00e4ll\u00e4 kurssilla k\u00e4sin, mutta kun teet harjoituksia scikit-learnin kanssa, sinun tulee osata arvioida luokittelumallin suorituskyky\u00e4. Toisin sanoen sinun tulee osata lukkea ja ymm\u00e4rt\u00e4\u00e4 alla esiteltyjen mittareiden tuloksia. Alla on suora esimerkki tulosteesta, joka on lainattu Recognizing hand-written digits-esimerkist\u00e4 Scikit-learnin dokumentaatiosta.</p> <pre><code>Classification report for classifier SVC(gamma=0.001):\n              precision    recall  f1-score   support\n\n           0       1.00      0.99      0.99        88\n           1       0.99      0.97      0.98        91\n           2       0.99      0.99      0.99        86\n           3       0.98      0.87      0.92        91\n           4       0.99      0.96      0.97        92\n           5       0.95      0.97      0.96        91\n           6       0.99      0.99      0.99        91\n           7       0.96      0.99      0.97        89\n           8       0.94      1.00      0.97        88\n           9       0.93      0.98      0.95        92\n\n    accuracy                           0.97       899\n   macro avg       0.97      0.97      0.97       899\nweighted avg       0.97      0.97      0.97       899\n</code></pre> <p>Kyseess\u00e4 on monen luokan luokitteluongelma, jossa jokaiselle luokalle on laskettu <code>precision</code>, <code>recall</code> ja <code>f1-score</code>. Luokkia eli uniikkeja <code>y</code>-arvoja on 10 kappaletta. Huomaa, ett\u00e4 monissa t\u00e4ll\u00e4 kurssilla k\u00e4ytetyiss\u00e4 esimerkeiss\u00e4 on vain kaksi luokkaa. Jotta s\u00e4\u00e4styisimme v\u00e4hemm\u00e4ll\u00e4 laskemisella, k\u00e4\u00e4nnet\u00e4\u00e4n sama ongelma siten, ett\u00e4 meill\u00e4 on vain kaksi luokkaa. Ennustetaan sit\u00e4, ett\u00e4 onko numero luku 3 vai jokin muu.</p> IPython<pre><code>from sklearn import datasets, metrics, svm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\ndigits = datasets.load_digits()\n\n# Convert into binary problem. (1)\ny = digits.target == 3\n\n# Fit and predict\nX_train, X_test, y_train, y_test = train_test_split(\n    digits.data, \n    y, \n    test_size=0.3, \n    random_state=160 # Seed (2)\n)\nclf = svm.SVC(gamma=0.001)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint(classification_report(\n    y_test, \n    y_pred, \n    target_names=['Luku N', 'Luku 3'] # (3)\n))\n</code></pre> <ol> <li>Alkuper\u00e4inen y sis\u00e4lt\u00e4\u00e4 luvut <code>range(0, 10)</code>. Luomme uuden <code>y</code>:n eli targetin, jossa on arvot <code>True</code> ja <code>False</code> (lukuina 1 ja 0). Jos y on True, luku on 3. Muutoin se on jokin muu luku (0-2 tai 4-9).</li> <li>Seed on k\u00e4sin valittu sellaiseksi, ett\u00e4 saamme v\u00e4hint\u00e4\u00e4n yhden False Positiven ja False Negativen.</li> <li>Nimit\u00e4mme lukua 3 <code>Luku 3</code> ja kaikkia muita <code>Luku N</code>.</li> </ol> stdout<pre><code>              precision    recall  f1-score   support\n\n      Luku N       0.99      1.00      1.00       479\n      Luku 3       0.98      0.95      0.97        61\n\n    accuracy                           0.99       540\n   macro avg       0.99      0.97      0.98       540\nweighted avg       0.99      0.99      0.99       540\n</code></pre>"},{"location":"data/suorituskyky/#hammennysmatriisi","title":"H\u00e4mmennysmatriisi","text":"<p>Tarvitsemme precision, recall ja f1-score arvojen laskemiseksi er\u00e4\u00e4nlaisia totuustestien lukemia, joita ovat True Positive (TP), True Negative (TN), False Positive (FP) ja False Negative (FN). N\u00e4m\u00e4 arvot ovat t\u00e4rkeit\u00e4, kun arvioidaan luokittelumallin suorituskyky\u00e4. T\u00e4ss\u00e4 on lyhyt selitys n\u00e4ille termeille.</p> <ul> <li>True Positive (TP): Luku 3 ennustettiin 3:ksi.</li> <li>True Negative (TN): Luku N ennustettiin N:ksi.</li> <li>False Positive (FP): Luku N ennustettiin 3:ksi.</li> <li>False Negative (FN): Luku 3 ennustettiin N:ksi.</li> </ul> <p>Note</p> <p>Kuvittele malli, joka ennustaa laboratoriomittausten valossa, onko sinulla jokin sairaus. Huomaa, ett\u00e4 <code>False Negative</code> on tilanne, jossa malli ennustaa, ett\u00e4 sinulla ei ole sairautta, vaikka todellisuudessa sinulla on. Mik\u00e4li saat t\u00e4m\u00e4n ennusteen, sinua ei ohjata jatkotutkimuksiin, joten sairaus j\u00e4\u00e4 hoitamatta. Mik\u00e4li saat <code>False Positive</code> ennusteen, sinut ohjataan jatkotutkimuksiin, mutta todellisuudessa sinulla ei ole sairautta. Jatkotutkimukset ovat kalliita ja turhia, mutta eik\u00f6 ole humaanimpaa olla turhan varovainen kuin j\u00e4tt\u00e4\u00e4 sairaus hoitamatta?</p> <p>T\u00e4m\u00e4 mielikuva osoittaa, ett\u00e4 kaikki v\u00e4\u00e4r\u00e4t vastaukset eiv\u00e4t ole samanarvoisia. Usein <code>False Negative</code> on pahempi kuin <code>False Positive</code>.</p> <p>Voimme laskea h\u00e4mmennysmatriisin Scikit-Learnin <code>confusion_matrix</code> -funktiolla. Luodaan n\u00e4m\u00e4 arvot palauttava funktio selvyyden ja kurssin hengen vuoksi \"from scratch\".</p> IPython<pre><code>def tp_fp_tn_fn(y_test, y_pred):\n    # Init\n    TP, TN, FP, FN = 0, 0, 0, 0\n\n    # Count\n    for pair in zip(y_test, y_pred):\n        match pair:\n            case (1, 1):\n                TP += 1\n            case (0, 0):\n                TN += 1\n            case (0, 1):\n                FP += 1\n            case (1, 0):\n                FN += 1\n\n    return TP, FP, TN, FN\n\nTP, FP, TN, FN = tp_fp_tn_fn(y_test, y_pred)\nprint(f\"TP: {TP}, FP: {FP}, TN: {TN}, FN: {FN}\")\n</code></pre> stdout<pre><code>TP: 58, FP: 1, TN: 478, FN: 3\n</code></pre> <p>Yll\u00e4 olevassa tulosteessa asetetut luvut laitetaan usein matriisiin, jossa pystyakseli edustaa ennustettua luokkaa ja vaaka-akseli todellista luokkaa. T\u00e4m\u00e4 matriisi on h\u00e4mmennysmatriisi. Huomaa, ett\u00e4 kenttien j\u00e4rjestys voi vaihdella ajoittain. J\u00e4rjestys riippuu siit\u00e4, miten akselit on m\u00e4\u00e4ritelty. Meid\u00e4n tapauksessa rivi on todellinen luokka, sarake on ennustettu luokka.</p> Ennustettu True Ennustettu False True TP FN False FP TN <p>Teht\u00e4v\u00e4</p> <p>K\u00e4y katsomassa yll\u00e4 mainitusta Scikit-learnin esimerkist\u00e4, milt\u00e4 h\u00e4mmennysmatriisi n\u00e4ytt\u00e4\u00e4, kun mukana ovat kaikki 10 luokkaa. T\u00e4ss\u00e4 viel\u00e4 linkki uudestaan: Recognizing hand-written digits</p> <p>Kun sy\u00f6t\u00e4mme oikeat target-nimet ja arvot, taulukko n\u00e4ytt\u00e4\u00e4 t\u00e4lt\u00e4:</p> Ennustettu luku 3 Ennustettu luku N Luku 3 58 3 Luku N 1 478"},{"location":"data/suorituskyky/#accuracy","title":"Accuracy","text":"<p>Nyt kun meill\u00e4 on tiedossa h\u00e4mmennysmatriisin arvot, voimme helposti laskea tarkkuuden (engl. accuracy). Tarkkuus on prosenttiosuus oikeista ennusteista. Se lasketaan seuraavalla kaavalla:</p> \\[ \\begin{align*}     \\text{Accuracy} &amp;= \\frac{TP + TN}{TP + TN + FP + FN} \\\\                     &amp;= \\frac{536}{540} \\\\                     &amp;= 0.9926 \\end{align*} \\] <p>Voimme tarkistaa, ett\u00e4 laskutoimituksemme t\u00e4sm\u00e4\u00e4 Scikit-learningin <code>accuracy_score</code> -funktion palauttamaan arvoon. Huomaa, ett\u00e4 <code>TP + TN + FP + FN</code> on yht\u00e4 suuri kuin <code>len(y_test)</code> eli koko testidatan havaintojen m\u00e4\u00e4r\u00e4. Jakaja on siis kaikki data.</p> IPython<pre><code>acc = (TP + TN) / (TP + TN + FP + FN)\nassert metrics.accuracy_score(y_test, y_pred) == acc\n</code></pre> <p>Tip</p> <p>Tarkkuus on siis lyhyesti: kuinka monta prosenttia ennusteista oli oikein.</p>"},{"location":"data/suorituskyky/#recall","title":"Recall","text":"<p>L\u00f6ytyvyysarvo (engl. recall, sensitivity) on yht\u00e4 helppo laskea.</p> \\[ \\begin{align*}     \\text{Recall} &amp;= \\frac{TP}{TP + FN} \\\\                   &amp;= \\frac{58}{61} \\\\                   &amp;\\approx 0.95 \\end{align*} \\] <p>Voimme tarkistaa, ett\u00e4 laskutoimituksemme t\u00e4sm\u00e4\u00e4 Scikit-learningin <code>recall_score</code> -funktion palauttamaan arvoon.</p> IPython<pre><code>recall = TP / (TP + FN)\nassert metrics.recall_score(y_test, y_pred) == recall\n</code></pre> <p>Tip</p> <p>L\u00f6ytyvyysarvo kertoo, kuinka monta prosenttia me ennustimme oikein, jos huomioidaan vain True caset (\"ylempi rivi\").</p>"},{"location":"data/suorituskyky/#precision","title":"Precision","text":"<p>My\u00f6s positiivinen ennustearvo (engl. precision) on helppo laskea.</p> \\[ \\begin{align*}     \\text{Precision} &amp;= \\frac{TP}{TP + FP} \\\\                     &amp;= \\frac{58}{59} \\\\                     &amp;\\approx 0.98 \\end{align*} \\] <p>Voimme tarkistaa, ett\u00e4 laskutoimituksemme t\u00e4sm\u00e4\u00e4 Scikit-learningin <code>precision_score</code> -funktion palauttamaan arvoon.</p> IPython<pre><code>precision = TP / (TP + FP)\nassert metrics.precision_score(y_test, y_pred) == precision\n</code></pre> <p>Tip</p> <p>Precision kertoo, kuinka monta prosenttia me ennustimme oikein, jos huomioidaan vain ennustetut Truet (\"vasen sarake\").</p>"},{"location":"data/suorituskyky/#f1-score","title":"F1 score","text":"<p>Yhten\u00e4 hyv\u00e4n mallin mittarina voidaan sanoa sellaista, jolla on korkea tarkkuus ja l\u00f6ytyvyysarvo. Sellainen arvo on nimelt\u00e4\u00e4n F1 score. F1 on harmoninen keskiarvo tarkkuudesta ja l\u00f6ytyvyysarvosta. Se lasketaan seuraavalla kaavalla:</p> \\[ \\begin{align*}     \\text{F1} &amp;= 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\\\               &amp;\\approx 0.97 \\end{align*} \\] <p>Voimme tarkistaa, ett\u00e4 laskutoimituksemme t\u00e4sm\u00e4\u00e4 Scikit-learningin <code>f1_score</code> -funktion palauttamaan arvoon.</p> IPython<pre><code>f1 = 2 * (precision * recall) / (precision + recall)\nassert metrics.f1_score(y_test, y_pred) == f1\n</code></pre> <p>Teht\u00e4v\u00e4</p> <p>Mik\u00e4 mahtaa olla F beta score eli \\(F_{\\beta}\\) ?Selvit\u00e4, mik\u00e4 on sen laskukaava ja miten se eroaa F1 scoresta.</p>"},{"location":"koneoppiminen/miksi/","title":"Miksi","text":""},{"location":"koneoppiminen/miksi/#miksi-koneoppimista-kaytetaan","title":"Miksi koneoppimista k\u00e4ytet\u00e4\u00e4n?","text":"<p>Koneoppiminen keskittyy algoritmien kehitt\u00e4miseen, jotka voivat oppia tietoa datasta. Koneoppimista k\u00e4ytet\u00e4\u00e4n monenlaisiin ongelmiin, kuten luokitteluun, regressioon, klusterointiin ja suositteluj\u00e4rjestelmiin. Syy koneoppimisen k\u00e4ytt\u00e4miseen on usein se, ett\u00e4 algoritmi auttaa automatisoimaan monimutkaisia teht\u00e4vi\u00e4. Alan asiantuntija voi tyypillisest\u00e4 tehd\u00e4 saman teht\u00e4v\u00e4n k\u00e4sin, mutta usein t\u00e4m\u00e4 on taloudellisesti kannattamatonta. On esimerkiksi t\u00e4ysin mahdollista, ett\u00e4 musiikkistriimauspalvelun (Spotify, Apple Music yms.) palkkaama asiantuntija analysoisi sinun kuunteluhistoriasi ja suosittelisi sinulle musiikkia, josta sin\u00e4 saattaisit pit\u00e4\u00e4. Kuinka paljon palvelun kuukausimaksu olisi, jos jokaiselle resursoitaisiin 2 tuntia asiantuntijaty\u00f6t\u00e4 per kuukausi?</p> <p>Question</p> <p>Kuinka varmistaisit, ett\u00e4 asiantuntija ei suosi omia mieltymyksi\u00e4\u00e4n? Koneoppimismallin objektiivisuus on numeraalisesti mitattavissa, mutta ei suinkaan vaivatonta. Kuinka tekisit t\u00e4m\u00e4n ihmisen kanssa?</p> <p>Tip</p> <p> Kannattaa tutustua my\u00f6s Oulun Yliopiston julkaisemaan  kirjaan Miten teko\u00e4ly vaikuttaa el\u00e4m\u00e4\u00e4mme 2050-luvulla?.</p> <p>Koneoppiminen voi my\u00f6s auttaa l\u00f6yt\u00e4m\u00e4\u00e4n uusia tietoa datasta, jota ihmiset eiv\u00e4t ole huomanneet. Koneoppimista k\u00e4ytet\u00e4\u00e4n monilla eri aloilla, kuten terveydenhuollossa, finanssialalla, markkinoinnissa ja teollisuudessa.</p> <p></p> <p>Kuvio 1: Koneoppimista ja tilastotiedett\u00e4 voi k\u00e4ytt\u00e4\u00e4 muun muassa luokitteluun. L\u00e4hde: xkcd 2893 (CC BY-NC)</p>"},{"location":"koneoppiminen/miksi/#esimerkki-kuvien-luokittelu","title":"Esimerkki: Kuvien luokittelu","text":"<p>Kuvio 2: Kuvien luokittelu k\u00e4sin on aikaa viev\u00e4\u00e4 ja tyls\u00e4\u00e4. T\u00e4ss\u00e4 DALL-E 3:n n\u00e4kemys.</p> <p>Kuvittele, ett\u00e4 olet ollut lomalla ja ottanut kuvia. K\u00e4vit lomalla kolmessa lokaatiossa: mets\u00e4ss\u00e4, vuorella ja museossa. Sinulla on yhteens\u00e4 10,000 valokuvaa. K\u00e4vit kussakin kohteessa useina eri p\u00e4ivin\u00e4, joten et voi tunnistaa kuvia helposti j\u00e4rjest\u00e4m\u00e4ll\u00e4 niit\u00e4 p\u00e4iv\u00e4m\u00e4\u00e4r\u00e4n tai juoksevan numeroinnin mukaan.</p> <p>Nykytilanne: kaikki 10,000 kuvaa ovat hakemistossa <code>vacation_pictures/</code>.</p> <p>Tavoitetilanne: kuvat on luokiteltuina kolmeen kansioon: <code>forest/</code>, <code>mountain/</code> ja <code>museum/</code>.</p>"},{"location":"koneoppiminen/miksi/#ratkaisu-1-kasin-luokittelu","title":"Ratkaisu 1: K\u00e4sin luokittelu","text":"<p>Yksi ratkaisu on luokitella kuvat k\u00e4sin. K\u00e4yt\u00e4t tunteja tai p\u00e4ivi\u00e4 katsomalla kuvia ja siirt\u00e4m\u00e4ll\u00e4 ne oikeisiin kansioihin. T\u00e4m\u00e4 on aikaa viev\u00e4\u00e4 ja tyls\u00e4\u00e4. Ent\u00e4 jos kuvia onkin 100,000? Tai 1,000,000?</p>"},{"location":"koneoppiminen/miksi/#ratkaisu-2-rautakoodattu-logiikka","title":"Ratkaisu 2: Rautakoodattu logiikka","text":"<p>Toinen ratkaisu on kirjoittaa ohjelma, joka luokittelee kuvat automaattisesti. Voit esimerkiksi kirjoittaa ohjelman, joka noudattaa kuvan v\u00e4riin sek\u00e4 kuvan metatietoihin perustuvaa logiikkaa. Jokainen if-else s\u00e4\u00e4nt\u00f6 kirjoitetaan k\u00e4sin. Alla esimerkki, jossa t\u00e4m\u00e4 logiikka on kuvattuna pseudokoodina:</p> <pre><code>procedure classify_pictures():\n    for each picture in vacation_pictures:\n        if picture.median_color == \"green\":\n            move picture to forest/\n        else if picture.median_color == \"brown\":\n            if picture.iso_sensitivity &gt; 800:\n                move picture to museum/\n            else:\n                move picture to mountain/\n        else if picture.median_color == \"grey\":\n            move picture to museum/\n</code></pre>"},{"location":"koneoppiminen/miksi/#ratkaisu-3-koneoppiminen","title":"Ratkaisu 3: Koneoppiminen","text":"<p>Kolmas ratkaisu on k\u00e4ytt\u00e4\u00e4 koneoppimista. Ty\u00f6h\u00f6n soveltuvia algoritmeja on useita. N\u00e4ist\u00e4 kaksi merkitt\u00e4v\u00e4sti poikkeavaa l\u00e4hestymistapaa ovat:</p> <ul> <li> Syv\u00e4oppiminen. Voisimme k\u00e4ytt\u00e4\u00e4 syv\u00e4oppimista ja siirto-oppimsita (engl. transfer learning). T\u00e4ll\u00f6in lataisimme valmiin mallin, joka on opetettu tunnistamaan kuvia. Alkuper\u00e4inen malli on koulutettu miljoonilla kuvilla, joten se on oppinut tehokkaasti erilaisia piirteit\u00e4 kuvista. Voimme k\u00e4ytt\u00e4\u00e4 t\u00e4t\u00e4 mallia pohjana ja opettaa sen tunnistamaan mets\u00e4-, vuori- ja museokuvia. Koulutusdata on kuvan raakapikselit tiettyyn resoluutioon skaalattuna. T\u00e4m\u00e4 malli ei k\u00e4y t\u00e4lle kurssille: neuroverkot esitell\u00e4\u00e4n syv\u00e4oppimiseen liittyvill\u00e4 kurssilla.</li> <li> Perinteiset koneoppimismallit. Voimme k\u00e4ytt\u00e4\u00e4 perinteisi\u00e4 koneoppimismalleja, kuten logistista regressiota, tukivektorikoneita tai p\u00e4\u00e4t\u00f6spuita. Koulutusdata ei ole raakaa pikselidataa, vaan kuvan pikselidatasta tulee luoda piirteit\u00e4 (features). T\u00e4h\u00e4n k\u00e4ytet\u00e4\u00e4n k\u00e4ytt\u00e4j\u00e4n valitsemia menetelmi\u00e4. Voimme luomme piirteit\u00e4 esimerkiksi kuvan pikseleiden v\u00e4reist\u00e4 histogrammin avulla tai kuvan reunapiirteist\u00e4 k\u00e4ytt\u00e4m\u00e4ll\u00e4 valittua visuaalisten piirteiden kuvailuun (engl. visual feature descriptor) sopivaa menetelm\u00e4\u00e4. Vaihtoehtoja olisivat esimerkiksi ORB ja BRIEF. N\u00e4ill\u00e4 ty\u00f6kaluilla luotu piirre sy\u00f6tett\u00e4isiin valitsemallemme koneoppimismallille, joka oppisi tunnistamaan kuvia.</li> </ul>"},{"location":"koneoppiminen/miksi/#vaihe-31-kuvan-piirteiden-luominen","title":"Vaihe 3.1: Kuvan piirteiden luominen","text":"<p>Kuvitellaan, ett\u00e4 meill\u00e4 on koodattuna piirrekuvaus, joka palauttaa aina tasan 10 piirrett\u00e4 kuvasta. Kuvitelluista piirteist\u00e4 kolme ensimm\u00e4ist\u00e4 ovat jatkuvia (engl. continuous features) ja loput seitsem\u00e4n ovat kategorisia (enlg. categorical features.) Funktiota kutsuttaisiin n\u00e4in:</p> IPython<pre><code>picture = load_picture(\"vacation_pictures/IMG_0001.jpg\")\nfeatures = describe_features(picture)\nprint(features)\n</code></pre> stdout (kommentoituna)<pre><code>     jatkuvat\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n[0.123, 0.555, 0.991, 1, 0, 0, 1, 0, 1, 1]\n                      \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        kategoriset\n</code></pre> <p>Note</p> <p>Koneoppimismallille ei siis sy\u00f6tet\u00e4 suoraan kuvan RGB-dataa vaan kuvasta lasketut piirteet. Syv\u00e4oppimismallit ovat poikkeus, sill\u00e4 ne voivat k\u00e4ytt\u00e4\u00e4 raakaa pikselidataa, ja ne oppivat luomaan piirteet itse. Ne eiv\u00e4t ole t\u00e4m\u00e4n kurssin aihealue.</p>"},{"location":"koneoppiminen/miksi/#vaihe-32-koneoppimismallin-kouluttaminen","title":"Vaihe 3.2: Koneoppimismallin kouluttaminen","text":"<p>Kuvio 2: Koneoppimismallin koulutus hoituisi sy\u00f6tt\u00e4m\u00e4ll\u00e4 mallille mets\u00e4-, vuori- ja museokuvien (vihre\u00e4t, ruskeat, siniset neli\u00f6iden) piirrevektoreita.</p> <p>Vaiheessa kaksi koulutamme koneoppimismallin. Koulutusdata koostuu kuvien piirteist\u00e4 ja niiden luokista. Voisimme esimerkiksi luokitella k\u00e4sin 200 mets\u00e4-, vuori- ja museokuvaa ja kouluttaa mallin n\u00e4iden avulla. N\u00e4in meid\u00e4n tarvisisi luokitella (engl. to label) vain 600 kuvaa k\u00e4sin, eik\u00e4 kaikkia 10,000 kuvaa. Malli oppii koulutusdatasta korrelaatioita piirteiden ja luokkien v\u00e4lill\u00e4. </p> <p>Tip</p> <p>Esimerkiksi <code>features[7] == 1</code> voi viitata vahvasti siihen, ett\u00e4 kuva on mets\u00e4kuva. Kenties kyseinen feature on binaarinen <code>is_bad_compress_ratio</code> ja se on saanut arvon 1, koska mets\u00e4kuvat pakkautuvat huonommin kuin vuori- ja museokuvat.</p>"},{"location":"koneoppiminen/miksi/#vaihe-33-koneoppimismallin-testaus","title":"Vaihe 3.3: Koneoppimismallin testaus","text":"<p>Voimme testata mallin antamalla sille esimerkiksi 100 mets\u00e4-, vuori- ja museokuvaa, joita se ei ole n\u00e4hnyt koulutusvaiheessa. Aivan kuten koulutusdata, my\u00f6s n\u00e4m\u00e4 on pit\u00e4nyt k\u00e4sin luokitella.</p> <p>Kuvitellaan, ett\u00e4 testaus on mennyt hyvin, ja malli on tunnistanut kuvat seuraavanlaisesti:</p> <ul> <li>Mets\u00e4kuvat:<ul> <li> 92/100 mets\u00e4</li> <li> 6/100 vuori</li> <li> 2/100 museo</li> </ul> </li> <li>Vuorikuvat:<ul> <li> 4/100 mets\u00e4</li> <li> 95/100 vuori</li> <li> 1/100 museo</li> </ul> </li> <li>Museokuvat:<ul> <li> 1/100 mets\u00e4</li> <li> 2/100 vuori</li> <li> 97/100 museo</li> </ul> </li> </ul>"},{"location":"koneoppiminen/miksi/#vaihe-34-koneoppimismallin-kaytto","title":"Vaihe 3.4: Koneoppimismallin k\u00e4ytt\u00f6","text":"<p>Kuvio 3: Koneoppimismallin k\u00e4ytt\u00f6. Kuvien piirteet sy\u00f6tet\u00e4\u00e4n mallille, joka antaa luokituksen.</p> <p>Lopulta mallia voi k\u00e4ytt\u00e4\u00e4 tunnistamaan kuvia, joita se ei ole n\u00e4hnyt. N\u00e4in voimme k\u00e4ytt\u00e4\u00e4 sit\u00e4 luokittelemaan ne 9,100 kuvaa, joita emme ole k\u00e4sin luokitelleet koulutus- tai testausvaiheessa. Alla viel\u00e4 selvyyden vuoksi koodiesimerkki, joka luokittelee kuvat:</p> IPython<pre><code>from models import VacationModel as model\n\n# Load\npicture = load_picture(\"vacation_pictures/IMG_09999.jpg\")\nfeatures = describe_features(picture)\n\n# Infer\npredicted_class = model.predict(features)\nprint(predicted_class)\n</code></pre> stdout<pre><code>\"forest\"\n</code></pre>"},{"location":"koneoppiminen/tyonkulku/","title":"Ty\u00f6nkulku","text":"<p>Ennen kuin t\u00e4t\u00e4 ohjetta seuraa pidemm\u00e4lle, pit\u00e4isi olla selvill\u00e4, miksi kyseist\u00e4 mallia ollaan ottamassa k\u00e4ytt\u00f6\u00f6n. Toisin sanoen esivaatimus koko touhulle on vaihe, jossa ongelma ja siihen toivottu ratkaisu m\u00e4\u00e4ritell\u00e4\u00e4n.</p> <p>Olettaen, ett\u00e4 ongelma on jo m\u00e4\u00e4ritelty, niin vaiheet voi jakaa muun muassa seuraavalla tavalla:</p> <ol> <li>Datan ker\u00e4\u00e4minen</li> <li>Datan esik\u00e4sittely</li> <li>Mallin valinta</li> <li>Mallin koulutus</li> <li>Mallin evaluointi</li> <li>Mallin tuunaus</li> <li>Mallin k\u00e4ytt\u00f6\u00f6notto</li> </ol> <p>Warning</p> <p>Huomaa, ett\u00e4 vaiheet ovat iteratiivisia ja niill\u00e4 on kesken\u00e4\u00e4n vahvoja riippuvuuksia. Jos vaiheista piirt\u00e4isi kaavion, niin kukin vaihe sis\u00e4lt\u00e4isi takaisinkytkent\u00f6j\u00e4 edellisiin vaiheisiin. Esimerkiksi mallin evaluoinnissa voi tulla ilmi, ett\u00e4 malli ei saavuta riitt\u00e4v\u00e4\u00e4 tarkkuutta ilman lis\u00e4datan ker\u00e4\u00e4mist\u00e4. K\u00e4yt\u00e4nn\u00f6ss\u00e4 t\u00e4ll\u00f6in palataan takaisin l\u00e4ht\u00f6ruutuun (joskin hieman viisaampana).</p>"},{"location":"koneoppiminen/tyonkulku/#vaiheet","title":"Vaiheet","text":""},{"location":"koneoppiminen/tyonkulku/#1-datan-keraaminen","title":"1. Datan ker\u00e4\u00e4minen","text":"<p>Koneoppimismallin rakentamiseen tarvitaan dataa. Datan pit\u00e4\u00e4 olla valitun ongelman kannalta merkitsev\u00e4\u00e4. Ongelma on, ett\u00e4 usein datan merkitsevyys ei ole itsest\u00e4\u00e4n selv\u00e4\u00e4 vaan paljastuu vasta my\u00f6hemmiss\u00e4 vaiheissa.</p> <p>Data voi olla esimerkiksi </p> <ul> <li>teksti\u00e4<ul> <li>kategorista teksti\u00e4</li> <li>vapaamuotoista teksti\u00e4</li> </ul> </li> <li>numeerisia arvoja<ul> <li>kategorista dataa</li> <li>lukum\u00e4\u00e4r\u00e4ist\u00e4 dataa</li> <li>jne.</li> </ul> </li> <li>kuva- tai \u00e4\u00e4nidataa</li> <li>p\u00e4iv\u00e4m\u00e4\u00e4ri\u00e4</li> </ul> <p>Dataa voidaan ker\u00e4t\u00e4 julkisista l\u00e4hteist\u00e4, oman palvelun k\u00e4ytt\u00e4jilt\u00e4, kyselyill\u00e4, sensoreilta, jne. Datan ker\u00e4\u00e4misess\u00e4 tehdyt virheet kostautuvat kaikissa seuraavissa vaiheissa. Yrityksen data voi hyvin sijaita siiloutuneena eri j\u00e4rjestelmiss\u00e4, jolloin datan yhdist\u00e4minen ja puhdistaminen voi olla yll\u00e4tt\u00e4v\u00e4nkin suurit\u00f6ist\u00e4. Sivuoireena t\u00e4st\u00e4 saattaa synty\u00e4 data-alusta, joka tuottaa yritykselle arvoa my\u00f6s muilla tavoin kuin koneoppimismallien kautta.</p> <p>Teht\u00e4v\u00e4</p> <p>Mieti, onko kaikki data 2000-luvun \u00f6ljy\u00e4? Vai onko umpim\u00e4hk\u00e4\u00e4n ker\u00e4tty data pikemminkin kuin maa-ainesta, josta ei kaikesta jalostuksesta huolimatta irtoa juuri mit\u00e4\u00e4n hy\u00f6dyllist\u00e4? Etsi v\u00e4itteit\u00e4 puolesta ja vastaan.</p> <p>T\u00e4h\u00e4n vaiheeseen saattaa kuulua my\u00f6s datan annotointi tai merkitseminen. Annotointi on ty\u00f6l\u00e4s vaihe, jossa ihmiset merkkaavat, ett\u00e4 onko kuvassa esimerkiksi kissa vai koira, tai onko s\u00e4hk\u00f6posti roskapostia vai ei. Annotointi on usein pullonkaula koneoppimismallien rakentamisessa, sill\u00e4 se vaatii ihmisty\u00f6t\u00e4 ja on usein virhealtista. </p> <p>Tip</p> <p>Ideaalitilanteessa annotointi saadaan joukkoistettua, jolloin useat ihmiset merkkaavat samaa dataa ja virheet voidaan havaita ja korjata. Jos olet joskus vastannut CAPTCHA-kyselyihin, olet todenn\u00e4k\u00f6isesti ollut osa joukkoistettua annotointia. My\u00f6s \"Mink\u00e4 ik\u00e4iselt\u00e4 n\u00e4yt\u00e4t?\" -tyyppiset kyselyt ovat joukkoistettua annotointia. Kun merkkaat Gmailissa s\u00e4hk\u00f6postia roskapostiksi, olet osa joukkoistettua annotointia.</p>"},{"location":"koneoppiminen/tyonkulku/#2-datan-esikasittely","title":"2. Datan esik\u00e4sittely","text":"<p>Datan esik\u00e4sittelyss\u00e4 k\u00e4sitell\u00e4\u00e4n dataa ennen koneoppimismallin rakentamista. T\u00e4h\u00e4n voi kuulua esimerkiksi puuttuvien arvojen t\u00e4ytt\u00e4minen, datan normalisointi, piirteiden yhdist\u00e4minen (esim. <code>et\u00e4isyys / aika = nopeus</code>), kategorisen datan enkoodaus useiksi bin\u00e4\u00e4ripiirteiksi ja niin edelleen. Kenties datasetti sis\u00e4lt\u00e4\u00e4 p\u00e4iv\u00e4m\u00e4\u00e4ri\u00e4, jotka ovat tulevaisuudessa. Se, kuinka n\u00e4ihin virheisiin reagoidaan, vaatii ymm\u00e4rryst\u00e4 siit\u00e4, kuinka data on ker\u00e4tty, ja siit\u00e4, mit\u00e4 data edustaa (eli substanssiosaamista.)</p> <p>Datan k\u00e4sittely ei ole mekaaninen vaihe vaan vaatii ymm\u00e4rryst\u00e4 datan luonteesta. T\u00e4h\u00e4n vaiheeseen sis\u00e4ltyy my\u00f6s eksploratiivinen analyysi, jossa pyrit\u00e4\u00e4n ymm\u00e4rt\u00e4m\u00e4\u00e4n datan piirteit\u00e4 ja mahdollisia ongelmia. Esimerkiksi jatkuvien lukujen jakautumista voidaan tarkastella histogrammien avulla ja eri piirteiden v\u00e4lisi\u00e4 korrelaatioita voidaan tarkastella korrelaatiomatriisin avulla. T\u00e4t\u00e4 piirteiden yhdist\u00e4mis- ja luomisprosessi tunnetaan englanninkielisell\u00e4 termill\u00e4 feature engineering.</p> <p>Datan k\u00e4sittelyn aikana tulisi muun muassa selvitt\u00e4\u00e4 ja dokumentoida, kuinka tasapainoista tai vinksahtanutta data on. Jos data on ep\u00e4tasapainoista, esimerkiksi 99 % negatiivisia ja 1 % positiivisia tapahtumia, mallin rakentaminen voi olla haastavampaa. T\u00e4ll\u00f6in malli voi oppia ennustamaan kaikki tapahtumat negatiivisiksi ja silti saavuttaa korkean tarkkuuden.</p> <p>Alla koneoppimismalli, joka ennustaa reilusti yli 99 % tarkkuudella, ett\u00e4 voittaako kyseinen k\u00e4ytt\u00e4j\u00e4 huomenna Lotto-arvonnassa p\u00e4\u00e4voiton.</p> IPython<pre><code>def is_lotto_winner_tomorrow(user_id):\n    return False\n</code></pre>"},{"location":"koneoppiminen/tyonkulku/#3-mallin-valinta","title":"3. Mallin valinta","text":"<p>Koneoppimismallin valinta riippuu siit\u00e4, millaista dataa on saatavilla ja mit\u00e4 halutaan ennustaa. Katso aiempi luku Tyypit kertauksena eri koneoppimismallityypeist\u00e4. On kuitenkin t\u00e4rke\u00e4\u00e4 korostaa, ett\u00e4 data ja sen laatu vaikuttavat lopputulokseen enemm\u00e4n kuin k\u00e4ytetty malli.</p> <p>It [his observation] implies that model behavior is not determined by architecture, hyperparameters, or optimizer choices.It\u2019s determined by your dataset, nothing else. Everything else is a means to an end in efficiently delivery compute to approximating that dataset.</p> <p>Then, when you refer to \u201cLambda\u201d, \u201cChatGPT\u201d, \u201cBard\u201d, or \u201cClaude\u201d then, it\u2019s not the model weights that you are referring to. It\u2019s the dataset. </p> <p>L\u00e4hde: James Betker (Open AI), kirjoitus: The \u201cit\u201d in AI models is the dataset.</p> <p>Scikit-learn tarjoaa vakaan API:n, joka mahdollistaa useiden eri mallien kouluttamisen ja k\u00e4yt\u00f6n samoilla <code>.fit()</code> ja <code>.transform()</code> metodeilla. Mallien vertailun on siis kehitt\u00e4j\u00e4lle vaivatonta. T\u00e4st\u00e4 syyst\u00e4 vaiheet 3-5 ovat usein iteratiivisia ja niiden v\u00e4lill\u00e4 on paljon takaisinkytkent\u00e4\u00e4. Mik\u00e4li k\u00e4ytetty data ei ole aivan big data -kokoluokassa, mallien kouluttaminen on usein kohtalaisen nopeaa, joten on t\u00e4ysin mahdollista vertailla useita eri malleja ja niiden hyperparametreja, vaikka n\u00e4ist\u00e4 tulisikin melko suuri m\u00e4\u00e4r\u00e4 testattavia kombinaatioita.</p>"},{"location":"koneoppiminen/tyonkulku/#4-mallin-koulutus","title":"4. Mallin koulutus","text":"<p>Koneoppimismalli koulutetaan koulutusdatalla; osa datasta laitetaan sivuun, ja sit\u00e4 k\u00e4ytet\u00e4\u00e4n my\u00f6hemmin mallin tai sen parametrien arvioimiseen. Mik\u00e4li k\u00e4yt\u00e4t jotakin valmista mallia, t\u00e4m\u00e4 on yksinkertaisin ja mekaanisin vaihe koko prosessissa. Mallin kouluttaminen on kuitenkin tietokoneen n\u00e4k\u00f6kulmasta raskas operaatio, joka vaatii paljon laskentatehoa. Kouluttaminen vaatii huomattavasti enemm\u00e4n resursseja kuin mallin k\u00e4ytt\u00e4minen ennustamiseen. Varsinkin neuroverkkojen kouluttaminen voi vied\u00e4 p\u00e4ivi\u00e4 tai jopa viikkoja, mik\u00e4li k\u00e4yt\u00f6ss\u00e4 ei ole riitt\u00e4v\u00e4sti laskentatehoa. T\u00e4ll\u00e4 kurssilla k\u00e4sitellyt mallit ovat kuitenkin niin yksinkertaisia, ja dataa on niin pieni\u00e4 m\u00e4\u00e4ri\u00e4, ett\u00e4 niiden kouluttaminen onnistuu yleens\u00e4 muutamassa sekunnissa tai minuutissa.</p>"},{"location":"koneoppiminen/tyonkulku/#5-mallin-evaluointi","title":"5. Mallin evaluointi","text":"<p>Koulutetun koneoppimismallin suorituskyky arvioidaan testidatalla. T\u00e4m\u00e4 auttaa arvioimaan, kuinka hyvin malli yleistyy uuteen dataan ja kuinka hyvin se ennustaa tulevia tapahtumia. Mallin suorituskyvyn arviointi riippuu siit\u00e4, onko kyseess\u00e4 luokittelu-, regressio- vai jokin muu ongelma. Luokittelumalleja voidaan arvioida esimerkiksi h\u00e4mmennysmatriisilla, tarkkuusarvolla (%) sek\u00e4 F1-pisteytyksell\u00e4. Regressiomalleja voidaan arvioida esimerkiksi keskivirheell\u00e4 (MAE/MSE) ja selitysasteella (R2-arvo).</p>"},{"location":"koneoppiminen/tyonkulku/#6-mallin-tuunaus","title":"6. Mallin tuunaus","text":"<p>Koneoppimismallin tuunausta (engl. hyperparameter tuning) k\u00e4ytet\u00e4\u00e4n mallin suorituskyvyn parantamiseen.</p> <p>Aivan kuten mallin koulutuksessa, my\u00f6s hienos\u00e4\u00e4d\u00f6ss\u00e4 k\u00e4ytet\u00e4\u00e4n koulutusdataa, jotta voidaan varmistaa, ettei mallin hyperparametreja optimoida testidatan suhteen. Tavoite on, ett\u00e4 hyperparametrien arvot ovat sellaiset, ett\u00e4 malli yleistyy parhaiten uuteen dataan - ei se, ett\u00e4 se toimii parhaiten koulutusdatalla.</p> <p>Parametrien tuunaus voi tapahtua esimerkiksi ristivalidoinnin (engl. cross-validation) avulla. Ristivalidoinnissa data jaetaan useaan osaan, joista osa k\u00e4ytet\u00e4\u00e4n koulutukseen ja osa testaukseen. T\u00e4t\u00e4 toistetaan useita kertoja, jolloin saadaan luotettavampi arvio mallin suorituskyvyst\u00e4.</p> <p>Pseudoesimerkki alla. Esimerkki\u00e4 lukiessa sinun ei tarvitse tiet\u00e4\u00e4, mit\u00e4 Lasso-malli tekee. Riitt\u00e4\u00e4, ett\u00e4 hyv\u00e4ksyt, ett\u00e4  <code>alpha</code>-arvon on hyperparametri, ja sen arvon valinta vaikuttaa merkitt\u00e4v\u00e4sti siihen, kuinka hyvin malli ennustaa. Hyperparametrien tuunaus on n\u00e4iden arvojen haarukoimista. Jos haarukoitavia arvoja on monta (kuvitteellisina esimerkkein\u00e4 alpha, beta, gamma, ...), niin kaikki n\u00e4iden kombinaatiot pit\u00e4\u00e4 testata.</p> IPython<pre><code>alpha_grid = [0.1, 0.5, 1.0, 1.5, 2.0]\n\nfor alpha in alpha_grid:\n    clf = linear_model.Lasso(alpha=alpha)\n    scores = cross_val_score(clf, X_train, y_train, cv=5)\n    print(f\"Alpha: {alpha}, Scores: {scores}\")\n</code></pre>"},{"location":"koneoppiminen/tyonkulku/#7-mallin-kayttoonotto","title":"7. Mallin k\u00e4ytt\u00f6\u00f6notto","text":"<p>K\u00e4ytt\u00f6\u00f6notto (engl. deployment) on vaihe, jossa hy\u00f6dylliseksi evaluoitu ja hienos\u00e4\u00e4detty malli otetaan k\u00e4ytt\u00f6\u00f6n. Kevyt malli voidaan esimerkiksi julkaista yksinkertaisen REST-rajapinnan taakse (esim. Python FastAPI), jolloin se on helposti k\u00e4ytett\u00e4viss\u00e4. Raskaampi malli voidaan julkaista hajautettuun j\u00e4rjestelm\u00e4\u00e4n, jossa kuormantasaajan takana olevat yksik\u00f6t skaalautuvat automaattisesti k\u00e4ytt\u00e4j\u00e4m\u00e4\u00e4r\u00e4n mukaan (esim. Kubernetes).</p> <p>K\u00e4yt\u00e4nn\u00f6n tasolla projekti ei kuitenkaan ole v\u00e4ltt\u00e4m\u00e4tt\u00e4 viel\u00e4 ohi. Mallista voi menn\u00e4 niin sanotusti parasta ennen p\u00e4iv\u00e4 umpeen (engl. model decay, model rot, model drift). T\u00e4lt\u00e4 voi v\u00e4ltty\u00e4 ker\u00e4\u00e4m\u00e4ll\u00e4 jatkuvasti dataa ja p\u00e4ivitt\u00e4m\u00e4ll\u00e4 mallia s\u00e4\u00e4nn\u00f6llisesti. Toisin sanoen yll\u00e4 olevista vaiheista tulee loop. Ideaalitilanteessa loppuk\u00e4ytt\u00e4j\u00e4t osallistuvat virheellisten ennusteiden merkint\u00e4\u00e4n, jolloin he osallistuvat mallin jatkokehitykseen (ks. vaihe 1).</p>"},{"location":"koneoppiminen/tyypit/","title":"Tyypit","text":"<p>Koneoppimisen suhteen on t\u00e4rke\u00e4 tunnistaa ja osata selitt\u00e4\u00e4 kaksi termi\u00e4:</p> <ul> <li>Algoritmi (engl. algorithm) on matemaattinen kaava tai tilastollinen kaava.</li> <li>Malli (engl. model) on algoritmin koulutuksen tuloksena syntynyt koneoppimismalli, joka osaa tehd\u00e4 luokituksia aikaisemmin n\u00e4kem\u00e4tt\u00f6mille sy\u00f6tteille.</li> </ul> <p>Kun sinulla on jokin koneoppimisen ongelma, sinun tulee valita ongelmaan sopiva malli. Mallin valitsemisen j\u00e4lkeen koulutat algoritmin koulutusdatalla. Koulutuksen j\u00e4lkeen voit testata mallin suorituskyky\u00e4 testidatalla. Testidata on tismalleen samanlaista dataa kuin koulutusdata, mutta sit\u00e4 ei ole paljastettu algoritmille koulutusvaiheessa. Lopuksi voit k\u00e4ytt\u00e4\u00e4 mallia tunnistamaan uusia sy\u00f6tteit\u00e4.</p>"},{"location":"koneoppiminen/tyypit/#kolme-koneoppimisen-paatyyppia","title":"Kolme koneoppimisen p\u00e4\u00e4tyyppi\u00e4","text":"<p>Koneoppiminen voidaan jakaa kolmeen p\u00e4\u00e4tyyppiin: </p> <ul> <li>ohjattu (engl. supervised learning)</li> <li>ohjaamaton (engl. unsupervised learning)</li> <li>vahvistusoppiminen (engl. reinforcement learning)</li> </ul>"},{"location":"koneoppiminen/tyypit/#ohjattu-oppiminen","title":"Ohjattu oppiminen","text":"<p>Kuvio 1. Talon hinnan arviointi DALL-E 3:n maalaamana. Huomaa talosta lent\u00e4v\u00e4t numerot, jotka edustavat joitakin talon piirteit\u00e4, kuten sijaintia, huoneiden m\u00e4\u00e4r\u00e4\u00e4 ja pinta-alaa.</p> <p>Ohjattu oppiminen on koneoppimisen tyypeist\u00e4 ilmeisin . Se on yleisesti k\u00e4yt\u00f6ss\u00e4 ja se on helppo ymm\u00e4rt\u00e4\u00e4: opetusdata sis\u00e4lt\u00e4\u00e4 oikean vastauksen. Koulutusvaiheessa algoritmi pyrkii l\u00f6yt\u00e4m\u00e4\u00e4n korrelaatioita piirteiden ja oikean vastauksen v\u00e4lill\u00e4. Lineaarinen regressio (ks. Kuvio 2) eli suoran sovitus on yksi yksinkertaisimmista ohjatun oppimisen algoritmeista. Huomaa, ett\u00e4 kone ei t\u00e4ss\u00e4 tapauksessa opi mit\u00e4\u00e4n ilmi\u00f6iden kausaalisuhteista, vaan ainoastaan korrelaatioista. Kone ei tied\u00e4 mit\u00e4\u00e4n logiikasta: se vain palauttaa x:n perusteella y:n - eik\u00e4 edes oikeaa y:n arvoa, vaan jonkin sortin mediaanin.</p> <p>K\u00e4yt\u00e4nn\u00f6n esimerkkej\u00e4:</p> <ul> <li>Kuvien luokittelu (forest, mountain, museum)</li> <li>S\u00e4hk\u00f6postien luokittelu (spam, ei-spam)</li> <li>Asunnon hinnan ennustaminen</li> <li>K\u00e4sin kirjoitetun numeron tunnistaminen</li> </ul> <p></p> <p>Kuvio 2. Regressio- ja luokitteluongelmien ero. Huomaa, ett\u00e4 kumpikin n\u00e4ist\u00e4 ongelmista on ohjattua oppimista. Regressio-ongelmassa y-akseli on se, mit\u00e4 ennustetaan. Luokitteluongelmassa el\u00e4inlaji on se, mit\u00e4 ennustetaan. Kumpikin n\u00e4ist\u00e4 on tiedossa.</p>"},{"location":"koneoppiminen/tyypit/#ohjaamaton-oppiminen","title":"Ohjaamaton oppiminen","text":"<p>Kuvio 2. DVD-elokuvien luokittelu DALL-E 3:n maalaamana. Huomaa, ett\u00e4 algoritmi tuskin p\u00e4\u00e4tyisi ihmisen tuntemiin genreihin kuten draama, komedia ja animaatio. Luokitteluun vaikuttaa merkitt\u00e4v\u00e4sti se, mit\u00e4 piirteit\u00e4 algoritmille sy\u00f6tet\u00e4\u00e4n. Jos piirrevektori sis\u00e4lt\u00e4\u00e4 pelk\u00e4n elokuvan keston ja leikkauksien m\u00e4\u00e4r\u00e4n, saat luokkia kuten \"Pitk\u00e4t elokuvat, joissa on v\u00e4h\u00e4n leikkauksia\"</p> <p>Ohjaamaton oppiminen on ohjatun oppimisen vastakohta. Ohjaamattomassa oppimisessa oikea vastaus puuttuu kokonaan koulutusdatasta. Valittu algoritmi pyrkii tunnistamaan jonkin sortin rakenteita datasta. K\u00e4ytt\u00f6tarpeita ovat esimerkiksi:</p> <ul> <li>klusterointi (engl. clustering)</li> <li>poikkeuksien tunnistaminen (engl. anomaly detection)</li> <li>ulottuvuuksien v\u00e4hent\u00e4minen (engl. dimensionality reduction)</li> </ul> <p>K\u00e4yt\u00e4nn\u00f6n tasolla teht\u00e4v\u00e4t oivat olla:</p> <ul> <li>Asiakassegmentointi (klusterointi)</li> <li>Vikojen tunnistaminen teollisuudessa (poikkeuksien tunnistaminen)</li> <li>Piirteiden yhdist\u00e4minen datasetist\u00e4 (ulottuvuuksien v\u00e4hent\u00e4minen)</li> </ul> <p>Klassinen esimerkki asiakassegmentoinnista ovat t-paidat. Kuvittele, ett\u00e4 sinulla on 1000 ihmist\u00e4, joilla haluat valmistaa t-paitoja. Sinulla on kunkin ihmisen mitat, kuten rinnanymp\u00e4rys, tiedossa. Kenties haluat kaavoittaa kolmenlaisia paitoja: S, M ja L. Ohjaamattomassa oppimisessa malli luokittelee ihmiset kolmeen ryhm\u00e4\u00e4n niin, ett\u00e4 keskim\u00e4\u00e4r\u00e4isin samankokoiset ihmiset ovat samassa ryhm\u00e4ss\u00e4. Mik\u00e4li valmistat t\u00e4m\u00e4n keskiarvon mukaan t-paitoja, ne todenn\u00e4k\u00f6isesti sopivat hyvin suurimmalle osalle ihmisist\u00e4. </p> <p>Warning</p> <p>Huomaa, ett\u00e4 kussakin luokassa on kuitenkin yksil\u00f6it\u00e4, joille vaatturi olisi mitannut eri kokoisen paidan. Virhe on aina olemassa. Mik\u00e4 m\u00e4\u00e4r\u00e4 virhett\u00e4 on ok? Jos suunnittelet L-koon paidan kyseisen kategorian mukaan, onko ok, ett\u00e4 10 ihmiselle paita ei mahdu ylle rikkomatta saumoja?</p>"},{"location":"koneoppiminen/tyypit/#vahvistusoppiminen","title":"Vahvistusoppiminen","text":"<p>Video 1. Vahvistusoppiminen Trackmania-pelin avulla. (L\u00e4hde: Yosh, Youtube)</p> <p>Vahvistusoppimisessa malli oppii toimimaan ymp\u00e4rist\u00f6ss\u00e4, jossa se saa palkkion tai rangaistuksen toiminnastaan. Yll\u00e4 on YouTube-video, jossa Yosh esittelee vahvistusoppimista Trackmania-pelin avulla. Porkkanana on radalla eteneminen ja rangaistuksena on radalta putoaminen. Jos video ei toimi, etsi vastaava video esimerkiksi hakusanoilla \"Reinforcement learning racing game\".</p>"},{"location":"koneoppiminen/tyypit/#algoritmin-valinta","title":"Algoritmin valinta","text":"<p>Huomaa, ett\u00e4 kukin yll\u00e4 mainituista p\u00e4\u00e4tyypeist\u00e4 rajaa algoritmin valintaa. Tiettyj\u00e4 termej\u00e4 voi esiinty\u00e4 sek\u00e4 luokitteluun ett\u00e4 regressioon liittyviss\u00e4 ongelmissa, mik\u00e4 voi alkuun h\u00e4mment\u00e4\u00e4. Esimerkiksi <code>Support Vector &lt;something&gt;</code> voi esiinty\u00e4 sek\u00e4 luokittelu- ett\u00e4 regressioalgoritmeissa, kuten my\u00f6s <code>K-&lt;something&gt;</code>.</p> <p>Teht\u00e4v\u00e4</p> <p>Selvit\u00e4, mit\u00e4 eroa on <code>Support Vector Classifier</code> ja <code>Support Vector Regressor</code> -algoritmeilla. Mik\u00e4 on niiden yhteinen piirre? Mik\u00e4 on <code>support vector</code>?</p> <p>Selvit\u00e4, mit\u00e4 eroa on <code>K-Nearest Neighbors</code> ja <code>K-Means</code> -algoritmeilla. Mik\u00e4 on niiden yhteinen piirre? Mik\u00e4 on <code>k</code>?</p> <p>Alla on miellekartta, joka auttaa valitsemaan oikean algoritmin ongelmaasi. Huomaa, ett\u00e4 miellekartan kolme haaraa riippuvat siit\u00e4, onko ongelmasi luokittelu, regressio vai klusterointi.</p> <pre><code>mindmap\n  root((Start Here))\n    Labeled categorical data\n        Text data\n            Naive Bayes\n        Numeric data\n            K-nearest neighbors\n            Logistic regression\n            Support vector machines\n            Decision trees\n    Unlabeled categorical data\n        K-means clustering\n    Predicting quantities\n        Normal equation\n        Support vector regression\n        Gradient descent</code></pre> <p>Teht\u00e4v\u00e4</p> <p>Yll\u00e4 oleva miellekartta pohjautuu vahvasti Scikit Learnin Choosing the right estimator -sivun kaavioon. Tutustu my\u00f6s t\u00e4h\u00e4n alkuper\u00e4iseen, hieman laajempaan kaavioon.</p>"},{"location":"koneoppiminen/tyypit/#tehtavat","title":"Teht\u00e4v\u00e4t","text":"<p>Alla on kuusi ongelmatilannetta. Pohdi voiko ongelman ratkaista koneoppimisella? Jos, niin mink\u00e4 tyyppisell\u00e4 koneoppimisella? Ohjattu, ohjaamaton vai vahvistusoppiminen? N\u00e4m\u00e4 pohdinnat ovat hyv\u00e4\u00e4 sis\u00e4lt\u00f6\u00e4 oppimisp\u00e4iv\u00e4kirjaan.</p>"},{"location":"koneoppiminen/tyypit/#ongelma-1-uutisryhmaviestit","title":"Ongelma 1: Uutisryhm\u00e4viestit","text":"<p>Datasetti sis\u00e4lt\u00e4\u00e4 uutisryhm\u00e4viestej\u00e4 ja niiden luokituksia. Luokituksia ovat aiheet (kuten urheilu, pelit, taide, ...). Sinun tulee luoda sovellus, joka ennustaa uutisryhm\u00e4viestin aiheen, jos viestill\u00e4 ei viel\u00e4 ole aiheen luokitusta.</p>"},{"location":"koneoppiminen/tyypit/#ongelma-2-ruusutarha","title":"Ongelma 2: Ruusutarha","text":"<p>Datasetti sis\u00e4lt\u00e4\u00e4 kasvihuoneen ruusujen parametreja <code>[l\u00e4mp\u00f6tila, kosteus, valon m\u00e4\u00e4r\u00e4... kasvu p\u00e4iv\u00e4ss\u00e4]</code>. Sinun tulee luoda sovellus, joka ennustaa kuinka paljon annettu ruusu kasvaa p\u00e4iv\u00e4ss\u00e4.</p>"},{"location":"koneoppiminen/tyypit/#ongelma-3-kissanruoka","title":"Ongelma 3: Kissanruoka","text":"<p>Datasetti sis\u00e4lt\u00e4\u00e4 kuvia kissoista. Kuvat ovat 50x50 pikselin RGB-kuvia ilman muuta dataa. Sinun tulee luoda sovellus, joka ennustaa, pit\u00e4\u00e4k\u00f6 kissa tietyst\u00e4 kissanruokamerkist\u00e4.</p>"},{"location":"koneoppiminen/tyypit/#ongelma-4-palvelinkeskus","title":"Ongelma 4: Palvelinkeskus","text":"<p>Datasetti sis\u00e4lt\u00e4\u00e4 palvelimen metriikoita [CPU-l\u00e4mp\u00f6tila, HDD-l\u00e4mp\u00f6tila, HDD-kuorma, ...]. Sinun tulee luoda sovellus, joka varoittaa, jos palvelin k\u00e4ytt\u00e4ytyy ep\u00e4tavallisesti.</p> <p>Note</p> <p>Kenties n\u00e4m\u00e4 oudosti k\u00e4ytt\u00e4ytyv\u00e4t palvelimet ovat joko hy\u00f6kk\u00e4yksen kohteena tai ovat hajoamassa!</p>"},{"location":"koneoppiminen/tyypit/#ongelma-5-pac-man","title":"Ongelma 5: Pac-Man","text":"<p>Sinulla ei ole datasetti\u00e4. Haluat luoda sovelluksen, joka osaa pelata Pac-Man -peli\u00e4.</p>"},{"location":"koneoppiminen/tyypit/#ongelma-6-ostoshistoria","title":"Ongelma 6: Ostoshistoria","text":"<p>Datasetti sis\u00e4lt\u00e4\u00e4 k\u00e4ytt\u00e4jien selaus- sek\u00e4 ostoshistoriaa yrityksenne verkkokaupassa. Markkinointiosasto on aiempina vuosina luonut vain yhden Black Friday -kampanjamainoksen, joka on n\u00e4ytetty kaikille kaikille k\u00e4ytt\u00e4jille. Markkinointi haluaisi kohdentaa mainoksen paremmin, mutta heill\u00e4 on resursseja luoda vain noin 10 erilaista mainosta. He haluavat tiet\u00e4\u00e4, millaisia tuotteita katselevia ja ostavia asiakkailla heill\u00e4 on.</p>"},{"location":"koneoppiminen/yleista/","title":"Yleist\u00e4","text":"<p>Keskitymme t\u00e4ss\u00e4 ensimm\u00e4isess\u00e4 luvussa alan t\u00e4rkeimpiin termeihin ja m\u00e4\u00e4ritelmiin.</p>"},{"location":"koneoppiminen/yleista/#datatieteet","title":"Datatieteet","text":"<p>T\u00e4m\u00e4 on kurssi koneoppimisesta ja teko\u00e4lyst\u00e4. Kumpaahkin n\u00e4ihin liittyy k\u00e4site sek\u00e4 tieteenala datatieteet (engl. data sciences).</p> <p>Quote</p> <p>\"Data science is the field of study that combines domain expertise, programming skills, and knowledge of mathematics and statistics to extract meaningful insights from data. Data science practitioners apply machine learning algorithms to numbers, text, images, video, audio, and more to produce artificial intelligence (AI) systems to perform tasks that ordinarily require human intelligence. In turn, these systems generate insights which analysts and business users can translate into tangible business value.\" - DataRobot</p> <p></p> <p>Kuvio 1: Venn-diagrammi datatieteist\u00e4. L\u00e4hde: Dren Conway, The Data Science Venn Diagram (CC-BY)</p> <p>Datatieteet ovat tieteenala, joka laittaa koneoppimisen k\u00e4yt\u00e4nt\u00f6\u00f6n. Konteksti, jossa koneoppiminen laitetaan k\u00e4yt\u00e4nt\u00f6\u00f6n liike-el\u00e4m\u00e4ss\u00e4 on jokin substanssiosaamista vaativa toimiala. Koneoppiminen tai teko\u00e4ly auttaa ratkaisemaan ongelmia. Henkil\u00f6t, jotka ty\u00f6skentelev\u00e4t datatieteiden parissa ovat titteleilt\u00e4\u00e4n esimerkiksi data scientist, data analyst tai data engineer. Ty\u00f6nkuvissa voi olla p\u00e4\u00e4llek\u00e4isyytt\u00e4, mutta niiss\u00e4 on my\u00f6s nyanssieroja.</p> <p>Question</p> <p>Pohdi ja selvit\u00e4, mit\u00e4 eroa on data scientistin, data analystin ja data engineerin ty\u00f6nkuvilla. Kenen ty\u00f6kalupakkiin kuuluisi todenn\u00e4k\u00f6isimmin ohjelmisto Tableau ja mit\u00e4 se tekee? Kuka kirjoittaisi koodia, joka hy\u00f6dynt\u00e4\u00e4 jotakin nimelt\u00e4\u00e4n Apache Spark? Ent\u00e4 p\u00e4rj\u00e4\u00e4k\u00f6 yhden roolin osaaja ilman kahta muuta?</p>"},{"location":"koneoppiminen/yleista/#tekoaly","title":"Teko\u00e4ly","text":"<p>Teko\u00e4ly on vahvasti elokuvateollisuuden ja muun fiktion v\u00e4ritt\u00e4m\u00e4 k\u00e4site. Osa fiktion tarjoamasta tiedosta on t\u00e4ytt\u00e4 humpuukia, ja todellisuudessa teko\u00e4lyn hupun alta paljastuu pikemminkin tilastotiedett\u00e4 ja matematiikkaa. T\u00e4m\u00e4 ei kuitenkaan v\u00e4henn\u00e4 teko\u00e4lyn arvoa liiketoiminnan kannalta t\u00e4rkeiden ongelmien ratkaisijana. Teko\u00e4ly itsess\u00e4\u00e4n on kattotermi koneoppimiselle, ja se sis\u00e4lt\u00e4\u00e4 kaiken sellaisen teknologian, joka matkii ihmisenkaltaista \u00e4lykkyytt\u00e4 tavalla tai toisella. N\u00e4ihin tapoihin kuuluvat my\u00f6s s\u00e4\u00e4nt\u00f6pohjaiset j\u00e4rjestelm\u00e4t (if-elif-elif-else).</p> <p>Teko\u00e4ly ei ole uusi keksint\u00f6. Ihmismielen p\u00e4\u00e4ttelyn ymm\u00e4rt\u00e4mist\u00e4 tai sit\u00e4 vastaavan mekaanisen laitteen rakentamista on yritetty satoja ellei jopa tuhansia vuosia. Ensimm\u00e4inen neuroverkkotietokone, SNARC, rakennettiin vuonna 1950 Minskyn ja Edmondsin toimesta. Se koostui 40:st\u00e4 keinotekoisesta neuronista, joiden rakennetta inspiroivat ihmisaivojen neuronit. Vertailun vuoksi hedelm\u00e4k\u00e4rp\u00e4sell\u00e4 on noin 100 000 neuronia. 1950-luvulla ja 1960-luvulla tietokoneet olivat jo ratkaisseet tarinallisia ongelmia ja pelanneet shakkia. N\u00e4m\u00e4 vuodet olivat t\u00e4ynn\u00e4 toivoa ja suuria unelmia. Teko\u00e4lyn tutkijat tekiv\u00e4t ennusteita ja lupauksia tulevista menestystarinoistaan: tietokone voittaisi ihmisen shakissa 10 vuoden kuluessa. 1960-luvun puoliv\u00e4list\u00e4 2000-luvun alkuun oli ajanjakso, joka sis\u00e4lsi monia nousuja ja laskuja tai \"hype-syklej\u00e4\". T\u00e4m\u00e4n aikakauden keskell\u00e4, 1980-luvulla, oli lyhyt hetki erityisen suurta innostusta ja suuria toiveita. Miljardeja dollareita k\u00e4ytettiin erilaisiin teko\u00e4lyn sovelluksiin, mik\u00e4 seurasi pettymyst\u00e4. T\u00e4m\u00e4 k\u00e4ynnisti uuden laskusuhdanteen tai \"teko\u00e4lyn talven\" (engl. AI winter). T\u00e4ll\u00e4 hetkell\u00e4 koemme uutta teko\u00e4lyn saapumista, \"teko\u00e4lyn kev\u00e4tt\u00e4\". Toiveet ja odotukset ovat j\u00e4lleen korkealla. Meill\u00e4 on enemm\u00e4n dataa ja laskentatehoa kuin koskaan, ja teko\u00e4lyn sovellukset ovat osoittautuneet liiketoiminnan kannalta hy\u00f6dylliseksi, joten hypeen on varmasti syit\u00e4.</p> <p>Teht\u00e4v\u00e4</p> <p>Tutustu AI Winter -teeman aikajanaan. Etsi artikkeli, joka k\u00e4sittelee teko\u00e4lyn historiaa sen talvineen. L\u00f6yd\u00e4tk\u00f6 aikajanan? Millaisissa sykleiss\u00e4 talvet ovat esiintyneet?</p>"},{"location":"koneoppiminen/yleista/#tekoalyn-maaritelma","title":"Teko\u00e4lyn m\u00e4\u00e4ritelm\u00e4","text":"<p>Jotta meill\u00e4 voisi olla ristiriidaton m\u00e4\u00e4ritelm\u00e4 teko\u00e4lylle, meill\u00e4 tulisi olla ensin ristiriidaton m\u00e4\u00e4ritelm\u00e4 (ihmisen) \u00e4lykkyydelle. T\u00e4llaista ei ole, joten my\u00f6s AI:n suhteen joudumme tyytym\u00e4\u00e4n vaihteleviin m\u00e4\u00e4ritelmiin. Kirjassa \"Artificial Intelligence: A Modern Approach\" (S. Russell &amp; P. Norvig, 3. painos, 2010) esitet\u00e4\u00e4n, ett\u00e4 teko\u00e4ly on ala, joka pyrkii ei vain ymm\u00e4rt\u00e4m\u00e4\u00e4n, vaan my\u00f6s rakentamaan \u00e4lykk\u00e4it\u00e4 toimijoita. Teko\u00e4lyn m\u00e4\u00e4ritelmi\u00e4 voidaan j\u00e4rjest\u00e4\u00e4 nelj\u00e4\u00e4n kategoriaan: ihmism\u00e4isesti ajattelemiseen, rationaalisesti ajattelemiseen, ihmism\u00e4isesti toimimiseen ja rationaalisesti toimimiseen. N\u00e4iden nelj\u00e4n kategorian jakautuminen perustuu kahteen akseliin: rationaali-ihmism\u00e4inen (engl. rational-humanly) ja ajattelu-toiminta (engl. thinking-acting). Jos teko\u00e4ly toimii ihmism\u00e4isesti (engl. acting humanly), sen k\u00e4ytt\u00e4ytyminen on vaikea erottaa ihmisen k\u00e4ytt\u00e4ytymisest\u00e4. Esimerkiksi kuulustelija ei tiet\u00e4isi, k\u00e4yk\u00f6 h\u00e4n keskustelua botin vai ihmisen kanssa. Vastakohta t\u00e4lle molemmilla akselilla on rationaalisesti ajattelu (engl. thinking rationally). T\u00e4m\u00e4n m\u00e4\u00e4ritelm\u00e4n mukaan botti noudattaisi t\u00e4ydellist\u00e4 p\u00e4\u00e4ttelyprosessia. Kaikki olisi t\u00e4ysin virheet\u00f6nt\u00e4 logiikkaa.</p> <p>Teht\u00e4v\u00e4</p> <p>Pohdi, mit\u00e4 esteit\u00e4 tulisi vastaan, jos yritt\u00e4isit ratkaista todellisen maailman ongelmia k\u00e4ytt\u00e4en bottia, joka pyrkii t\u00e4ydellisesti aukottomaan, rationaaliseen ajatteluun?</p> <p>Rationaalinen toiminta (engl. rational-acting) vaikuttaa olevan parhaiten soveltuva l\u00e4hestymistapa k\u00e4yt\u00e4nn\u00f6n teko\u00e4lylle. Rationaalinen toimija on olio, joka havaitsee ymp\u00e4rist\u00f6ns\u00e4 erilaisten antureiden avulla ja toimii sen mukaisesti, mutta pystyy sopeutumaan muutoksiin ja tavoittelee p\u00e4\u00e4m\u00e4\u00e4ri\u00e4. T\u00e4m\u00e4n kurssin aikana luomme useita erilaisia rationaalisia toimijoita ja niiden komponentteja: yksi niist\u00e4 on koneoppiminen, joka on t\u00e4ll\u00e4 hetkell\u00e4 hallitseva tapa rakentaa teko\u00e4ly\u00e4.</p> <p>T\u00e4m\u00e4n materiaalin puitteissa voit luottaa seuraavaan m\u00e4\u00e4ritelm\u00e4\u00e4n: AI eli teko\u00e4ly on mit\u00e4 tahansa, mik\u00e4 ulkoap\u00e4in vaikuttaa joltakin, mik\u00e4 tyypillisesti vaatii ihmisen \u00e4lykkyytt\u00e4. Esimerkiksi kielioppivirheit\u00e4 tai syntaksivirheit\u00e4 voi poistaa tekstist\u00e4 s\u00e4\u00e4nt\u00f6pohjaisella logiikalla k\u00e4ytt\u00e4m\u00e4tt\u00e4 koneoppimista laisinkaan. My\u00f6s esimerkiksi \"\u00e4lyliikennevaloja\" voi ohjata hyvinkin s\u00e4\u00e4nt\u00f6pohjaisesti.</p> <p>Question</p> <p>Onko jokin alla listatuista teoksista tuttu? Mit\u00e4 teko\u00e4ly tarkoittaa kyseisess\u00e4 tarinassa? Mitk\u00e4 muut elokuvat, tv-sarjat tai kirjat kuuluisivat listalle?</p> <ul> <li>2001: A Space Odyssey (1968)</li> <li>Hitchhiker's Guide to the Galaxy (1978/...)</li> <li>Terminator (1984/...)</li> <li>The Matrix (1999/...)</li> <li>A.I. Artificial Intelligence (2001)</li> <li>Moon (2009)</li> <li>Her (2013)</li> <li>Ex Machina (2014)</li> <li>Companion (2025)</li> </ul>"},{"location":"koneoppiminen/yleista/#tekoalyn-haarat","title":"Teko\u00e4lyn haarat","text":"<p>Teko\u00e4ly on kattok\u00e4site ja sen alle lukeutuu eri aloja. Kirjassa Artificial Intelligence with Python (Joshi, P. 2017) esitell\u00e4\u00e4n teko\u00e4lyn eri haarat seuraavasti:</p> <ul> <li>Koneoppiminen ja hahmontunnistus (engl. machine learning and pattern recognition): datasta oppiminen ja siit\u00e4 ennustaminen. T\u00e4m\u00e4n kurssin AI edustaa p\u00e4\u00e4asiassa t\u00e4t\u00e4.</li> <li>Logiikkapohjainen AI (engl. logic-based AI): s\u00e4\u00e4nt\u00f6pohjaiset j\u00e4rjestelm\u00e4t, jotka perustuvat logiikkaan. K\u00e4ytet\u00e4\u00e4n esimerkiksi kielen parsimiseen.</li> <li>Haku (engl. search): algoritmit, jotka etsiv\u00e4t esimerkiksi optimaalista reitti\u00e4. Peleist\u00e4 ja navigaattoreista tuttuja.</li> <li>Tiedon esitt\u00e4minen (enlg. knowledge representation): yhteyksien luominen tiedon v\u00e4lille taksonomian tai muun hierarkisen j\u00e4rjestelm\u00e4n avulla.</li> <li>Suunnittelu (engl. planning): algoritmit, jotka suunnittelevat toimintaa tavoitteiden saavuttamiseksi.</li> <li>Heuristiikka (engl. heuristics): reittien tai ratkaisuiden etsiminen tilanteessa, jossa optimaalista ratkaisua ei ole mahdollista tai k\u00e4yt\u00e4nn\u00f6llist\u00e4 l\u00f6yt\u00e4\u00e4, kenties nojaten nyrkkis\u00e4\u00e4nt\u00f6\u00f6n tai akateemiseen arvaukseen, joka on osoitettavissa riitt\u00e4v\u00e4n hyv\u00e4ksi.</li> <li>Geneettinen ohjelmointi (engl. genetic programming): algoritmit, jotka k\u00e4ytt\u00e4v\u00e4t evoluutioteoriaa ratkaisujen l\u00f6yt\u00e4miseen.</li> </ul>"},{"location":"koneoppiminen/yleista/#koneoppiminen","title":"Koneoppiminen","text":"<p>Kuvio 2: Koneoppiminen, teko\u00e4ly ja datatiede.</p> <p>Koneoppiminen on teko\u00e4lyn osa-alue. Kaikki koneoppiminen on teko\u00e4ly\u00e4, mutta kaikki teko\u00e4ly ei ole koneoppimista. Koneoppimisen m\u00e4\u00e4ritelm\u00e4\u00e4n kuuluu, ett\u00e4 teko\u00e4lymalli oppii datasta. Malli oppii siis kokemuksesta. Koneoppimismallin luominen (\"mallinnus\") on prosessi, jossa valittu koneoppimisalgoritmi oppii datasta. Ihminen valitsee sek\u00e4 algoritmin ett\u00e4 datan - ja n\u00e4iden valinnalla on merkitt\u00e4v\u00e4 vaikutus valmiin mallin laatuun.</p>"},{"location":"koneoppiminen/yleista/#maaritelmia","title":"M\u00e4\u00e4ritelmi\u00e4","text":"<p>\"Machine learning (ML) is a collection of algorithms and techniques used to design systems that learn from data. These systems are then able to perform predictions or deduce patterns from the supplied data.\"</p> <p>L\u00e4hde: Lee, W. 2019. Python Machine Learning. Wiley. </p> <p>The machine learning portion of the picture enabled an AI to perform these tasks:</p> <ul> <li>Adapt to new circumstances that the original developer didn't envision</li> <li>Detect patterns in all sorts of data sources</li> <li>Create new behaviors based on the recognized patterns</li> <li>Make decisions based on the success of failure of these behaviors.</li> </ul> <p>L\u00e4hde: Mueller, P &amp; Massaron, L. 2016. Machine Learning for Dummies. No Starch Press. L\u00f6ytyy Finna-palvelusta.</p> <p>\"Difference between machine learning and AI: </p> <p>If it is written in Python, it's probably machine learning </p> <p>If it is written in PowerPoint, it's probably AI\" </p> <p>L\u00e4hde: Matt Velloso:n Twitter-viesti marraskuulta 2018</p> <p>\"Machine learning is a field of study concerned with giving computers the ability to learn without being explicitly programmed.\"</p> <p>L\u00e4hde: Arthur Smith, 1959.</p> <p>\"A computer program is said to learn from experience, E, with respect to a task, T, and a performance measure, P, if its performance on T, as measured by P, improves with experience E.\"</p> <p>L\u00e4hde: Tom Mitchell, 1998.</p> <p>\"A program or system that builds (trains) a predictive model from input data. The system uses the learned model to make useful predictions from new (never-before-seen) data drawn from the same distribution as the one used to train the model. Machine learning also refers to the field of study concerned with these programs or systems.\"</p> <p>L\u00e4hde: Google Developers Machine Learning Glossary</p>"}]}